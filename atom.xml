<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>零一人生</title>
  
  <subtitle>什么都想学，什么都学不会。</subtitle>
  <link href="http://chengfeng96.com/atom.xml" rel="self"/>
  
  <link href="http://chengfeng96.com/"/>
  <updated>2022-01-03T03:39:28.000Z</updated>
  <id>http://chengfeng96.com/</id>
  
  <author>
    <name>Hazza Cheng</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NAS 学习笔记（二十）- PC-DARTS</title>
    <link href="http://chengfeng96.com/blog/2021/11/21/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89-PC-DARTS/"/>
    <id>http://chengfeng96.com/blog/2021/11/21/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89-PC-DARTS/</id>
    <published>2021-11-21T12:16:58.000Z</published>
    <updated>2022-01-03T03:39:28.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　PC-DARTS<sup>[1]</sup> 由华为发表在 ICLR 2020 上，作者主要改进了DARTS 搜索时开销太大的问题，结局的方法是对 channel进行随机采样，这样可以减少显存的开销，使每个 batch有更多的样本，是搜索更加稳定。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　DARTS 在 op 选择的时候，全部都要forward，计算量和内存很大，不能使用大的批量。PC-DARTS的动机就是不把全部通道送入 op 选择中，而是随机采样 channel子集进行运算，其他的直接通过。这样做带来的问题是，由于采样的随机性，前向节点的选择可能是不稳定的，因此作者通过添加一个额外的边选择超参数，来引入了edge normalization 进行稳定。因为对 channel 进行了采样，我们可以增大batch size，选择 <span class="math inline">\(\frac{1}{K}\)</span>的通道可以减少 <span class="math inline">\(K\)</span> 倍内存，batch size增大 <span class="math inline">\(K\)</span> 倍，不仅可以加速 K倍，还可以稳定搜索。</p><h1 id="proxylessnas">ProxylessNAS</h1><h2 id="channel-采样">channel 采样</h2><p>　　我们记原始的 DARTS 边和节点计算的过程：</p><p><span class="math display">\[f_{i, j}\left(\mathbf{x}_{i}\right)=\sum_{o \in \mathcal{O}} \frac{\exp\left\{\alpha_{i, j}^{o}\right\}}{\sum_{o^{\prime} \in \mathcal{O}} \exp\left\{\alpha_{i, j}^{o^{\prime}}\right\}} \cdoto\left(\mathbf{x}_{i}\right)\]</span></p><p><span class="math display">\[\mathbf{x}_{j}=\sum_{i&lt;j} f_{i, j}\left(\mathbf{x}_{i}\right)\]</span></p><p>　　PC-DARTS 的做法其实很简单，就是引入一个随机采样的 mask <span class="math inline">\(\mathbf{S}_{i, j}\)</span>：</p><p><span class="math display">\[f_{i, j}^{\mathrm{PC}}\left(\mathbf{x}_{i} ; \mathbf{S}_{i,j}\right)=\sum_{o \in \mathcal{O}} \frac{\exp \left\{\alpha_{i,j}^{o}\right\}}{\sum_{o^{\prime} \in \mathcal{O}} \exp \left\{\alpha_{i,j}^{o^{\prime}}\right\}} \cdot o\left(\mathbf{S}_{i, j} *\mathbf{x}_{i}\right)+\left(1-\mathbf{S}_{i, j}\right) * \mathbf{x}_{i}\]</span></p><p>　　作者同时引入了一个超参数 <span class="math inline">\(K\)</span>，它代表只有 <span class="math inline">\(\frac{1}{K}\)</span> 的 channel会被采样到，这个超参数其实是速度和准确率的一个 trade-off，它不仅可以减少<span class="math inline">\(K\)</span>倍的计算量，还可以有更多的样本来采样，这对于网络搜索尤为重要，图示为：</p><p><img src="/blog/2021/11/21/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%E5%8D%81%EF%BC%89-PC-DARTS/1.png"></p><h2 id="edge-normalizaion">edge normalizaion</h2><p>　　channel 采样的好处是减少所选 op 的 bias，也就是对于对于边 <span class="math inline">\((i,j)\)</span>，给定 <span class="math inline">\(\mathbf{x}_{i}\)</span>，使用两组超参数 <span class="math inline">\(\left\{\alpha_{i, j}^{o}\right\}\)</span> 和 <span class="math inline">\(\left\{\alpha_{i, j}^{&#39;o}\right\}\)</span>的差距就减小了。但是它减弱了 weight-free 的op（skip-connect，maxpooling）的优势。前面有些论文也提到，在搜索的早期，算法更喜欢weight-free 的 op，因为这些 op 没有 weight 能输出一致的结果。但是对于有weight 的 op，优化过程中会出现不一致的情况。这样无 weight 的 op会占据很大的比重，后续有 op 的 op 优化的很好也无法超过他们。这种现象在proxy dataset 比较困难的时候尤其严重，也导致了 DARTS 在 ImageNet上效果不好。PC-DARTS 能有更好并且更稳定的效果。 　　channel采样的坏处是在原始的 DARTS 中，每个输出节点 <span class="math inline">\(\mathbf{x}_{j}\)</span> 都需要从 <span class="math inline">\(\{\mathbf{x}_{0},\mathbf{x}_{1},...,\mathbf{x}_{j-1}\}\)</span>挑选两个输入节点，权重分别为： <span class="math inline">\(\max _{o}\alpha_{0, j}^{o}, \max _{o} \alpha_{1, j}^{o}, \ldots, \max _{o}\alpha_{j-1, j}^{o}\)</span> 。但是这些结构参数是通过随机采样的 channel来优化的，可能并不稳定，导致网络结构的波动变化。PC-DARTS 使用 edgenormalizaion 来缓解这个问题，对每条边 <span class="math inline">\((i,j)\)</span> 显式赋予权重，记作 <span class="math inline">\(\beta_{i, j}\)</span>： <span class="math display">\[\mathbf{x}_{j}^{\mathrm{PC}}=\sum_{i&lt;j} \frac{\exp \left\{\beta_{i,j}\right\}}{\sum_{i^{\prime}&lt;j} \exp \left\{\beta_{i^{\prime},j}\right\}} \cdot f_{i, j}\left(\mathbf{x}_{i}\right)\]</span></p><p>　　实际上用到了两套结构参数的乘积。</p><h1 id="conclusion">Conclusion</h1><p>　　作者为了减少 DARTS 的开销来加速搜索，提出了对 channel随机采样，并且采样的前提假设是采样的 channel子集可以作为全集的近似，因此为了让采样后的结果更加稳定，作者引入了 edgenormalization 来使搜索更加稳定。</p><h1 id="refer">Refer</h1><ul><li>[1] Xu, Yuhui, et al. "PC-DARTS: Partial Channel Connections forMemory-Efficient Architecture Search." International Conference onLearning Representations. 2019.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　PC-DARTS&lt;sup&gt;[1]&lt;/sup&gt; 由华为发表在 ICLR 2020 上，作者主要改进了
DARTS 搜索时开销太大的问题，结局的方法是对 channel
进行随机采样，这样可以减少显存的开销，使每个 batch
有更多的样本，是搜索更加稳定。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="PC-DARTS" scheme="http://chengfeng96.com/tags/PC-DARTS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十九）- ProxylessNAS</title>
    <link href="http://chengfeng96.com/blog/2021/11/17/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89-ProxylessNAS/"/>
    <id>http://chengfeng96.com/blog/2021/11/17/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89-ProxylessNAS/</id>
    <published>2021-11-17T02:43:22.000Z</published>
    <updated>2022-01-03T03:40:12.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　ProxylessNAS<sup>[1]</sup> 由 MIT 发表在 ICLR 2019 上 ，NAS在大型数据集会出现 GPU 内存消耗过大和耗时太长的问题，Proxyless主要解决的问题就是这个，它可以直接在大型数据集上搜索，而不是使用 proxytask。论文内容乍看上去想法跟 DARTS有点像，但是其实视角还是有些去别的。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　DARTS 把搜索空间转为可微形式，把 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(\alpha\)</span> 联合优化，但它计算堆叠 block的时候仍然占用着大量 GPU内存，这样会造成在小数据集上搜索出的模型，可能在大数据集上表现并不佳。<br>　　ProxylessNAS 就是优化了这个问题，它整体框架是 One-Shot形式的，可以直接在大规模数据集上学习并且可以在损失函数里加上对 latency的限制。</p><h1 id="proxylessnas">ProxylessNAS</h1><h2 id="搜索空间">搜索空间</h2><p><img src="/blog/2021/11/17/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89-ProxylessNAS/1.png"></p><p>　　搜索空间有点类似 SPOS 的形式，可以同时学习权重参数 <span class="math inline">\(w\)</span> 和二值化架构参数 <span class="math inline">\(\alpha\)</span>，每次只给一条边设置为1，其它设置为 0，每次只更新一条边。</p><h2 id="二值化路径">二值化路径</h2><p>　　为了减少 GPU内存占用，在训练超网络时只保留一条路径，也就是对整个路径进行二值化，主要是基于二进制门<span class="math inline">\(g\)</span>：</p><p><span class="math display">\[g=\operatorname{binarize}\left(p_{1}, \cdots,p_{N}\right)=\left\{\begin{array}{cl}{[1,0, \cdots, 0]} &amp; \text { with probability } p_{1} \\\cdots &amp; \\{[0,0, \cdots, 1]} &amp; \text { with probability } p_{N}\end{array}\right.\]</span></p><p><span class="math display">\[m_{\mathcal{O}}^{\text {Binary }}(x)=\sum_{i=1}^{N} g_{i}o_{i}(x)=\left\{\begin{array}{cl}o_{1}(x) &amp; \text { with probability } p_{1} \\\cdots &amp; \\o_{N}(x) &amp; \text { with probability } p_{N}\end{array}\right.\]</span></p><p>　　训练 <span class="math inline">\(w\)</span> 和 <span class="math inline">\(\alpha\)</span> 需要二者迭代进行，首先要固定好<span class="math inline">\(w\)</span>，然后随机采样二值化gate，得到一个网络结构进行训练。训练 <span class="math inline">\(\alpha\)</span> 参数就要先固定 <span class="math inline">\(w\)</span>，然后重置 gate，在验证集上更新 <span class="math inline">\(\alpha\)</span> 参数。<br>　　与 <span class="math inline">\(w\)</span> 不同，<span class="math inline">\(\alpha\)</span>不直接涉及计算图，因此不能使用标准的梯度下降更新，论文提出一种基于梯度的方法来学习<span class="math inline">\(\alpha\)</span>：</p><p><span class="math display">\[\frac{\partial L}{\partial \alpha_{i}}=\sum_{j=1}^{N} \frac{\partialL}{\partial p_{j}} \frac{\partial p_{j}}{\partial \alpha_{i}} \approx\sum_{j=1}^{N} \frac{\partial L}{\partial g_{j}} \frac{\partialp_{j}}{\partial \alpha_{i}}=\sum_{j=1}^{N} \frac{\partial L}{\partialg_{j}} \frac{\partial\left(\frac{\exp \left(\alpha_{j}\right)}{\sum_{k}\exp \left(\alpha_{k}\right)}\right)}{\partial\alpha_{i}}=\sum_{j=1}^{N} \frac{\partial L}{\partial g_{j}}p_{j}\left(\delta_{i j}-p_{i}\right)\]</span></p><p>　　因为存在 N 条路径，因此在更新 <span class="math inline">\(\alpha\)</span> 的时候回需要 N 倍的 GPU内存占用，为了解决这个问题，文中根据多项式分布 <span class="math inline">\(p_1,...,p_N\)</span>采样两条路径，这样就可以把候选数从 N 降到 2，同时路径权值 <span class="math inline">\(p_i\)</span> 和二进制门 <span class="math inline">\(g_i\)</span>被相应地重置也要重置。然后使用采样的到的两个路径通过上述公式更新 <span class="math inline">\(\alpha\)</span>。因为路径权值 <span class="math inline">\(p_i\)</span> 是通过 softmax来计算的，所以需要进行比例放缩来更新 <span class="math inline">\(\alpha\)</span>来保持没有被采样得到的路径权重不变。因此一个采样路径权值增加，另一个采样路径权值减少，而所有其他路径保持不变。这样，无论N的值是多少，网络参数的每个更新步骤只涉及两条路径，从而将内存需求降低。<br>　　其实我感觉这个和 SPOS 有点像，只不过 SPOS训练超网络的时候采样时随机采样的，但是 ProxylessNAS是根据一个概率分布采样的，并且整个概率分布是可学习的，不知道我的理解对不对。</p><h2 id="latency-可微分">latency 可微分</h2><p>　　模型的 latency 是一个不可微的变量，为了使 latency可微，作者将网络的延迟建模为神经网络维度的连续函数 <span class="math inline">\(F\)</span>，对一个 mixed op，它的 latency可以建模为：</p><p><span class="math display">\[\mathbb{E}\left[\text { latency }_{i}\right]=\sum_{j} p_{j}^{i} \timesF\left(o_{j}^{i}\right)\]</span></p><p>　　整个网络的 latency 可以建模为：</p><p><span class="math display">\[\mathbb{E}[\text { latency }]=\sum_{i} \mathbb{E}\left[\text { latency}_{i}\right]\]</span></p><p>　　整个过程如下图所示：</p><p><img src="/blog/2021/11/17/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B9%9D%EF%BC%89-ProxylessNAS/2.png"></p><p>　　整个损失函数为：</p><p><span class="math display">\[\text { Loss }=\text { Loss }_{C E}+\lambda_{1}\|w\|_{2}^{2}+\lambda_{2}\mathbb{E}[\text { latency }]\]</span></p><h1 id="conclusion">Conclusion</h1><p>　　ProxylessNAS 训练时使用路径二值化以此可以直接在 image net上训练。supernet训练完后，只需选择具有最高路径权值的路径，以此来剪枝掉冗余的路径得到最终的网络。</p><h1 id="refer">Refer</h1><ul><li>[1] Cai, Han, Ligeng Zhu, and Song Han. "Proxylessnas: Direct neuralarchitecture search on target task and hardware." arXiv preprintarXiv:1812.00332 (2018).</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　ProxylessNAS&lt;sup&gt;[1]&lt;/sup&gt; 由 MIT 发表在 ICLR 2019 上 ，NAS
在大型数据集会出现 GPU 内存消耗过大和耗时太长的问题，Proxyless
主要解决的问题就是这个，它可以直接在大型数据集上搜索，而不是使用 proxy
task。论文内容乍看上去想法跟 DARTS
有点像，但是其实视角还是有些去别的。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="ProxylessNAS" scheme="http://chengfeng96.com/tags/ProxylessNAS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十八）- DARTS+PT</title>
    <link href="http://chengfeng96.com/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/"/>
    <id>http://chengfeng96.com/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/</id>
    <published>2021-04-06T07:49:44.000Z</published>
    <updated>2022-01-03T03:40:25.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　DARTS+PT<sup>[1]</sup> 发表在 ICLR 2021 上，视角很独特，它指出架构参数 <span class="math inline">\(\alpha\)</span>中的权重参数在很多情况下并不能衡量对应候选操作对于 supernet的重要性。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　我们知道在 DARTS 中，每条边 evaluate 阶段最后选择的 op是根据架构参数 <span class="math inline">\(\alpha\)</span> 的 argmax得到，这样会带来 search 阶段和 evaluate 阶段架构存在性能可能一个gap，并且会带来 skip-connect 富集的现象。<br>　　如下图所示，作者通过实验指出，在很多情况下，每条边中最大值对应的候选操作，并不是对于supernet 而言最重要的操作：</p><p><img src="/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/1.png"></p><p>　　当我们采取更简单的 search space，即每条边的 candidate op 只有skip connection 和 seq_conv_3x3 时，下图展现出了问题，会出现大量的 skipconnection，14 条边里有 12 个都是 skipconnection，显然这样造成了最后模型的精度不佳。</p><p><img src="/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/2.png"></p><p>　　如 VGG 这样的的线性叠加的深度网络，其各层 layers 如果在 inference阶段进行重新排序的话，将极大影响模型性能。而对于 ResNet 而言，channel相同的连续层都在输出相同的 optimal feature map（参考[2]），这样保证了各层的网络输出在最终收敛时相对接近。因此，ResNet 对于inference 阶段的 layer reordering 具有一定的稳定性。如下图所示，作者认为DARTS 无论从结构还是性能表现而言，都接近于 ResNet 结构而非 VGG，因此DARTS 也会具备上述 ResNet 的特性，即 Cell 中的各条边也会 share 相同的optimal feature map。</p><p><img src="/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/.png"></p><p>　　结合上述理解，作者用数学证明 DARTS 中 skip-connect富集现象的原因，这里假设搜索空间里只有两种 op（conv 和skip-connect），即：</p><p><span class="math display">\[\bar{m}_{e}\left(x_{e}\right)=\frac{\exp\left(\alpha_{\operatorname{con} v}\right)}{\exp\left(\alpha_{\operatorname{conv}}\right)+\exp \left(\alpha_{s k ip}\right)} o_{e}\left(x_{e}\right)+\frac{\exp \left(\alpha_{s k ip}\right)}{\exp \left(\alpha_{\operatorname{conv}}\right)+\exp\left(\alpha_{s k i p}\right)} x_{e}\]</span></p><p>　　进一步得到（相关证明在论文的附录中有，主要用了拉格拉日法）：</p><p><span class="math display">\[\begin{array}{l}\alpha_{\operatorname{conv}}^{*} \propto\operatorname{var}\left(x_{e}-m^{*}\right) \\\alpha_{s k i p}^{*} \propto\operatorname{var}\left(o_{e}\left(x_{e}\right)-m^{*}\right)\end{array}\]</span></p><p>　　<span class="math inline">\(m^{*}\)</span> 表示各条边共享的optimal feature map，<span class="math inline">\(x_{e}\)</span>表示每条边的 input feature map（也表示 skip-connect 操作的输出），<span class="math inline">\(o_{e}(x_{e})\)</span> 表示卷积操作的输出。<br>　　可知 <span class="math inline">\(\alpha\)</span> 根据不同 op的大小关系，其实可以通过 op 产生的输出与共享的 optimal feature map之间方差值体现，是个正比关系。由于 skip-connect 的输出即为输入，则 <span class="math inline">\(x_e\)</span> 即为 Cell 中上一条边的输出 <span class="math inline">\(\bar{m}_{e}\left(x_{e}\right)\)</span> ，因为 Cell中的各边会 share 相同的 optimal feature map，故 <span class="math inline">\(x_e\)</span> 其实是在直接估计该 Cell 的 optimalfeature map，即 <span class="math inline">\(m^{*}\)</span>。而 <span class="math inline">\(o_{e}\left(x_{e}\right)\)</span>只是卷积操作的输出，与边 <span class="math inline">\(e\)</span>的混合输出存在 gap，因此在模型收敛时，该值会偏离最终的 optimal featuremap <span class="math inline">\(m^{*}\)</span>，导致下面公式的方差逐渐增大，产生 skip-connect对应权重在优化时逐渐增大的结果。<br>　　下图还表示 supernet 的最终收敛效果越好，skip-connect 与 conv操作之间 <span class="math inline">\(\alpha\)</span> 权重的 gap就会越大：</p><p><img src="/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/4.png"></p><h1 id="perturbation-based-architecture-selection">Perturbation-basedArchitecture Selection</h1><p>　　作者重新思考了 DARTS中架构参数的作用。并认为应该按照对搜索网络的贡献度来进行候选操作的选择，进一步提出了一种alternative perturbation-based 的架构选择方法。<br>　　作者首先提出了 discretization accuracy at convergence。对于在 search阶段已经收敛的 supernet，evaluate 阶段首先将其对应的架构参数进行discretization，转换为 one-hot 向量，然后对调整后的 supernet 进行fine-tune，直至其收敛。这样可以得到 supernet再次收敛之后的分类准确率，作者称之为 discretization accuracy atconvergence。作者将该准确率作为当前候选操作对搜索网络贡献度的指标，则discretization accuracy最高的候选操作被记为当前边中对搜索网络贡献度最大的操作。<br>　　作者提出的方法，便是基于这一指标，具体实现的方法是每次随机采样一条边，依次丢弃边上的每一个op，然后计算验证集上的 acc，然后做 discretization，保留 acc减少最多的那个 op。作者在 edge-level 和 node-level都采用了这种方法进行架构选择，且每次 edge 与 node的采样都是随机的，算法如下：</p><p><img src="/blog/2021/04/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AB%EF%BC%89-DARTS-PT/5.png"></p><h1 id="conclusion">Conclusion</h1><p>　　本文的视角和独特，不仅指出了 <span class="math inline">\(\alpha\)</span> 并不一定能代表每个 op的性能，并且从理论上分析了 skip-connect 富集的原因。同时用了一种perturbation-based 的方法来缓解 search 阶段和 evaluate 阶段 performancegap 的问题。</p><h1 id="refer">Refer</h1><ul><li>[1] Wang, Ruochen, et al. "Rethinking architecture selection indifferentiable NAS." International Conference on LearningRepresentations. 2021.</li><li>[2] <a href="https://blog.csdn.net/FengF2017/article/details/85110055">理解ResNet</a></li><li>[3] <a href="https://zhuanlan.zhihu.com/p/344538995">ICLR 2021 Oral| 重新思考可微分NAS方法中的架构选择</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　DARTS+PT&lt;sup&gt;[1]&lt;/sup&gt; 发表在 ICLR 2021 上
，视角很独特，它指出架构参数 &lt;span class=&quot;math inline&quot;&gt;\(\alpha\)&lt;/span&gt;
中的权重参数在很多情况下并不能衡量对应候选操作对于 supernet
的重要性。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="DARTS" scheme="http://chengfeng96.com/tags/DARTS/"/>
    
    <category term="DARTS+PT" scheme="http://chengfeng96.com/tags/DARTS-PT/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十七）- SNAS</title>
    <link href="http://chengfeng96.com/blog/2020/12/04/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89-SNAS/"/>
    <id>http://chengfeng96.com/blog/2020/12/04/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89-SNAS/</id>
    <published>2020-12-04T12:06:15.000Z</published>
    <updated>2022-01-03T03:40:03.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　SNAS<sup>[1]</sup> 由商汤在 ICLR 2019 上提出，它解释了 ENAS利用强化学习去搜索带来的收敛慢的原因，通过对 NAS进行重新建模，直接通过梯度优化NAS的目标函数。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　 作者首先解释了 NAS 和 ENAS 利用 MDP结合强化学习收敛慢的原因，因为 NAS是一个确定环境中的完全延迟奖励的任务，这里的延迟奖励指的是 RL 的controller 要等整个架构 search 完毕然后在验证集上 evaluate 才能得到一个accuracy 作为一个 reward，也就是：</p><p><span class="math display">\[r_{t}=\left\{\begin{array}{ll}0, &amp; s_{t} \neq \text {terminal} \\\text {acc,} &amp; s_{t}=\text {terminal}\end{array} .\right.\]</span></p><p>　　延迟奖励会指数级延长TD的收敛需要的更新次数，延迟奖励会给指数级多的状态的MC 价值评估带来抖动。</p><h1 id="snas">SNAS</h1><h2 id="search-space-and-architecture-sampling">Search Space andArchitecture Sampling</h2><p><img src="/blog/2020/12/04/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%83%EF%BC%89-SNAS/1.png"></p><p>　　如上图所示，搜索空间可以通过一个 onehot 矩阵来表示，每条边上的 op可以随机产生，计算一个新的节点时可以通过下面的公式，和 DARTS有点类似：</p><p><span class="math display">\[x_{j}=\sum_{i&lt;j} \tilde{\boldsymbol{O}}_{i,j}\left(x_{i}\right)=\sum_{i&lt;j} \boldsymbol{Z}_{i, j}^{T}\boldsymbol{O}_{i, j}\left(x_{i}\right)\]</span></p><p>　　<span class="math inline">\(Z_{i,j}\)</span> 表示上图左边在边<span class="math inline">\((i,j)\)</span> 上的 one-hot随机变量，从母网络中产生子网络，可以通过在母网络的每一条边的所有可能神经变换的结果后乘上一个one-hot向量来实现。而对于子网络的采样，就因此自然转化为了对一系列one-hot随机变量的采样。<br>　　同时为了解决 MDP 中延迟奖励的问题，作者用 loss function 来替代accuracy，不需要额外拟合一个 score function，NAS 问题的 score就已经不是一个来自环境的常数而是一个可微函数了，这可以大幅提高 NAS的搜索效率，又因为损失函数和准确率都可以表达一个网络学习的结果，这一替换并没有在本质上改变NAS 问题原本的优化网络结构分布以使得它们的期望性能最好的目标： <span class="math display">\[\mathbb{E}_{\boldsymbol{Z} \simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}[R(\boldsymbol{Z})]=\mathbb{E}_{\boldsymbol{Z}\simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}\left[L_{\boldsymbol{\theta}}(\boldsymbol{Z})\right]\]</span></p><h2 id="reparameterization-and-differentiable">Reparameterization andDifferentiable</h2><p>　　之前提到的 <span class="math inline">\(Z_{i,j}\)</span>是离散的，作者利用了 reparameterization<sup>[2]</sup> 和 softmax来近似化可微：</p><p><span class="math display">\[\begin{aligned}\boldsymbol{Z}_{i, j}^{k} &amp;=f_{\boldsymbol{\alpha}_{i,j}}\left(\boldsymbol{G}_{i, j}^{k}\right) \\&amp;=\frac{\exp \left(\left(\log \boldsymbol{\alpha}_{i,j}^{k}+\boldsymbol{G}_{i, j}^{k}\right) / \lambda\right)}{\sum_{l=0}^{n}\exp \left(\left(\log \boldsymbol{\alpha}_{i, j}^{l}+\boldsymbol{G}_{i,j}^{l}\right) / \lambda\right)}\end{aligned}\]</span></p><p>　　其中 <span class="math inline">\(\boldsymbol{G}_{i,j}^{k}=-log(-log(\boldsymbol{U}_{i, j}^{k}))\)</span>，<span class="math inline">\(\boldsymbol{U}_{i, j}^{k}\)</span> 是一个 uniform的随机变量，<span class="math inline">\(\boldsymbol{\alpha}_{i,j}^{k}\)</span> 是 architecture 参数。</p><p>　　为了实现离散分布，作者用到了 Gumbel-max，它先采样与 one-hotvector 维度相同数量的 uniform distribution 的随机变量，将他们经过 Gumbel变换转为 Gumbel 随机变量，并从中选择最大的那一维度为1，其他维度为0，这样采样的随机变量的分布与该离散分布相同，而离散分布的参数也就转化为了Gumbel max 中的参数，实现了对该离散分布的重参数化。<span class="math inline">\(\lambda\)</span> 为 softmax 的temperature，当它趋近于 0 时，该方法产生的随机变量趋近于该离散分布。</p><p>　　对应的梯度为： <span class="math display">\[\begin{aligned}\frac{\partial \mathcal{L}}{\partial x_{j}} &amp;=\sum_{m&gt;j}\frac{\partial \mathcal{L}}{\partial x_{m}} \boldsymbol{Z}_{m}^{T}\frac{\partial \boldsymbol{O}_{m}\left(x_{j}\right)}{\partial x_{j}} \\\frac{\partial \mathcal{L}}{\partial \boldsymbol{\theta}_{i, j}^{k}}&amp;=\frac{\partial \mathcal{L}}{\partial x_{j}} \boldsymbol{Z}_{i,j}^{k} \frac{\partial \boldsymbol{O}_{i, j}\left(x_{i}\right)}{\partial\boldsymbol{\theta}_{i, j}^{k}} \\\frac{\partial \mathcal{L}}{\partial \boldsymbol{\alpha}_{i, j}^{k}}&amp;=\frac{\partial \mathcal{L}}{\partial x_{j}} \boldsymbol{O}_{i,j}^{T}\left(x_{i}\right)\left(\boldsymbol{\delta}\left(k^{\prime}-k\right)-\boldsymbol{Z}_{i,j}\right) \boldsymbol{Z}_{i, j}^{k} \frac{1}{\lambda\boldsymbol{\alpha}_{i, j}^{k}}\end{aligned}\]</span></p><h2 id="resource-constraint">Resource Constraint</h2><p>　　作者还通过在目标函数里添加模型复杂度的惩罚项来限制模型的规模，尽可能的搜到比较稀疏的模型：</p><p><span class="math display">\[\mathbb{E}_{\boldsymbol{Z} \simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}\left[L_{\boldsymbol{\theta}}(\boldsymbol{Z})+\etaC(\boldsymbol{Z})\right]=\mathbb{E}_{\boldsymbol{Z} \simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}\left[L_{\boldsymbol{\theta}}(\boldsymbol{Z})\right]+\eta\mathbb{E}_{\boldsymbol{Z} \simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}[C(\boldsymbol{Z})]\]</span></p><p>　　惩罚项的量值可以包括参数量、浮点计算数（FLOPs）以及需要的内存，采样出的子网络的这些值的总量计算与每一条边有关，因而相较于在每一条输入边上优化一个全局的网络前向传播的，我们只需要优化每条边上自己对时延的贡献量。如果回到之前贡献分配的语境，全局的时延惩罚被线性分配到了每一条边的决策<span class="math inline">\(Z_{i,j}\)</span>上，这有利于提高收敛效率：</p><p><span class="math display">\[C(\boldsymbol{Z})=\sum_{i, j} C\left(\boldsymbol{Z}_{i,j}\right)=\sum_{i, j} \boldsymbol{Z}_{i, j}^{T}C\left(\boldsymbol{O}_{i, j}\right)\]</span></p><p>　　又因为上式是一个线性的变换，我们既可以用重参数化计算的 <span class="math inline">\(\mathbb{E}_{z \sim p_{\alpha}}[C(z)]\)</span>期望，也可以用策略梯度的方法。</p><h2 id="与-darts-的区别">与 DARTS 的区别</h2><p>　　这篇文章的里有许多数学推导，需要仔细斟酌，我看到中间最大的疑问是它和DARTS 的最大区别是啥？作者在论文里和知乎解读里也给出了解答，这里直接摘抄<sup>[3]</sup>。<br>　　DARTS，不同于 SNAS 中通过完整的概率建模来提出新方法，DARTS将网络结构直接近似为确定性的连续权重，类似于attention。在搜索过程中，表达这个 softmax 连续权重的参数 <span class="math inline">\(\alpha\)</span> 与网络神经变换的参数 <span class="math inline">\(\theta\)</span> 同时被更新，完全收敛之后选择 <span class="math inline">\(\alpha\)</span> 的 argmax 搭建子网络，再重新训练<span class="math inline">\(\theta\)</span> 。<br>　　作者在论文的附录里也证明了 SNAS 不依靠 MDP 假设，提出了一种factorization 的方式。</p><p>　　因为 SNAS 直接优化 NAS 的目标，作者从 SNAS 的建模出发，对 DARTS的这一近似作出了概率建模下的解释：这种连续化的近似可以被理解为是将中<span class="math inline">\(\mathbb{E}_{z \simp_{\alpha}}\left[L_{\theta}(z)\right]\)</span> 的全局期望： <span class="math display">\[\begin{aligned}&amp; \mathbb{E}_{\boldsymbol{Z} \simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}\left[L_{\boldsymbol{\theta}}\left(\boldsymbol{Z}_{j,l}^{T} \boldsymbol{O}_{j, l}\left(\boldsymbol{Z}_{i, j}^{T}\boldsymbol{O}_{i, j}\left(x_{i}\right)\right)+\boldsymbol{Z}_{j, m}^{T}\boldsymbol{O}_{j, m}\left(\boldsymbol{Z}_{i, j}^{T} \boldsymbol{O}_{i,j}\left(x_{i}\right)\right)\right)\right] \\=&amp; \mathbb{E}_{\boldsymbol{Z} \simp_{\boldsymbol{\alpha}}(\boldsymbol{Z})}\left[L_{\boldsymbol{\theta}}\left(\sum_{m&gt;j}\boldsymbol{Z}_{j, m}^{T} \boldsymbol{O}_{j, m}\left(\boldsymbol{Z}_{i,j}^{T} \boldsymbol{O}_{i, j}\left(x_{i}\right)\right)\right]\right.\end{aligned}\]</span></p><p>　　直接分解到每一条输入边上，计算了一个解析的期望：</p><p><span class="math display">\[L_{\boldsymbol{\theta}}\left(\sum_{m&gt;j}\mathbb{E}_{p_{\boldsymbol{\alpha}_{j, m}}}\left[\boldsymbol{Z}_{j,m}^{T} \boldsymbol{O}_{j, m}\left(\mathbb{E}_{p_{\boldsymbol{\alpha}_{i,j}}}\left[\boldsymbol{Z}_{i, j}^{T} \boldsymbol{O}_{i,j}\left(x_{i}\right)\right]\right)\right]\right)\]</span></p><p>　　如果说 <span class="math inline">\(L\)</span> 对于每一个 <span class="math inline">\(Z\)</span>都是线性的，上面两个式子就是等价的。但是因为设计了 ReLU-Conv-BN的堆叠，带来了非线性，这两个目标并不等价。也就是说，在 DARTS的连续化近似中带来了很大的偏差（bias）。这一方面带来了最终优化的结果并没有理论保证的问题，使得一阶梯度的结果不尽人意；另一方面因为连续化近似并没有趋向离散的限制，最终通过删除较低权重的边和神经变换产生的子网络将无法保持训练时整个母网络的精度，DARTS提出用二阶梯度通过基于梯度的元学习来解决第一个问题，但是对于第二个问题，并没有给出一个自动化的解法，而是人工定义了一些规则来挑选边和神经变换，构建子网络，再重新训练。</p><h1 id="conclusion">Conclusion</h1><p>　　非常硬核的一篇文章，解答了我对 NAS 中 MDP 问题的许多问题。作者把op 的选择建模成一个分布，然后利用 reparameterization的方法进行优化，并且作者还在目标函数里添加模型复杂度的惩罚项来限制模型的规模，尽可能的搜到比较稀疏的模型。作者同时也介绍了DARTS 的缺陷和 SNAS 做的相应的改进。</p><h1 id="refer">Refer</h1><ul><li>[1] S. Xie, H. Zheng, C. Liu, and L. Lin, “SNAS: Stochastic neuralarchitecture search,” 7th Int. Conf. Learn. Represent. ICLR 2019, pp.1–17, 2019.</li><li>[2] https://spaces.ac.cn/archives/6705</li><li>[3] https://zhuanlan.zhihu.com/p/53920376</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　SNAS&lt;sup&gt;[1]&lt;/sup&gt; 由商汤在 ICLR 2019 上提出，它解释了 ENAS
利用强化学习去搜索带来的收敛慢的原因，通过对 NAS
进行重新建模，直接通过梯度优化NAS的目标函数。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="DARTS" scheme="http://chengfeng96.com/tags/DARTS/"/>
    
    <category term="SNAS" scheme="http://chengfeng96.com/tags/SNAS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十六）- NoisyDARTS</title>
    <link href="http://chengfeng96.com/blog/2020/11/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89-NoisyDARTS/"/>
    <id>http://chengfeng96.com/blog/2020/11/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%85%AD%EF%BC%89-NoisyDARTS/</id>
    <published>2020-11-09T12:56:10.000Z</published>
    <updated>2022-01-03T03:40:29.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　NoisyDARTS<sup>[1]</sup> 由小米在 BMVC 2021提出，它主要优化了原来 DARTS 中 skip-connect过多带来性下降的问题，主要的方法就是添加噪声。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　 这篇文章要解决的问题主要是 DARTS 在搜索过程中出现的 skip-connect过多的问题，这对模型稳定性有较大影响。之前小米在他们的另外一篇论文Fair-DARTS 中已经提出了一个解决这个问题的方法，将 softmax 换成 sigmoid使每种操作有自己的权重，这样鼓励不同的操作之间相互合作。华为在 P-DARTS中也提出了一种方法，对 skip-connet 加正则（dropout 和限制最大skip-connect 的个数）。<br>　　 本文提出了一个新的方法，既然 skip connection存在不公平优势，那么对其注入噪声即可干扰其优势，抑制其过度发挥，从而解决skipconnection 富集现象。</p><h1 id="noisydarts">NoisyDARTS</h1><p>　　其实 NoisyDARTS 的思想非常简单，实现起来几乎只要给原来的 DARTS代码中添加一行即可，主要思想就是：</p><p><span class="math display">\[\mathcal{L}=g(y), \quad y=f\left(\alpha^{s k i p}\right)\cdot(x+\tilde{x})\]</span></p><p>　　就是给 skip-connect 的输入添加一个噪音 <span class="math inline">\(\tilde{x}\)</span>，这个 <span class="math inline">\(\tilde{x}\)</span>需要满足的条件是它要很小，这样就可以达到如下估计：</p><p><span class="math display">\[y^{\star} \approx f(\alpha) \cdot x \quad \text { when } \quad \tilde{x}\ll x\]</span> 　　需要注意的是，后向传播时噪音也要加入计算：</p><p><span class="math display">\[\frac{\partial \mathcal{L}}{\partial \alpha^{s k i p}}=\frac{\partial\mathcal{L}}{\partial y} \frac{\partial y}{\partial \alpha^{s k ip}}=\frac{\partial \mathcal{L}}{\partial y} \frac{\partialf\left(\alpha^{s k i p}\right)}{\partial \alpha^{s k i p}}(x+\tilde{x})\]</span></p><p>　　为什么要这样呢？因为加入噪声会为梯度更新带来不确定性，因此选择噪声的原则首先要保持梯度的更新是有效的。我们先将它的梯度的期望做一个分解：</p><p><span class="math display">\[\mathbb{E}\left[\nabla_{s k i p}\right]=\mathbb{E}\left[\frac{\partial\mathcal{L}}{\partial y} \frac{\partial f\left(\alpha^{s k ip}\right)}{\partial \alpha^{s k i p}}(x+\tilde{x})\right] \approx\frac{\partial \mathcal{L}}{\partial y^{\star}} \frac{\partialf\left(\alpha^{s k i p}\right)}{\partial \alpha^{s k ip}}(\mathbb{E}[x]+\mathbb{E}[\tilde{x}])\]</span></p><p>　　因此 NoisyDARTS提出，应该加注一种无偏的并且方差较小的噪声，这样便可以使 <span class="math inline">\(\mathbb{E}[\tilde{x}] = 0\)</span>。 　　因此DARTS 原来的每个 mixed op 的计算便变成了这样：</p><p><span class="math display">\[\bar{o}_{i, j}(x)=\sum_{k=1}^{M-1} f\left(\alpha_{o^{k}}\right)o^{k}(x)+f\left(\alpha_{o^{s k i p}}\right) o^{s k i p}(x+\tilde{x})\]</span></p><p>　　作者在论文里使用的噪声是 <span class="math inline">\(\mu =0\)</span>，<span class="math inline">\(\sigma = \lambda \cdotstd(x)\)</span> 的高斯分布，实验中 <span class="math inline">\(\lambda =0.2\)</span> 的效果最好。</p><h1 id="conclusion">Conclusion</h1><p>　　NoisyDARTS 通过对 skip-connect中加入噪声（无偏小方差高斯噪声），极大的限制了原本 skip-connect的不公平竞争问题，解决了 DARTS 中 skip-connect富集和模型化后性能损失的问题。</p><h1 id="refer">Refer</h1><ul><li>[1] X. Chu, B. Zhang, and X. Li, “Noisy Differentiable ArchitectureSearch,” May 2020.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　NoisyDARTS&lt;sup&gt;[1]&lt;/sup&gt; 由小米在 BMVC 2021
提出，它主要优化了原来 DARTS 中 skip-connect
过多带来性下降的问题，主要的方法就是添加噪声。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="DARTS" scheme="http://chengfeng96.com/tags/DARTS/"/>
    
    <category term="NoisyDARTS" scheme="http://chengfeng96.com/tags/NoisyDARTS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十五）- P-DARTS</title>
    <link href="http://chengfeng96.com/blog/2020/11/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89-P-DARTS/"/>
    <id>http://chengfeng96.com/blog/2020/11/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89-P-DARTS/</id>
    <published>2020-11-06T10:15:45.000Z</published>
    <updated>2022-01-03T03:40:20.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　P-DARTS<sup>[1]</sup> 由华为在 ICCV 2019 上提出，它主要优化了原来DARTS 中 depth gap 和 skip-connect 过多的问题。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　 这篇文章要解决的问题主要是 DARTS 在搜索过程中出现的 proxydataset 和 target dataset 出现的 depth gap 的问题。proxy dataset 指的是NAS 搜索时用的数据集，这个数据集往往比较小，例如 CIFAR10，此时 NAS堆叠的 cell 数往往更小。而 target dataset 是 DARTS 搜出 cell的结构后需要 evaluate 的数据集，这个数据集往往比较大，例如ImageNet，所以需要堆叠的 cell 数也更多。因此便产生了 depthgap，为什么不直接也在 proxy dataset 上堆叠较多的 cell 呢？这由于 DARTS受限于内存和计算消耗，一般都会在 proxy dataset 上进行较浅的 initialchannel 及 layer depth 搜索，然后把搜好的模型再扩充较大的 channel 和depth 放到target集上重训重测。本文就是试图解决这个问题。<br>　　 本文另外一个优化的问题是 DARTS 搜出来的 skip connection过多会占据主导地位，这对模型稳定性有较大影响。作者对 skip connection做了一些正则化约束。 DARTS 为什么随着搜索的进行搜出来的 skip-connect越来越多呢，知乎上有一个很好的解释<sup>[2]</sup>。</p><blockquote><p>　　DARTS 发生 Collapse 背后的原因是在两层优化中，alpha 和 w的更新过程存在先合作（cooperation）后竞争（competition）的问题。粗略来说，在刚开始更新的时候，alpha和 w 是一起被优化，从而 alpha 和 w都是越变越好。渐渐地，两者开始变成竞争关系，由于 w 在竞争中比 alpha更有优势（比如，w 的参数量大于 alpha 的参数量，One-Shot 模型在大多数alpha 下都能收敛，等等），alpha开始被抑制，因此网络架构出现了先变好后变差的结果，也就是上上图中蓝线的情况。<br>　　具体来说，在搜索过程的初始阶段，One-Shot模型欠拟合到数据集，因此在搜索过程刚开始的时候，alpha 和 w（也就是One-Shot 模型的参数）都会朝着变好的方向更新，这就是合作的阶段。由于整个One-Shot 模型中，前面的 cell 比后面的 cell能接触到更干净的数据，如果我们允许不同的 cell可以拥有不同的网络结构（打破 DARTS 中 cell共享网络结构的设定），那么前面的 cell 会比后面的 cell更快地学到特征。<br>　　一旦前面的 cell 已经学到了不错的特征表达，而后面的 cell学到的特征表达相对较差，那么后面的 cell 接下来会倾向于选择skip-connect，来把前面 cell 已经学好的特征表达直接传递到后面。下图是打破DARTS 中 cell 共享网络结构的设定下，搜出来的网络结构图：可以看到，前面的cell 大部分都是卷积算子，而靠后的 cell 大部分都是 skip-connect。<br>　　回到 DARTS 的设定，如果我们强制不同的 cell 共享同一个网络结构，那么skip-connect 就会从后面的 cell 扩散到前面的 cell。当 skip-connect开始显著变多的时候，合作的阶段就转向了竞争的阶段：alpha 开始变坏，DARTS开始 collapse。</p></blockquote><h1 id="p-darts">P-DARTS</h1><h2 id="depth-gap">Depth Gap</h2><p>　　P-DARTS 优化 depth gap的方法其实很简单，是一个渐进式的方法，随着搜索的进行不断增加层数，所以计算量也会成倍增加，因此为了解决这个问题，P-DARTS提出了search space approximation策略，即每当层数增加时就会相应地减少候选操作的数量，一张图就可以理解：</p><p><img src="/blog/2020/11/06/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%94%EF%BC%89-P-DARTS/1.png"></p><ul><li>cells=5 时，每个 node 间有 5 个 candidate，当训练好了 25 epochs后，会有对应的 softmax 置信度。</li><li>接着进行 cells=11 的搜索，虽然深度加了一倍多，但这时每个 node 间operation candidate 将会减少接近一半，即把 (a) 中最后置信度较低的operation 直接 delete 掉。</li><li>同样的流程，最后进行 cells=17 的搜索，再砍掉置信度低的一半opeartion。</li></ul><p>　　这实际上也是一个 trade-off。</p><h2 id="skip-connect">Skip-connect</h2><p>　　P-DARTS 为了避免出现过多的 skip-connect，给 skip-connect加了正则，主要是两种方法：</p><ol type="1"><li>在每个 skip-connect 操作后插入 operation-leveldropout，然后训练时逐渐减少 dropout rate。</li><li>加个超参 M 来限制最后 cell 内部 skip-connect 的最大总数。</li></ol><p>　　需要注意的是作者提出第二种正则方法必须是在使用第一种正则方法的前提上才能使用，否则，在没有operation-level dropout 的情况下，搜索过程会产生低质量的 architectureweight，基于此即使有固定数量 skip-connect 的architecture，我们也无法构建更好的 architecture。</p><h1 id="conclusion">Conclusion</h1><p>　　P-DARTS 通过一种渐进式的算法缓解了 DARTS 中 depth gap，并且给skip-connect 加上了正则，也进一步提高了搜出的架构的稳定性。</p><h1 id="refer">Refer</h1><ul><li>[1] X. Chen, L. Xie, J. Wu, and Q. Tian, “Progressive differentiablearchitecture search: Bridging the depth gap between search andevaluation,” Proc. IEEE Int. Conf. Comput. Vis., vol. 2019-Octob, pp.1294–1303, 2019.</li><li>[2] https://zhuanlan.zhihu.com/p/82679004</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　P-DARTS&lt;sup&gt;[1]&lt;/sup&gt; 由华为在 ICCV 2019 上提出，它主要优化了原来
DARTS 中 depth gap 和 skip-connect 过多的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="DARTS" scheme="http://chengfeng96.com/tags/DARTS/"/>
    
    <category term="P-DARTS" scheme="http://chengfeng96.com/tags/P-DARTS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十四）- GDBT-NAS</title>
    <link href="http://chengfeng96.com/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/"/>
    <id>http://chengfeng96.com/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/</id>
    <published>2020-07-27T11:36:02.000Z</published>
    <updated>2022-01-03T03:40:33.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　GDBT-NAS<sup>[1]</sup> 由 MSRA 在 2020 年提出，它主要用 GBDT 作为NAS 算法中预测 candidate architecture 的 predictor，同时它还将 GBDT 作为search space 的 pruner ，思想还是比较简单的，本文对它做简单记录。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　 这篇文章要优化的问题其实很容易理解，就是在 NAS搜索过程中，我们需要评估一些 candidate architecuture的性能，这通常是一个开销非常大的过程。因为 train from scratch的开销很大，即使在 weight sharing 的基础上 fine tuning也有比较大的开销，所以有些研究者采取一些简单的 predcitor 去直接预测candidate architecuture 的性能，然后根据预测的结果挑选出 top-k 的architecuture 去用真实数据去训练，生成 architecture-accuracy pairs的样本去继续训练 predictor。<br>　　 这是一个很好的思路，之前的研究者使用过 MLP，LSTM 和 GCN 等等来作为predictor，而本文使用了 GBDT。为什么使用 GBDT 呢，因为 GBDT 更适合于tabular 的数据，而 architecuture 往往可用通过 discrete的方式来表示，因为它只可以包含有限的 operation，文中作者就将architecuture 用 one-hot 的形式表示了，还是很简单的。<br>　　 除了 predict 外，作者还将 GBDT 作为 search space 的 pruner 来减小search space 的大小并且能搜索出更好的 architecture，作者代码中使用的GBDT 是 LightGBM。</p><h1 id="gbdt-nas">GBDT-NAS</h1><p>　　作者提出了 GBDT-NAS，它在每次 iteration 时执行下面三个步骤：</p><ol type="1"><li>Train predictor：采样 <span class="math inline">\(N\)</span> 个architecture-accuracy pairs 去训练 GBDT predictor。</li><li>Predict：predict 随机采样的 <span class="math inline">\(M\)</span>个 architecture</li><li>Validation：挑选有着 top-k accuracy 的 architecture，生成architecture-accuracy pairs 加入下次 iteration 训练 predictor的训练集里。</li></ol><h1 id="gbdt-nas-3s">GBDT-NAS-3S</h1><p>　　用 GBDT 来给 search space 剪枝是很直观的，因为 GBDT 本来就有计算feature importance 的功能，而 search space 用 one-hot 表示后，每个 op是否使用其实就可以看作一个特征，通常 feature importance有两种常见的计算方法：</p><ul><li>weight：权重，某特征在整个树群节点中出现的次数，出现越多，价值就越高</li><li>gain：某特征在整个树群作为分裂节点的信息增益之和再除以某特征出现的频次</li></ul><p>　　然而作者没有简单的使用这两种，而是使用了 SHapley AdditiveexPlanation (SHAP)，可以衡量 GBDT 预测中某个 feature对结果（在我们的场景下就是 architectureaccuracy）的有正贡献或负贡献，然后选取很低 SHAP 值的 feature进行剪枝。例如 layer_1_is_conv1x1=1 产生了很大的负贡献（例如-0.2），我们就可以在 search space 里将layer_1_is_conv1x1=0，这样便达到了剪枝的作用。</p><blockquote><p>SHAP是用 Python开发的一个“模型解释”包，可以解释任何机器学习模型的输出。在合作博弈论的启发下SHAP构建一个加性的解释模型，所有的特征都视为“贡献者”。对于每个预测样本，模型都产生一个预测值，SHAP值就是该样本中每个特征所分配到的数值。相关介绍可以看<a href="https://zhuanlan.zhihu.com/p/83412330">这里</a>，<a href="https://github.com/slundberg/shap">源码</a>。</p></blockquote><p>　　我们上面说的是 first-order pruning，它的算法伪代码如下：</p><p><img src="/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/2.png"></p><p>　　first-order pruning 它每次只针对一个 feature 计算 SHAP值判断是否剪枝，然而有时两个 op 间可能存在关联，计算两个 feature 关联的SHAP 值可能更具有说服力，所以作者 second-order pruning，它计算任意两个feature 之间的 interaction SHAP 值，算法伪代码如下：</p><p><img src="/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/3.png"></p><p>　　作者将带剪枝的算法命名为 GBDT-NAS-3S(GBDT-NAS enhanced withsearch space search，算法伪代码如下：</p><p><img src="/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/1.png"></p><h1 id="experiments">Experiments</h1><p>　　主要展示两个实验，第一个和同类方法比较 predictor 性能，用的是NASBench-101 这个数据集，采用 pairwise accuracy <span class="math inline">\(\frac{\sum_{x_{1} \in X, x_{2} \in X}\mathbb{1}_{f\left(x_{1}\right) \geq f\left(x_{2}\right)}\mathbb{1}_{y_{1} \geq y_{2}}}{|X|(|X|-1) / 2}\)</span>：</p><p><img src="/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/4.png"></p><p>　　第二个是比较 NAS 性能, 主要在ImageNet比较,搜出来的性能不错但是参数量不少</p><p><img src="/blog/2020/07/27/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E5%9B%9B%EF%BC%89-GDBT-NAS/5.png"></p><h1 id="conclusion">Conclusion</h1><p>　　之前的工作使用基于 NN 的预测器，但它无法很好地利用 architecture类似 tabular 数据格式。因此作者提出利用 GBDT 作为 NAS的预测指标，并且可以通过 GBDT 根据 SHAP 值来对 search space进行剪枝。</p><h1 id="refer">Refer</h1><ul><li>[1] R. Luo, X. Tan, R. Wang, T. Qin, E. Chen, and T.-Y. Liu, “NeuralArchitecture Search with GBDT,” 2020.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　GDBT-NAS&lt;sup&gt;[1]&lt;/sup&gt; 由 MSRA 在 2020 年提出，它主要用 GBDT 作为
NAS 算法中预测 candidate architecture 的 predictor，同时它还将 GBDT 作为
search space 的 pruner ，思想还是比较简单的，本文对它做简单记录。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="GDBT-NAS" scheme="http://chengfeng96.com/tags/GDBT-NAS/"/>
    
    <category term="GBDT" scheme="http://chengfeng96.com/tags/GBDT/"/>
    
    <category term="SHAP" scheme="http://chengfeng96.com/tags/SHAP/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十三）- NASP</title>
    <link href="http://chengfeng96.com/blog/2020/07/24/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89-NASP/"/>
    <id>http://chengfeng96.com/blog/2020/07/24/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89-NASP/</id>
    <published>2020-07-24T14:21:11.000Z</published>
    <updated>2022-01-03T03:40:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　最近读了一篇 WWW2020 的论文<sup>[1]</sup>，它在协同过滤中应用NAS，其中提出的 SIF 算法本文就不做介绍了。本文介绍 SIF 里用到的 NAS 算法NASP <sup>[2]</sup>，这两篇文章的作者都是第四范式的一个大佬，NASP 发表在AAAI2020，本文对它作一个简单的笔记。</p><span id="more"></span><h1 id="darts-的缺陷">Darts 的缺陷</h1><p>　　NASP 针对的主要是 DARTS 的一些缺陷，作者在论文中列出了 DARTS的一些缺陷：</p><ul><li>Search efficiency：DARTS 的 supernet 和 softmax 的计算方式导致search space 里的所有 operations 都需要在计算 <span class="math inline">\(w\)</span> 的时候做 forward 和backward-propagated，而且还涉及到一些二阶梯度的计算，这是非常大的一个开销。</li><li>Architecture performance：因为 architecture 是通过 softmax后的概率得到的，所以各个 operation 之间可能存在一些correlation，就比如针对一条边上有三个候选 operation，它们的概率分别是0.3，0.3 和 0.4，你很难说这三个 operation 的性能差距很大，甚至概率小的operation 会有更好的性能。</li><li>Model complexity：DARTS 在损失函数上并不能有效限制模型的大小。</li></ul><h1 id="nasp">NASP</h1><p>　　针对 DARTS 的缺陷，NASP 提出一些改进方法，下面分别介绍下。</p><h2 id="proximal-algorithm-pa">Proximal Algorithm (PA)</h2><p>　　作者在论文中说，NASP 是在 NAS 领域中首先应用 PA算法的，也就是近端梯度算法。PA 算法的关键步骤是：</p><p><span class="math display">\[\mathbf{x}=\operatorname{prox}_{\mathcal{S}}(\mathbf{z})=\arg \min_{\mathbf{y}} 1 / 2\|\mathbf{y}-\mathbf{z}\|_{2}^{2} \text { s.t. }\mathbf{y} \in \mathcal{S}\]</span></p><p>　　然后不断迭代：</p><p><span class="math display">\[\mathbf{x}_{t+1}=\operatorname{prox}_{\mathcal{S}}\left(\mathbf{x}_{t}-\varepsilon\nabla f\left(\mathbf{x}_{t}\right)\right)\]</span></p><p>　　PA 算法也有一种变种叫做 lazy proximal step：</p><p><span class="math display">\[\overline{\mathbf{x}}_{t}=\operatorname{prox}_{\mathcal{S}}\left(\mathbf{x}_{t}\right),\quad \mathbf{x}_{t+1}=\mathbf{x}_{t}-\varepsilon \nablaf\left(\overline{\mathbf{x}}_{t}\right)\]</span></p><p>　　NASP 中应用 PA 的目的主要是为了处理后面加的 discrete 的限制，利用PA 可以进行相应的优化。</p><h2 id="search-objective">Search Objective</h2><p>　　NASP 有一个很重要的思想就是在 search 的时候保持 search space 是differentiable，但是训练模型的时候又让 architecture 是 discrete的，其实做法也很简单，也就是在训练时把每条 edge 上的所有 op 的 logits取一个 argmax，这个 op 的权重赋值成 1，其它 op 的权重赋值成0，这就是一个 discrete 的过程，这样在训练 child model 的时候，每条 edge就只会用到一个 op 了，在训练完 child model 后再把 edge 上所有 op的权重恢复成取 argmax 之前的数值，再做梯度下降。论文中也画了一张图来表现NASP 与其它算法的区别：</p><p><img src="/blog/2020/07/24/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89-NASP/1.png"></p><p>　　具体来说，discrete 的限制可以写成：</p><p><span class="math display">\[\begin{aligned}&amp;\bar{O}^{(i, j)}\left(x^{(i)}\right)=\sum_{k=1}^{|\mathcal{O}|}a_{k}^{(i, j)} \mathcal{O}_{k}\left(x^{(j)}\right)\\&amp;\text { s.t. } \mathbf{a}^{(i, j)} \in \mathcal{C}\end{aligned}\]</span></p><p>　　<span class="math inline">\(\mathcal{C}\)</span> 的限制也很简单<span class="math inline">\(\left\{\mathbf{a} \mid\|\mathbf{a}\|_{0}=1,\text { and } 0 \leq a_{k} \leq 1\right\}\)</span>，它通过 0范数保证了只能有一个 op 的权重大于 0。 　　为了控制模型的复杂度，NASP又设计了一个正则项： <span class="math display">\[\mathcal{R}(\mathbf{A})=\sum_{k=1}^{|\mathcal{O}|} p_{k} /\bar{p}\left\|\mathbf{\dot { a }}_{k}\right\|_{2}^{2} \\\bar{p}=\sum_{i=1}^{|\mathcal{O}|} p_{i}\]</span></p><p>　　<span class="math inline">\(\mathbf{\dot { a }}_{k}\)</span> 是<span class="math inline">\(\mathbf{A}\)</span> 里的第 <span class="math inline">\(k\)</span> 列，<span class="math inline">\(p_i\)</span> 表示第 <span class="math inline">\(i\)</span> 个 op的参数量，思想其实很简单，就是针对每个 op 的参数量做一个加权。　　所以整个 search objective 可以写成： <span class="math display">\[\min _{\mathbf{A}} \mathcal{F}\left(w^{*}, \mathbf{A}\right), \mathbf{s. t .}\left\{\begin{array}{l}w^{*}=\arg \min \mathcal{L}_{\text {train }}(w, \mathbf{A}) \\\mathbf{a}^{(i, j)} \in \mathcal{C}\end{array}\right. \\\mathcal{F}(w, \mathbf{A})=\mathcal{L}_{\mathrm{val}}(w,\mathbf{A})+\eta \mathcal{R}(\mathbf{A})\]</span></p><p>　　越大的 <span class="math inline">\(\eta\)</span> 往往意味 search到一个越小的 architecture。</p><h2 id="search-algorithm">Search Algorithm</h2><p>　　如果我们在搜索的过程里应用 PA 算法：</p><p><span class="math display">\[\mathbf{A}_{t+1}=\operatorname{prox}_{\mathcal{C}}\left(\mathbf{A}_{t}-\varepsilon\nabla_{\overline{\mathbf{A}}_{t}}\mathcal{F}\left(w\left(\mathbf{A}_{t}\right),\mathbf{A}_{t}\right)\right)\]</span></p><p>　　可以看出依然需要计算二阶梯度，开销依然很大，或者应用 lazyproximal step：</p><p><span class="math display">\[\begin{aligned}\overline{\mathbf{A}}_{t}&amp;=\operatorname{prox}_{\mathcal{C}}\left(\mathbf{A}_{t}\right) \\\mathbf{A}_{t+1} &amp;=\mathbf{A}_{t}-\varepsilon\nabla_{\overline{\mathbf{A}}_{t}}\mathcal{F}\left(w\left(\overline{\mathbf{A}}_{t}\right),\overline{\mathbf{A}}_{t}\right)\end{aligned}\]</span></p><p>　　然而依然不可行，以为这并不能保证 <span class="math inline">\(\mathbf{A}_t\)</span> 是在 <span class="math inline">\([0,1]\)</span> 之间的。 　　因此作者提出了 NASP算法，伪代码如下：</p><p><img src="/blog/2020/07/24/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89-NASP/2.png"></p><p>　　NASP 主要基于一个发现 <span class="math inline">\(\operatorname{prox}_{\mathcal{C}}(\mathbf{a})=\operatorname{prox}_{\mathcal{C}_{1}}\left(\operatorname{prox}_{\mathcal{C}_{2}}(\mathbf{a})\right)\)</span>，其中<span class="math inline">\(\mathcal{C}_{1}=\left\{\mathbf{a}\mid\|\mathbf{a}\|_{0}=1\right\}\)</span>（作者开源的源码里主要就是通过argmax 得到，然后二值化） 和 <span class="math inline">\(\mathcal{C}_{2}=\left\{\mathbf{a} \mid 0 \leqa_{k} \leq1\right\}\)</span>（作者开源的源码里主要就是通过硬剪裁得到），这个发现具体的证明如下：</p><p><img src="/blog/2020/07/24/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%89%EF%BC%89-NASP/3.png"></p><p>　　同时，我们可以发现 NASP是没有计算二阶梯度的，论文中给出的解释这样的：</p><blockquote><p>Specifically, in step 3, discretized version of architectures aremore stable than the continuous ones in DARTS, as it is less likely forsubsequent updates in <span class="math inline">\(w\)</span> to change<span class="math inline">\(\overline{\mathbf{A}}\)</span>. Thus, we cantake wt (step 4) as a constant w.r.t. <span class="math inline">\(\overline{\mathbf{A}}\)</span>, which helps usremove the second order approximation in (6) and significantly speed uparchitectures updates.</p></blockquote><p>　　大致意思就是 discrete 的 architecture 要比 continuous 的architecture 更加稳定，不需要在更新 <span class="math inline">\(w\)</span> 的时候再更新architecture，因此去掉二阶梯度并不会带来精度上的过大损失，反而能大大加速模型的搜索过程。论文中给出NASP 要比 STOA 有着 10 倍以上的加速。</p><h1 id="conclusions">Conclusions</h1><p>　　阅读完 NASP，还是挺有启发的，特别是 search space 之间 discrete 和continuous 的转换，除此之外作者还应有了 PA算法，但是我不太懂这一个应用的创新，阅读了作者开源的源码，感觉也就是在做梯度下降，作者开源的NASP 源码包括 SIF 的源码和 DARTS的源码有大部分基本是一样的，可以看作是在 DARTS 工作上的一个拓展吧。</p><h1 id="refer">Refer</h1><ul><li>[1] Q. Yao, X. Chen, J. T. Kwok, Y. Li, and C.-J. Hsieh, “EfficientNeural Interaction Function Search for Collaborative Filtering,” inProceedings of The Web Conference 2020, 2020, pp. 1660–1670.</li><li>[2] Q. Yao, J. Xu, W.-W. Tu, and Z. Zhu, “Efficient NeuralArchitecture Search via Proximal Iterations,” 2020.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　最近读了一篇 WWW2020 的论文&lt;sup&gt;[1]&lt;/sup&gt;，它在协同过滤中应用
NAS，其中提出的 SIF 算法本文就不做介绍了。本文介绍 SIF 里用到的 NAS 算法
NASP &lt;sup&gt;[2]&lt;/sup&gt;，这两篇文章的作者都是第四范式的一个大佬，NASP 发表在
AAAI2020，本文对它作一个简单的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="DARTS" scheme="http://chengfeng96.com/tags/DARTS/"/>
    
    <category term="NASP" scheme="http://chengfeng96.com/tags/NASP/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十二）- SemiNAS</title>
    <link href="http://chengfeng96.com/blog/2020/07/18/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89-SemiNAS/"/>
    <id>http://chengfeng96.com/blog/2020/07/18/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89-SemiNAS/</id>
    <published>2020-07-18T14:45:49.000Z</published>
    <updated>2022-01-03T03:40:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　SemiNAS<sup>[1]</sup> 由微软在 CVPR 2020上提出，它结合了半监督学习，并且论文中主要利用微软之前提出的 NAO来结合半监督学习，并且首次把 NAS 应用到 Text To Speech(TTS) 任务上。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　我们知道传统的 NAS 方法通常包括一个 controller 和一个evaluator，其中 controller 负责生成不同的 architecture，然后交由evaluator 进行评估。evaluator 需要对 architecture进行训练，然后获得其在验证集上的准确率，并返回给 controller， controller利用 architecture及其对应的准确率进行学习，进而生成更好的网络结构。在这一过程里，evaluator对 architecture 进行评估的过程非常耗时，因为其需要对每个 architecture进行训练，有的是利用了 weight sharing，有的直接 train from scratch，而controller 的学习又需要尽可能多的 architecture和准确率数据对作为训练数据，从而使得整个搜索过程中 evaluator的耗时非常高。SemiNAS 的动机就是为了加速这一过程。</p><h1 id="seminas">SemiNAS</h1><p>　　SemiNAS 不仅加速 controller的训练过程，并且同时提升搜索精度。它的思想非常简单，首先随机采样一些architecture，用较为足够的时间去训练它们，然后通过 evaluator的得到准确度，一次拿到一条监督样本。然后用这些精确度作为标签的监督样本去训练controller，接着再采样获得大量无监督数据（只有 architecture没有对应的准确率），然后用 evaluater去估计一个准确度，生成新的有标签的样本，这其实也就是一个半监督学习的过程，非常简单。获取无监督数据的过程的开销是非常小的，因为只需要controller 去随机采样一些 architecture 即可。<br>　　所以 SemiNAS 就需要一个额外的性能预测器 <span class="math inline">\(f_p\)</span>，有了这个 <span class="math inline">\(f_p\)</span>，SemiNAS 就可以和多种 NAS方法相结合，例如基于强化学习的算法（NASNet，ENAS）和基于进化算法的算法（AmoebaNet，SPOS），<span class="math inline">\(f_p\)</span> 可以用于为生成的候选 architecture预测准确率。对于基于梯度的算法（Darts，NAO），可以直接用 <span class="math inline">\(f_p\)</span> 预测的 architecture 的准确率对architecture 求导，更新 architecture。<br>　　论文中作者结合了他们之前提出的 NAO，NAO的思想也比较简单，主要就包括一个 encoder <span class="math inline">\(f_e\)</span>，一个 predictor <span class="math inline">\(f_p\)</span> 和一个 decoder <span class="math inline">\(f_d\)</span>，可以看看我之前写的，这里简单提几个公式。首先是<span class="math inline">\(f_p\)</span> 的 MSE：</p><p><span class="math display">\[\begin{array}{l}e_{x}=f_{e}(x) \\\mathcal{L}_{p}=\left(y-f_{p}\left(e_{x}\right)\right)^{2}\end{array} \tag{1}\]</span></p><p>　　生成伪标签：</p><p><span class="math display">\[\hat{y}_{i}=f_{p}\left(f_{e}\left(\hat{x}_{i}\right)\right), i=1,2,\ldots, M \tag{2}\]</span></p><p>　　训练 controller 的 loss：</p><p><span class="math display">\[\mathcal{L}_{\text {total}}=\lambda \mathcal{L}_{p}+(1-\lambda)\mathcal{L}_{r e c} \tag{3}\]</span></p><p>　　利用梯度下降搜索更优的 architecture：</p><p><span class="math display">\[e_{x}^{\prime}=e_{x}+\eta \frac{\partialf_{p}\left(e_{x}\right)}{\partial e_{x}} \tag{4}\]</span></p><p>　　算法的伪代码如下：</p><p><img src="/blog/2020/07/18/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%BA%8C%EF%BC%89-SemiNAS/1.png"></p><h1 id="conclusion">Conclusion</h1><p>　　SemiNAS 的思想还是非常简单的，特别是结合了 NAO后，首先用少量有标签的数据训练，然后从搜索空间中采样获得大量无标签的神经网络结构，利用训练好的框架对这些网络结构预测精确率。然后使用原始的有标签的数据和自标注好的无标签数据一起充分训练整个框架。之后按照NAO中的方法进行优化，生成更好的网络结构。我也有一个小小的疑问，就是如果引入的无标签数据的标签质量不行的话，不是会引入大量噪声吗，会不会反而拖累controller 的性能，感觉再引入无标签的数据时还可以作作文章。</p><h1 id="refer">Refer</h1><ul><li>[1] R. Luo, X. Tan, R. Wang, T. Qin, E. Chen, and T.-Y. Liu,“Semi-Supervised Neural Architecture Search,” 2020.-Shot NAS with GreedySupernet,” 2020.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　SemiNAS&lt;sup&gt;[1]&lt;/sup&gt; 由微软在 CVPR 2020
上提出，它结合了半监督学习，并且论文中主要利用微软之前提出的 NAO
来结合半监督学习，并且首次把 NAS 应用到 Text To Speech(TTS) 任务上。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="NAO" scheme="http://chengfeng96.com/tags/NAO/"/>
    
    <category term="SemiNAS" scheme="http://chengfeng96.com/tags/SemiNAS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十一）- GreedyNAS</title>
    <link href="http://chengfeng96.com/blog/2020/07/16/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89-GreedyNAS/"/>
    <id>http://chengfeng96.com/blog/2020/07/16/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89-GreedyNAS/</id>
    <published>2020-07-16T15:04:21.000Z</published>
    <updated>2022-01-03T03:39:58.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　商汤在 CVPR 2020 上提出了 GreedyNAS<sup>[1]</sup>，它也是一种One-Shot NAS，优点是对 supernet 做了贪心操作，本文对它做简单的笔记。</p><span id="more"></span><h1 id="motivation">Motivation</h1><p>　　我们知道在 One-Shot 模型里，supernet的训练质量是至关重要的，因为在搜索的时候采样的 child architecture不会再训练，直接 infer。我们之前介绍过的 Single Path One-Shot 和 FairNAS方法都是尽可能的想让 sueprnet 训练时对每个采样都公平点，将 supernet中每一个 architecture 认为 是同等重要的，supernet应该对每个结构进行准确评估或相对排序。然而supernet中所包含的搜索空间的是非常巨大的，想要准确的评估对于 supernet来说是非常困难的，会导致 supernet中结构的表现与其真实表现相关性很差，要求 supernet正确预测所有路径准确率过于严苛。并且由于训练超网过程中所有路径高度共享，训练不好的路径可能对好的路径造成干扰。<br>　　因此 GreedyNAS试图解决这些问题，主要思路就是使用多路径采样策略过滤不好的路径，使supernet 训练更加聚焦潜在优异的路径。</p><h1 id="greedy-path-filtering">Greedy path filtering</h1><p>　　作者将 supernet 的搜索空间作如下定义：</p><p><span class="math display">\[\mathcal{A}=\mathcal{A}_{\text {good}} \bigcup \mathcal{A}_{\text{weak}}, \mathcal{A}_{\text {good}} \bigcap \mathcal{A}_{\text{weak}}=\emptyset \tag{1}\]</span></p><p>　　<span class="math inline">\(\mathcal{A}_{\text {good}}\)</span>中的 path <span class="math inline">\(a\)</span> 都要比 <span class="math inline">\(\mathcal{A}_{\text {weak}}\)</span> 中的 paht<span class="math inline">\(b\)</span> 要好，即：</p><p><span class="math display">\[\operatorname{ACC}\left(\boldsymbol{a}, \mathcal{N}_{o}, \mathcal{D}_{va l}\right) \geq \operatorname{ACC}\left(\boldsymbol{b},\mathcal{N}_{o}, \mathcal{D}_{v a l}\right)\]</span></p><p>　　其中 <span class="math inline">\(\mathcal{N}_{o}\)</span>是未知的 supernet。</p><h2 id="multi-path-sampling-with-rejection">Multi-path sampling withrejection</h2><p>　　我们的目标就是在训练 supernet 时从 <span class="math inline">\(\mathcal{A}_{\text {good}}\)</span> 采样path，即：</p><p><span class="math display">\[p\left(\boldsymbol{a} ; \mathcal{N}_{o}, \mathcal{D}_{v al}\right)=\frac{1}{\left|\mathcal{A}_{g o o d}\right|}\mathbb{I}\left(\boldsymbol{a} \in \mathcal{A}_{g o o d}\right)\]</span></p><p>　　但是因为 supernet 是未知的，我们很难分出 <span class="math inline">\(\mathcal{A}_{\text {good}}\)</span> 和 <span class="math inline">\(\mathcal{A}_{\text{weak}}\)</span>，遍历搜索空间一遍开销肯定是不能接受的，所以作者提出了一种多路径拒绝式采样方法。<br>　　我们定义 <span class="math inline">\(q = \left|\mathcal{A}_{g o od}\right| /|\mathcal{A}|\)</span>，则从 <span class="math inline">\(\mathcal{A}_{\text {weak}}\)</span> 的概率就是<span class="math inline">\(1-q\)</span>，那么假设我们采样 <span class="math inline">\(m\)</span> 个 path，则至少有 <span class="math inline">\(k\)</span> 个 path 来自 <span class="math inline">\(\mathcal{A}_{\text {good}}\)</span> 的概率就是：<span class="math display">\[\sum_{j=k}^{m} \mathbb{C}_{m}^{j} q^{j}(1-q)^{m-j}\]</span></p><p>　　当 <span class="math inline">\(q\)</span> 较大或者 <span class="math inline">\(k\)</span>较小时这个概率都是很大的，所以作者每次就简单的挑选 top-<span class="math inline">\(k\)</span> 的 path 去训练。<br>　　因为挑选 top-<span class="math inline">\(k\)</span> 又涉及到了在<span class="math inline">\(D_{val}\)</span> 上训练得到 ACC去排序，这个操作也是开销比较大的，因次作者仅使用 <span class="math inline">\(D_{val}\)</span> 的一个子集 <span class="math inline">\(\mathcal{D}_{v a l}\)</span> 来训练得到 loss去做排序，整个过程的伪代码如下：</p><p><img src="/blog/2020/07/16/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89-GreedyNAS/1.png"></p><h1 id="greedy-training-of-supernet">Greedy training of supernet</h1><h2 id="training-with-exploration-and-exploitation">Training withexploration and exploitation</h2><p>　　因为我们每次做完 greedy path filtering 后得到一些可能比较好的path，它们一次训练可能不够充分，有重复训练的必要，所以这里作者也提出了一个exploration 和 exploitation 的 trade-off，即使用一个 candidate pool<span class="math inline">\(\mathcal{P}\)</span> 来存储这些比较好的path，并且它其实是一个优先队列，priority 是每个 path 做 evaluation 时的loss，下次训练时采取如下的采样：</p><p><span class="math display">\[\boldsymbol{a} \sim(1-\epsilon) \cdot U(\mathcal{A})+\epsilon \cdotU(\mathcal{P})\]</span></p><p>　　因为 supernet 在训练初期是训练不充分的，每个 path 得到的 loss不可信，也就是 priority 不可信，所以作者将 <span class="math inline">\(\epsilon\)</span> 从 0 开始逐渐升大。<br>　　这样一种采样方式其实也提高了我们从 <span class="math inline">\(\mathcal{A}_{\text {good}}\)</span>中采样的概率：</p><p><span class="math display">\[q=\epsilon+(1-\epsilon)\left|\mathcal{A}_{g o o d}\right| / \mid\mathcal{A}\]</span></p><h2 id="stopping-principle-via-candidate-pool-different">Stoppingprinciple via candidate pool Different</h2><p>　　作者采取一种自适应的条件来终止训练，主要依据 candidate pool的大小有没有稳定了：</p><p><span class="math display">\[\pi:=\frac{\left|\mathcal{P}_{t} \cap \mathcal{P}\right|}{|\mathcal{P}|}\leq a\]</span> 　　<span class="math inline">\(\mathcal{P}_{t}\)</span>这一次迭代前的 candidate pool。整个训练的伪代码如下：</p><p><img src="/blog/2020/07/16/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89-GreedyNAS/2.png"></p><h2 id="searching-with-candidate-pool">Searching with candidatepool</h2><p>　　supernet 训练完毕后，使用 NSGA-II进化算法在超网中搜索符合条件的最优模型，并且使用 candidate pool 初始化Population，相较于随机初始化，借助于候选池能够使进化算法有一个更好的初始，提升搜索效率及最终的精度能得到更好的模型分布：</p><p><img src="/blog/2020/07/16/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%E4%B8%80%EF%BC%89-GreedyNAS/3.png"></p><h1 id="conclusion">Conclusion</h1><p>　　商汤的这篇论文解决的核心问题就是让 supernet 更注重于有潜力的好path 的训练，并且使用了贪心算法，提出了 multi-path sampling withrejection 和 candidate pool 的方法，还是非常让人有启发的。</p><h1 id="refer">Refer</h1><ul><li>[1] S. You, T. Huang, M. Yang, F. Wang, C. Qian, and C. Zhang,“GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet,” 2020.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　商汤在 CVPR 2020 上提出了 GreedyNAS&lt;sup&gt;[1]&lt;/sup&gt;，它也是一种
One-Shot NAS，优点是对 supernet 做了贪心操作，本文对它做简单的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="One-Shot" scheme="http://chengfeng96.com/tags/One-Shot/"/>
    
    <category term="Weight Sharing" scheme="http://chengfeng96.com/tags/Weight-Sharing/"/>
    
    <category term="GreedyNAS" scheme="http://chengfeng96.com/tags/GreedyNAS/"/>
    
    <category term="NSGA-II" scheme="http://chengfeng96.com/tags/NSGA-II/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（十）- Fair DARTS</title>
    <link href="http://chengfeng96.com/blog/2020/07/13/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89-Fair-DARTS/"/>
    <id>http://chengfeng96.com/blog/2020/07/13/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89-Fair-DARTS/</id>
    <published>2020-07-13T14:57:25.000Z</published>
    <updated>2022-01-03T03:39:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　Fair DARTS<sup>[1]</sup> 由小米在 ECCV 2020上提出，主要是为了解决 DARTS 中的两个缺陷，本文对它做简单的笔记。</p><span id="more"></span><h1 id="darts-的缺陷">DARTS 的缺陷</h1><p>　　作者上来就提出了 DARTS 的两个问题：</p><ul><li>搜索过程中 skip connection 在优化后期出现过多</li><li>continuous 的 op 参数 discrete 后存在 gap</li></ul><h2 id="skip-connections-的缺陷">Skip Connections 的缺陷</h2><p>　　第一个缺陷就是训练后期收敛后一个 cell 里 skip connection的过多。并且作者认为本文认为 skip connection过多出现有两个不可或缺的条件：exclusive competition（两个 node之间只能选 1 个 op）和 unfairadvantage（作者文中给出了定义，在优化过程中对 supernet 的 contribution大于对最终 discrete 的网络 performance 的contribution，即优化过程中虚报了，实际并没那么强）。skip connection和其他 op 形成了类似 residual 结构，在 ResNet中已经明确指出了跳跃连接在深层网络的训练过程中中起到了良好的梯度疏通效果，进而有效减缓了梯度消失现象。因此，在超网络的搜索训练过程中，skipconnections 可以借助其他操作的关系达到疏通效果，使得 skip connections相较于其他操作存在不公平优势。</p><h2 id="discretization-的缺陷">Discretization 的缺陷</h2><p>　　这个缺陷就很好理解了，就是 continuous space 想要根据 softmax 值去argmax 来 discrete 时，softmax 值并没有很强的区分度，就比如一组 op 的softmax 值为 [0.174, 0.170, 0.176, 0.112, 0.116, 0.132, 0.118]，你很难说0.176 的 op 就比 0.174 的 op 好。</p><h1 id="fair-darts">Fair DARTS</h1><h2 id="缓解-skip-connections-的缺陷">缓解 Skip Connections 的缺陷</h2><p>　　通过上面的叙述，我们知道要想解决 skip connection 带来的collapse，只需要打破其中任意一个条件即可。Fair DARTS 基本使用采用打破exclusive competition 这个策略，将 softmax 换成 sigmoid 函数，这样即使有skip connection 的值饱和到 1，依然有可能有其他 op 的值饱和到1，就形成了一个类似 multi-hot 的 approximation，给予每个 op独立的结构化参数，多个 op 之间不会相互抑制。<br>　　对于 unfairadvantage，论文结尾给了一个加高斯随机噪声实验，仍能得到不错的结果，这也就孕育了后面的Noise DARTS。</p><h2 id="缓解-continuous-和-discrete-表示的差异">缓解 Continuous 和Discrete 表示的差异</h2><p>　　为了减小 continuous 到 discrete 的 gap，作者添加了辅助的 0-1损失，作者也给这个损失函数提出了三个条件：</p><ul><li>It needs to have a global maximum at z = 0.5 (a fair starting point)and a global minimum at 0 and 1.</li><li>The gradient magnitude <span class="math inline">\(\frac{d f}{d z}\mid z \approx 0.5\)</span> has to be adequately small to allowarchitectural weights to fluctuate, but large enough to attract ztowards 0 or 1 when they are a bit far from 0.5.</li><li>It should be differentiable for backpropagation.</li></ul><p>　　即在公平条件下，我们得以将不同 op 的权重参数推向 0 或1，扩大相对差异，即要么靠近 0 要么靠近 1，这样也就减少了 discrete 后的gap了。很自然的，我们可以这样定义：</p><p><span class="math display">\[L_{0-1}=-\frac{1}{N}\sum_{i}^{N}\left(\sigma\left(\alpha_{i}\right)-0.5\right)^{2}\]</span></p><p>　　总的目标函数就为：</p><p><span class="math display">\[\begin{aligned}&amp;\min _{\alpha} \mathcal{L}_{v a l}\left(w^{*}(\alpha),\alpha\right)+w_{0-1} L_{0-1}\\&amp;\text { s.t. } w^{*}(\alpha)=\operatorname{argmin}_{w}\mathcal{L}_{\text {train}}(w, \alpha)\\&amp;\bar{o}_{i, j}(x)=\sum_{o \in \mathcal{O}}\sigma\left(\alpha_{o_{i, j}}\right) o(x)\end{aligned}\]</span></p><h1 id="experiments">Experiments</h1><p>　　具体的实验数据这里就不说了，上一张论文中 Fair DARTS 解决 skipconnection 占 dominate 和 exclusive 的 heat map图，实验的效果就一目了然了：</p><p><img src="/blog/2020/07/13/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%8D%81%EF%BC%89-Fair-DARTS/1.png"></p><h1 id="conclusion">Conclusion</h1><p>　　总的来说，Fair DARTS 提出的 DARTS 中 exclusive competition 和unfair advantage的缺陷还是非常有趣的，解决的方法也不难，是一篇很值得学习的论文。</p><h1 id="refer">Refer</h1><ul><li>[1] H. Liu, K. S. Deepmind, and Y. Yang, “DARTS: DIFFERENTIABLEARCHITECTURE SEARCH.”</li><li><a href="https://zhuanlan.zhihu.com/p/94136888">FairDARTS：消除不公平优势的 DARTS</a></li><li><a href="https://zhuanlan.zhihu.com/p/158908387">神经网络架构搜索——可微分搜索（Fair-DARTS）</a></li><li><a href="https://zhuanlan.zhihu.com/p/157415268">ECCV 2020 |小米提出 Fair DARTS ：公平的可微分神经网络搜索</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　Fair DARTS&lt;sup&gt;[1]&lt;/sup&gt; 由小米在 ECCV 2020
上提出，主要是为了解决 DARTS 中的两个缺陷，本文对它做简单的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="DARTS" scheme="http://chengfeng96.com/tags/DARTS/"/>
    
    <category term="Fair DARTS" scheme="http://chengfeng96.com/tags/Fair-DARTS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（九）- FairNAS</title>
    <link href="http://chengfeng96.com/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/"/>
    <id>http://chengfeng96.com/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/</id>
    <published>2020-07-11T15:18:35.000Z</published>
    <updated>2022-01-03T03:39:22.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　FairNAS <sup>[1]</sup> 由小米在 ICCV 2021上发表，它和我们上文介绍的 SPOS 非常像，不同的只是将其中的 uniformsampling 改成了公平采样，本文对它做简单的笔记。</p><span id="more"></span><h1 id="fairness">Fairness</h1><p>　　本文主要要解决的问题是训练 child architecture 时产生的 unfairness的问题，ENAS 和 DARTS 之类的算法都是偏向于初始性能好的 childarchitecture，在后面的训练中它们更可能被采样，这便是 unfairbias，因此如果 supernet 因为 unfair bias 错误估计了 child architecture的性能，也就是对 candidate child architecture 的 rank出现问题时，会大大影响我们的搜索过程，造成unfairness，难以搜索出真正最优的 child architecture。<br>　　作者为了缓解这个问题，提出两种 fairness 的机制，Expectation Fairness和 Strict Fairness。<br>　　FairNas 的 supernet 结构和 SPOS 是类似的，都是 singlepath，每层里采样一个 choice block，我们定义训练过程为 <span class="math inline">\(P(m, n, L)\)</span>，其中 <span class="math inline">\(m\)</span> 指的每层里 choice block 的个数，<span class="math inline">\(n\)</span> 指的是 weights 被更新的次数，<span class="math inline">\(L\)</span> 指的是 supernet 的层数。</p><h2 id="expectation-fairness">Expectation Fairness</h2><p>　　顾名思义，Expectation Fairness　要保证的就是，所有的 m 种 choiceblock 在更新 <span class="math inline">\(n\)</span>次后都能有相同的期望，它的定义如下：</p><p><img src="/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/1.png"></p><p>　　SPOS 中采用的 uniform sampling 是满足这个约束的，且：</p><p><span class="math display">\[\begin{aligned}E\left(Y_{l_{i}}\right) &amp;=n * p_{l_{i}}=n / m \\\operatorname{Var}\left(Y_{l_{i}}\right) &amp;=n *p_{l_{i}}\left(1-p_{l_{i}}\right)=\frac{n(m-1)}{m^{2}}\end{aligned} \tag{1}\]</span></p><p>　　但即使如此，SPOS 中也存在 unfairness，它主要是一个 orderissue，即如果我们在训练 supernet 时采样了一组 architecture <span class="math inline">\(\{M_1, M_2, M_3\}\)</span>，训练 <span class="math inline">\(M_2\)</span> 的时候，已经用到了受到了训练完 <span class="math inline">\(M_1\)</span>造成的影响了，并且如果学习率还不同的话，会产生更复杂的情况。<br>　　这里作者还指出了一个反逻辑的事实，也就是 <span class="math inline">\(n\)</span> 趋近无穷时时，每个 choice block的采样数不可能一样，数学化就是： <span class="math display">\[\operatorname{Regarding} P(m, n, L), \forall n \in\left\{x: x \% m=0, x\in N_{+}\right\}, \lim _{n \rightarrow+\infty} p\left(Y_{l 1}=\right.\left.Y_{l 2}=\ldots=Y_{l m}\right)=0\]</span></p><p>　　它的证明如下：</p><p><span class="math display">\[Let \quad f(m, n)=p\left(Y_{l 1}=Y_{l 2}=\ldots=Y_{l m}\right) \\f(m, n)=C_{n}^{\frac{n}{n}} C_{\frac{n(m-1)}{m}}^{\frac{n}{m}} \ldots C\frac{\frac{n}{m}}{\frac{n}{m}} \frac{1}{m^{n}}=\frac{n!}{\left(\frac{n}{m} !\right)^{m}} \frac{1}{m^{n}} \\\begin{aligned} \lim _{n \rightarrow+\infty} f(m, n) &amp;=\lim _{n\rightarrow+\infty} \frac{n !}{\left(\frac{n}{m} !\right)^{m} \timesm^{n}} \\ &amp;=\lim _{n \rightarrow+\infty} \frac{\sqrt{2 \pin}\left(\frac{n}{e}\right)^{n}}{\sqrt{2 \pi\frac{n}{m}}^{m}\left(\frac{n}{e}\right)^{n}} \\ &amp;=\lim _{n\rightarrow+\infty} \frac{\sqrt{m}}{\frac{2 \pi n}{m} \frac{m-1}{2}} \\&amp;=0 \end{aligned}\]</span></p><p>　　证明中用到了 Stirling’s approximation。</p><h2 id="strict-fairness">Strict Fairness</h2><p>　　针对 Expectation Fairness 的问题，作者提出了 StrictFairness：</p><p><img src="/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/2.png"></p><p>　　它可以保证 <span class="math inline">\(P(Y_{l_1}=Y_{l_2}=...=Y_{l_m})=1\)</span>随时都成立。</p><h1 id="fairnas">FairNAS</h1><p>　　FairNas 和 SPOS 一样，也是一个 two-stage 的 One-Shot 算法，第一个stage 训练 train supernet，第二个 stage 搜索最优的 architecture。</p><h2 id="train-supernet-with-strict-fairness">Train Supernet with StrictFairness</h2><p>　　FairNAS 是支持 Strict Fairness 的，它实际上是在 uniform sampling的基础上加了一个公平采样，就是每次 update weights 时同时采样出 <span class="math inline">\(m\)</span> 个architecture，并且是不放回的，所以每层的每个 choice block都有被采样的机会，然后分别训练这 <span class="math inline">\(m\)</span>个 architecture，然后分别做 BP，将梯度累加起来在 supernet 中做一次update weights，论文中称这个为一次 supernetstep，论文中的图把这个思想解释的很清楚：</p><p><img src="/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/3.png"></p><p>　　并且这样做符合了： <span class="math display">\[\begin{aligned} E\left(Y_{l_{i}}^{\prime}\right) &amp;=n / m \\\operatorname{Var}\left(Y_{l_{i}}^{\prime}\right) &amp;=0 \end{aligned}\]</span></p><p>　　方差相比 uniform sampling 来说还减小了。伪代码如下：</p><p><img src="/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/4.png"></p><h2 id="searching-with-supernet-as-an-evaluator">Searching with Supernetas an Evaluator</h2><p>　　搜索的过程依然使用了 evolutionary algorithm，它基于MoreMNAS，使用 Proximal Policy Optimization 作为强化算法, 称为NSGA-II，这个我还没研究过，伪代码如下：</p><p><img src="/blog/2020/07/11/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89-FairNAS/5.png"></p><h1 id="conclusion">Conclusion</h1><p>　　综合来说，FairNAS 的思想是很简单的，感觉相比 SPOS就是多做了一个公平采样，类似于一个trick，不过关于概率推导那段还是很反逻辑的，值得学习。</p><h1 id="refer">Refer</h1><ul><li>[1] X. Chu, B. Zhang, R. Xu, and J. Li, “FairNAS: RethinkingEvaluation Fairness of Weight Sharing Neural Architecture Search,”2020.</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　FairNAS &lt;sup&gt;[1]&lt;/sup&gt; 由小米在 ICCV 2021
上发表，它和我们上文介绍的 SPOS 非常像，不同的只是将其中的 uniform
sampling 改成了公平采样，本文对它做简单的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="One-Shot" scheme="http://chengfeng96.com/tags/One-Shot/"/>
    
    <category term="Weights Sharing" scheme="http://chengfeng96.com/tags/Weights-Sharing/"/>
    
    <category term="SinglePath" scheme="http://chengfeng96.com/tags/SinglePath/"/>
    
    <category term="Evolutionary Algorithm" scheme="http://chengfeng96.com/tags/Evolutionary-Algorithm/"/>
    
    <category term="SPOS" scheme="http://chengfeng96.com/tags/SPOS/"/>
    
    <category term="FairNAS" scheme="http://chengfeng96.com/tags/FairNAS/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（八）- SPOS</title>
    <link href="http://chengfeng96.com/blog/2020/07/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-SPOS/"/>
    <id>http://chengfeng96.com/blog/2020/07/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-SPOS/</id>
    <published>2020-07-09T15:05:17.000Z</published>
    <updated>2022-01-03T03:39:37.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　SPOS(Single Path One-Shot)<sup>[1]</sup> 由旷视在 ECCV 2020年提出，它是 One-Shot 的，并且作者提出了一种 Single Path 的supernet，以及用 uniform sampling 进行采样，论文中采取 search 的方法是evolutionary algorithm，本文对它做简单的笔记。</p><span id="more"></span><h1 id="drawbacks-of-weights-sharing">Drawbacks of weights sharing</h1><p>　　首先作者提出一些关于在 NAS 中应用 weight sharing方法的缺陷，这两个缺陷也是我之前一直感到疑惑的地方：</p><ol type="1"><li>supernet 中将要给采样 child model 共享的 weights是耦合的，无法确定对于特定 architecture 直接使用 supernet的权重是否有效。</li><li>joint optimization 会进一步加剧采样的 architecture 的 parameters 和supernet weights 之间的耦合的程度。</li></ol><p>　　下图是作者在论文中列出各种使用了 weights sharing 方法的 NAS算法的比较：</p><p><img src="/blog/2020/07/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-SPOS/1.png"></p><h1 id="one-shot-nas">One-Shot NAS</h1><p>　　One-Shot NAS 一般分两个阶段，第一个阶段训练 supernet相当于并行训练子网络，第二个阶段就不训练采样出来的 architecture了，直接在验证集上预测查看性能。而 weight sharing 可能在训练 controller的时候也训练部分 supernet：这个过程可以描述为公式 (1) 和 公式 (2)：</p><p><span class="math display">\[W_{\mathcal{A}}=\underset{W}{\operatorname{argmin}} \mathcal{L}_{\text{train }}(\mathcal{N}(\mathcal{A}, W)) \tag{1}\]</span></p><p><span class="math display">\[a^{*}=\underset{a \in \mathcal{A}}{\operatorname{argmax}}\mathrm{ACC}_{\mathrm{val}}\left(\mathcal{N}\left(a,W_{\mathcal{A}}(a)\right)\right) \tag{2}\]</span></p><p>　　所以一般的 One-Shot 模型可以解决 weights sharing方法的第二个缺陷，但却很难解决 weights sharing方法的第一个缺陷，supernet 的 weihts 对于采样的 architecture来说依然耦合了。</p><h1 id="single-path">Single Path</h1><p>　　我们知道我们的目标就是找到一个好的 <span class="math inline">\(W_{\mathcal{A}}\)</span> 让所有在 search space里采样得到的 architecture 可以也有很好的性能，即：</p><p><span class="math display">\[W_{\mathcal{A}}=\underset{W}{\operatorname{argmin}} \mathbb{E}_{a \sim\Gamma(\mathcal{A})}\left[\mathcal{L}_{\text {train }}(\mathcal{N}(a,W(a)))\right] \tag{3}\]</span></p><p>　　为了进一步缓解 weights sharing 方法的第一个缺陷，也就是为了减少node weights 的 co-adaptation，例如如两个网络共享了很多node，更新其中一个相当于部分更新了另外一个,耦合很严重，所以作者设计了一个非常简单的supernet，也代表着非常简洁的 search space，如下图所示：</p><p><img src="/blog/2020/07/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-SPOS/2.png"></p><p>　　它和一些使用 dropout strategy 的 NAS方法形成了鲜明对比，因为往往这些 NAS 方法的性能都对 drop rate非常敏感，而在 Single Path 中不存在 drop rate，因为每次在 choice block里也就只保留一条边。<br>　　作者设计了两种 choice block：</p><ul><li>Channel NumberSearch：它主要用来搜索卷积层的通道数，主要思想是预先分配具有最大通道数的weight tensor。在 supernet 训练期间，系统会随机选择通道数并切出相应的tensor 进行卷积。</li><li>Mixed-Precision Quantization Search：它主要用来在搜索 weiths 和features 的量化精度以用于卷积层。在 supernet 中，feature map 的 width 和kernal weights 是随机选择的。</li></ul><h1 id="uniform-sampling">Uniform Sampling</h1><p>　　公式 (3) 中用到的 prior distribution <span class="math inline">\(\Gamma(\mathcal{A})\)</span>是非常重要的，论文中作者直接使用了 uniformsampling，非常简单，但是在作者的实验中发现效果却很好，可能是因为它可以使所有的architecture 可以同时公平的 optimize。<br>　　需要注意的是，<span class="math inline">\(\Gamma(\mathcal{A})\)</span> 跟不之前的一些 NAS方法不同，在论文中它是 fixed的，是不可学习的，这样做可以进一步缓解耦合的情况。</p><h1 id="evolutionary-architecture-search">Evolutionary ArchitectureSearch</h1><p>　　在搜索最优 architecture 的时候，作者使用了 evolutionaryalgorithm，我们知道对于一般的问题来说这样的开销很大，但是因为我们搜索到每一个architecture 后只 infer 不 train，所以还是挺高效的，伪代码如下：</p><p><img src="/blog/2020/07/09/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-SPOS/3.png"></p><h1 id="conclusion">Conclusion</h1><p>　　综合来说，作者为了解决 weight sharing 方法和 One-Shot方法一些耦合的问题，提出了很简单的 Single Path supernet，以及用 uniformsampling，最后用 evolutionary algorithm 来搜索最优的architecture，总体思路还是非常简单高效的，也容易理解。</p><h1 id="refer">Refer</h1><p>[1]. [1] Z. Guo et al., “Single Path One-Shot Neural ArchitectureSearch with Uniform Sampling,” 2019.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　SPOS(Single Path One-Shot)&lt;sup&gt;[1]&lt;/sup&gt; 由旷视在 ECCV 2020
年提出，它是 One-Shot 的，并且作者提出了一种 Single Path 的
supernet，以及用 uniform sampling 进行采样，论文中采取 search 的方法是
evolutionary algorithm，本文对它做简单的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="One-Shot" scheme="http://chengfeng96.com/tags/One-Shot/"/>
    
    <category term="Weights Sharing" scheme="http://chengfeng96.com/tags/Weights-Sharing/"/>
    
    <category term="Evolutionary Algorithm" scheme="http://chengfeng96.com/tags/Evolutionary-Algorithm/"/>
    
    <category term="SPOS" scheme="http://chengfeng96.com/tags/SPOS/"/>
    
    <category term="Single Path" scheme="http://chengfeng96.com/tags/Single-Path/"/>
    
  </entry>
  
  <entry>
    <title>NAS 学习笔记（七）- NAO</title>
    <link href="http://chengfeng96.com/blog/2020/06/23/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-NAO/"/>
    <id>http://chengfeng96.com/blog/2020/06/23/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-NAO/</id>
    <published>2020-06-23T08:13:23.000Z</published>
    <updated>2022-01-03T03:39:14.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　NAO[1] 由微软在 NeurIPS 2018 上发表，它和 DARTS一样都是搜索空间在 continuous space 上优化和搜索 architecture，但是本质上又很不一样，本文对它做一些笔记。</p><span id="more"></span><h1 id="search-space">Search Space</h1><p>　　我们都知道，基于强化学习和进化算法的 NAS 算法都是把搜索空间限定于discrete space 上，它通过 softmax 将 discrete space 进行了relaxation，然后从 mixture weights 里 argmax 出一个最优的architecture，而 NAO 是通过一个 encoder 和 decoder 来搜索和优化architecture。<br>　　NAO 把搜索空间构造成 continous space 实际是通过 embedding去做到的，它包含三个核心部分：</p><ul><li>An encoder embeds/maps neural network architectures into acontinuous space.</li><li>A predictor takes the continuous representation of a network asinput and predicts its accuracy.</li><li>A decoder maps a continuous representation of a network back to itsarchitecture.</li></ul><p>　　同时它也支持和 ENAS 一样 sharing parameters的功能。下面我们分这三个核心部分介绍下NAO，它的总的算法过程如下图所示：</p><p><img src="/blog/2020/06/23/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-NAO/1.png"></p><h1 id="encoder">Encoder</h1><p>　　encoder <span class="math inline">\(E: \mathcal{X} \rightarrow\mathcal{E}\)</span> 实际上非常简单，就是一个 LSTM，输入初始的architecture <span class="math inline">\(x\)</span> 得到 LSTM 的 hiddenstates <span class="math inline">\(e_{x}=\left\{h_{1}, h_{2}, \cdots,h_{T}\right\} \in \mathcal{R}^{T \times d}\)</span> 作为这个architecture 的 embedding 表示 <span class="math inline">\(e_x\)</span>。</p><h1 id="performance-predictor.">Performance predictor.</h1><p>　　NAO 通过一个 performance predictor <span class="math inline">\(f:\mathcal{E} \rightarrow \mathcal{R}^{+}\)</span>来评估每个网络的性能，它是一个 feed-forward network，通过输入 <span class="math inline">\(\bar{e}_{x}=\frac{1}{T} \sum_{t}^{T}h_{t}\)</span> 来得到一个值作为对性能的估计，然后再和 <span class="math inline">\(x\)</span> 在验证集上的精度 <span class="math inline">\(s_x\)</span> 做均方误差 <span class="math inline">\(\left(s_{x}-f(E(x))\right)^{2}\)</span> 来优化<span class="math inline">\(f\)</span>。<br>　　因为要保证 architecture embedding对相似结构有差不多的性能，作者还采取了一个数据增强的方式，拿 CNN来说，它会将一个 architecture <span class="math inline">\(x_1\)</span>的左右 branch 调个边作为 <span class="math inline">\(x_2\)</span>，然后将 <span class="math inline">\((s_x, x_1)\)</span> 和 <span class="math inline">\((s_x, x_2)\)</span> 一起去训练 encoder 和performance predictor。</p><h1 id="decoder">Decoder</h1><p>　　decoder <span class="math inline">\(D: \mathcal{E} \rightarrowx\)</span> 的原理也很简单，就和 sequence-to-sequence 模型中的 decoder差不多，也是一个 LSTM，用 <span class="math inline">\(h_T(x)\)</span>作为 initial hidden state <span class="math inline">\(s_0\)</span>，并且结合了 attention机制，计算出一个 <span class="math inline">\(ctx_r\)</span>。<br>　　decoder 的目的就是最大化 <span class="math inline">\(P_{D}\left(x |e_{x}\right)=\prod_{r=1}^{T} P_{D}\left(x_{r} | e_{x},x_{&lt;r}\right)\)</span> 得到一个最优的结构，其中 <span class="math inline">\(P_{D}\left(x_{r} | e_{x},x_{&lt;r}\right)=\frac{\exp \left(W_{x_{r}}\left[s_{r}, c tx_{r}\right]\right)}{\sum_{x^{\prime} \in V_{r}} \exp\left(W_{x^{\prime}}\left[s_{r}, c tx_{r}\right]\right)}\)</span>，优化的时候通常是取一个 log。</p><h1 id="training">Training</h1><p>　　NAO 通过最小化 performance prediction loss <span class="math inline">\(L_{p p}\)</span> 加上 structure reconstructionloss <span class="math inline">\(L_{r e c}\)</span> 将 <span class="math inline">\(E\)</span>，<span class="math inline">\(f\)</span>和 <span class="math inline">\(D\)</span> 一起训练和优化：</p><p><span class="math display">\[L=\lambda L_{p p}+(1-\lambda) L_{r e c}=\lambda \sum_{x \inX}(s_{x}-f(E(x))^{2}-(1-\lambda) \sum_{x \in X} \log P_{D}(x | E(x))\]</span></p><p>　　这里 <span class="math inline">\(L_{p p}\)</span>实际上可以看作是一个正则化项，它可以使 encoder 和 decoder不是一个简单的编码和解码的过程。</p><h1 id="inference">Inference</h1><p>　　当 encoder 和 decoder 都收敛后，我们就可以试图推断出更好的architecture。挑选一个已经有比较好的性能的 <span class="math inline">\(x\)</span>，计算它的 <span class="math inline">\(e_x\)</span>，然后利用梯度下降试图找到一个更好的<span class="math inline">\(e_{x^{\prime}}\)</span>：</p><p><span class="math display">\[h_{t}^{\prime}=h_{t}+\eta \frac{\partial f}{\partial h_{t}}, \quade_{x^{\prime}}=\left\{h_{1}^{\prime}, \cdots, h_{T}^{\prime}\right\}\]</span></p><p>　　最后通过 decoder 得到 <span class="math inline">\(x^{\prime}\)</span>，重复这个过程迭代，NAO的伪代码如下：</p><p><img src="/blog/2020/06/23/NAS-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-NAO/2.png"></p><h1 id="conclusion">Conclusion</h1><p>　　NAO 提出了一种基于 encoder，performance predictor 和 decoder的算法，它可以在 continous 的搜索空间上通过梯度下降法搜索更优的architecture。</p><h1 id="refer">Refer</h1><p>[1] Luo R, Tian F, Qin T, et al. Neural architectureoptimization[C]//Advances in neural information processing systems.2018: 7816-7827.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　NAO[1] 由微软在 NeurIPS 2018 上发表，它和 DARTS
一样都是搜索空间在 continuous space 上优化和搜索 architecture
，但是本质上又很不一样，本文对它做一些笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/categories/AI/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/categories/AI/AutoML/NAS/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="AutoML" scheme="http://chengfeng96.com/tags/AutoML/"/>
    
    <category term="NAS" scheme="http://chengfeng96.com/tags/NAS/"/>
    
    <category term="NAO" scheme="http://chengfeng96.com/tags/NAO/"/>
    
    <category term="LSTM" scheme="http://chengfeng96.com/tags/LSTM/"/>
    
    <category term="encoder" scheme="http://chengfeng96.com/tags/encoder/"/>
    
    <category term="decoder" scheme="http://chengfeng96.com/tags/decoder/"/>
    
    <category term="sequence-to-sequence" scheme="http://chengfeng96.com/tags/sequence-to-sequence/"/>
    
  </entry>
  
  <entry>
    <title>XGBoost 和 Lightgbm 笔记</title>
    <link href="http://chengfeng96.com/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/"/>
    <id>http://chengfeng96.com/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/</id>
    <published>2020-06-16T02:49:50.000Z</published>
    <updated>2021-03-10T07:40:29.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　XGBoost 和 LightGBM 是现在非常流行的两种 Boosting Tree的实现，本文对它们做一些笔记以及比较。Boosting Tree的基本原理本文就不赘述了，可以看<a href="https://zhuanlan.zhihu.com/p/84139957">这篇文章</a>。</p><span id="more"></span><h1 id="rf-与-gbdt-的区别">RF 与 GBDT 的区别</h1><p>　　我们列出 RF 与 GBDT 的一些区别：</p><ul><li>集成学习：RF 属于 bagging 思想，而 GBDT 是 boosting 思想</li><li>偏差-方差权衡：RF 不断的降低模型的方差，而 GBDT不断的降低模型的偏差</li><li>训练样本：RF 每次迭代的样本是从全部训练集中有放回抽样形成的，而 GBDT每次使用全部样本</li><li>并行性：RF 的树可以并行生成，而 GBDT只能顺序生成(需要等上一棵树完全生成)</li><li>最终结果：RF 最终是多棵树进行多数表决（回归问题是取平均），而 GBDT是加权融合</li><li>数据敏感性：RF 对异常值不敏感，而 GBDT 对异常值比较敏感</li><li>泛化能力：RF 不易过拟合，而 GBDT 容易过拟合</li></ul><h1 id="xgboost">XGBoost</h1><p>　　和随机森林一样，XGBoost实际上也是由很多棵树组成，是一个加法模型，也是一个集成模型。</p><h2 id="objective-function">Objective Function</h2><p>　　预测是通过多棵树叠加得到： <span class="math display">\[\hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K}f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F} \\\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q:\mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)\]</span> 　　<span class="math inline">\(T\)</span>代表每棵树的叶子节点个数，<span class="math inline">\(q\)</span>代表将样本映射到对应树的叶子节点的 index，<span class="math inline">\(w\)</span> 代表叶子节点的 weights。</p><p>　　目标函数很简单，就是经验风险加上结构风险： <span class="math display">\[\begin{array}{l}\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k}\Omega\left(f_{k}\right) \\\text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}\end{array}\]</span></p><p>　　损失函数是可微的凸函数，结构复杂度的定义有点意思，是叶子结点的数量加上叶子结点权重向量的L2 范数。<br>　　XGBoost 是一个加法模型，很显然我们可以一棵一棵树去学习，当我们学习第<span class="math inline">\(t\)</span> 树时，损失函数可以表示为： <span class="math display">\[\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i},\hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)\]</span></p><p>　　这实际上也是一个贪心算法，当我们学习第 <span class="math inline">\(t\)</span> 棵树时，前面 <span class="math inline">\(t-1\)</span>棵树已经固定了，我们对损失函数做一个二阶的泰勒展开（<span class="math inline">\(f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Deltax+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}\)</span>）：</p><p><span class="math display">\[\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i},\hat{y}^{(t-1)}\right)+g_{i}f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i}f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right) \\g_{i}=\partial_{\hat{y}^{(t-1)}} l\left(y_{i}, \hat{y}^{(t-1)}\right) \\h_{i}=\partial_{\hat{y}^{(t-1)}}^{2} l\left(y_{i},\hat{y}^{(t-1)}\right)\]</span></p><p>　　因为 <span class="math inline">\(l(y_{i},\hat{y}^{(t-1)})\)</span> 实际上在前 <span class="math inline">\(t-1\)</span> 树已经被计算出来了，优化 <span class="math inline">\(t\)</span> 棵树时可以不再考虑它：</p><p><span class="math display">\[\tilde{\mathcal{L}}^{(t)}=\sum_{i=1}^{n}\left[g_{i}f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i}f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)\]</span></p><p>　　我们用 <span class="math inline">\(I_{j}=\left\{i |q\left(\mathbf{x}_{i}\right)=j\right\}\)</span> 来表示叶子结点 <span class="math inline">\(j\)</span>包含的样本。目标函数进一步可改写为：</p><p><span class="math display">\[\begin{aligned}\tilde{\mathcal{L}}^{(t)} &amp;=\sum_{i=1}^{n}\left[g_{i}f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i}f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda\sum_{j=1}^{T} w_{j}^{2} \\&amp;=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right)w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right)w_{j}^{2}\right]+\gamma T\end{aligned}\]</span> 　　对于一个固定的结构 <span class="math inline">\(q(\mathbf{x})\)</span>，我们可以很简单的计算出叶子节点<span class="math inline">\(j\)</span> 的最优值：</p><p><span class="math display">\[w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}}h_{i}+\lambda}\]</span></p><p>　　代入目标函数可继续化简为：</p><p><span class="math display">\[\tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T}\frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}}h_{i}+\lambda}+\gamma T\]</span></p><p>　　这其实也就可以作为判断一个数结构的 scorefunction，示意图如下：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/1.png"></p><h2 id="split-method">Split Method</h2><p>　　那么如何判断要不要分裂一个结点呢，在决策树中我们通常是通过判断分裂后的信息增益是否大于0 来判断是否需要分裂节点的，在 XGBoost中也是这么做的，只不过信息增益的定义我们可以用上节中推导出的目标函数来代替，这也是非常自然的：</p><p><span class="math display">\[\mathcal{L}_{s p l i t}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}}g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i\in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}}h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \inI} h_{i}+\lambda}\right]-\gamma\]</span>　　但是在我们分裂一个结点时，会有很多个候选分割点，如果我们暴力的搜索每个分割点，开销显然是不可接受的，所以通常会使用一种exact greedy algorithm，它的伪代码如下：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/2.png"></p><p>　　大体思路就是遍历每个结点的每个特征，对每个特征，按特征值大小将特征值排序，线性扫描，找出每个特征的最佳分裂特征值，在所有特征中找出最好的分裂点（分裂后增益最大的特征及特征值），这是一种贪心的方法，每次进行分裂尝试都要遍历一遍全部候选分割点，也是一种全局扫描法。但当数据量过大导致内存无法一次载入或者在分布式的环境下，贪心算法的效率就会变得很低，全局扫描法不再适用。<br>　　因此 XGBoost 采取了一种近似算法，选取一些分位点作为 candidate splitpoint，在提出这些点时有两种策略：</p><ul><li>Global：学习每棵树前就提出候选切分点，并在每次分裂时都采用这种分割；</li><li>Local：每次分裂前将重新提出候选切分点。</li></ul><p>　　直观上来看，Local 策略需要更多的计算步骤，而 Global策略因为节点没有划分所以需要更多的 candidate。</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/3.png"></p><p>　　其实就是对每个特征按照特征值排序后，采用类似分位点选取的方式，仅仅选出常数个特征值作为该特征的候选分割点，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。<br>　　判断让树停止生长通常有两种方法，它们其实也就是两个超参数：</p><ul><li>设置树的最大深度</li><li>最小样本权重和，计算叶子节点的样本权重和是否大于最小样本权重和，这样如果一个叶子节点包含的样本数量太少也会放弃分裂，防止树分的太细，这也是过拟合的一种措施，样本权重和为<span class="math inline">\(w_{j}^{*}=-\frac{\sum_{i \in I_{j}}g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}\)</span>。</li></ul><h3 id="weighted-quantile-sketch">Weighted Quantile Sketch</h3><p>　　按照分位点寻找 candidate split points 也是有讲究的，它定义了一个rank 函数：</p><p><span class="math display">\[r_{k}(z)=\frac{1}{\sum_{(x, h) \in \mathcal{D}_{k}} h} \sum_{(x, h) \in\mathcal{D}_{k}, x&lt;z} h\]</span></p><p>　　然后通过下面的条件找出符合的 candidate split point <span class="math inline">\(\left\{s_{k 1}, s_{k 2}, \cdots s_{kl}\right\}\)</span>：</p><p><span class="math display">\[\left|r_{k}\left(s_{k, j}\right)-r_{k}\left(s_{k,j+1}\right)\right|&lt;\epsilon, s_{k 1}=\min _{i} \mathbf{x}_{i k}, s_{kl}=\max _{i} \mathbf{x}_{i k}\]</span></p><p>　　事实上， XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值<span class="math inline">\(h_i\)</span> 作为样本的权重进行划分，使用<span class="math inline">\(h_i\)</span>作为样本权重的理由也很直观：</p><p><span class="math display">\[\begin{aligned}\operatorname{Obj}^{(t)} &amp; \approx \sum_{i=1}^{n}\left[g_{i}f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i}f_{t}^{2}\left(x_{i}\right)\right]+\sum_{i=1}^{t}\Omega\left(f_{i}\right) \\&amp;=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2}h_{i} f_{t}^{2}\left(x_{i}\right)+\frac{1}{2}\frac{g_{i}^{2}}{h_{i}}\right]+\Omega\left(f_{t}\right)+ constant \qquad(\frac{1}{2}\frac{g_{i}^{2}}{h_{i}} 和 constant均为常数，不影响目标函数的值)\\&amp;=\sum_{i=1}^{n} \frac{1}{2}h_{i}\left[f_{t}\left(x_{i}\right)-\left(-\frac{g_{i}}{h_{i}}\right)\right]^{2}+\Omega\left(f_{t}\right)+constant\end{aligned}\]</span></p><p>　　可以看出 <span class="math inline">\(h_i\)</span>就是平方损失函数中样本的权重。对于样本权值相同的数据集来说，找到候选分位点已经有了解决方案（GK算法），当样本权值不一样时，作者提出了一种叫 Weighted Quantile Sketch的算法，论文给出的补充材料里有详细的证明，这里就不赘述了。　　</p><h3 id="sparsity-aware-split-finding">Sparsity-aware Split Finding</h3><p>　　XGBoost 还专门针对稀疏的输入作了优化，例如下图所示的存在 missingvalue 的情况，它会指定一个 default direction：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/4.png"></p><p>　　学习 default direction 可以通过下面这个算法，主要是利用non-missing 的样本，在寻找最佳 split point 时，不会对该列特征 missing的样本进行遍历，而只对该列特征值为 non-missing的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找split point 的时间开销：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/5.png"></p><p>　　同时，为了保证完备性，会将该特征值 missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。</p><h2 id="parallel">Parallel</h2><p>　　树学习中，最耗时的往往是将数据进行排序，为此，XGBoost在训练之前，预先对每个特征按照特征值大小进行排序，然后保存为 block结构，后面的迭代计算中会重复地使用这个结构，这使计算量大大减小。同时由于各个特性已预先存储为block 结构，XGBoost支持利用多个线程并行地计算每个特征的最佳分割点，这大大提升了结点的分裂速度，也极利于大规模训练集在分布式环境下的适应性扩展。<br>　　XGBoost 还做了一些内存和硬盘上的优化：</p><ul><li>Cache-aware Access：使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个 block 中样本的梯度信息并存入连续的 Buffer 中。</li><li>Blocks for Out-of-core Computation：Block 预先放入内存；Block按列进行解压缩；将 Block 划分到不同硬盘来提高吞吐。</li></ul><h2 id="overfitting">Overfitting</h2><p>　　除了给目标函数添加正则项和对样本采样，XGBoost中也采取了另外两种防止过拟合的方法：</p><ul><li>weight shrinkage [3]：相当于学习速率，XGBoost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。</li><li>column（feature） subsampling：队列进行采样，类似 RF的做法，不仅能降低过拟合，还能减少计算。</li></ul><h1 id="lightgbm">LightGBM</h1><p>　　我们可以发现 XGBoost虽然利用预排序和近似算法可以降低寻找最佳分裂点的计算量，但在节点分裂过程中仍需要遍历数据集。并且预排序过程的空间复杂度过高，不仅需要存储特征值，还需要存储特征对应样本的梯度统计值的索引，相当于消耗了两倍的内存，这是XGBoost 最主要的两个缺点。<br>　　而 LightGBM 在这些方面都作了大量优化，相比 XGBoost，LightGBM可以说是速度更快，消耗的内存更少。<br>　　论文中更主要提出了 Gradient- based One-Side Sampling (GOSS)和Exclusive Feature Bundling (EFB) 两种技术，我们分别介绍下。</p><h2 id="直方图加速">直方图加速</h2><p>　　在介绍 GOSS 和 EFB 前，我们先介绍下 LightGBM中用到的直方图的思想，算法如下：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/6.png"></p><p>　　直方图算法的基本思想很简单，就是是将连续的特征离散化分桶为 <span class="math inline">\(k\)</span> 个离散特征，同时构造一个宽度为 <span class="math inline">\(k\)</span> 的直方图用于统计信息，即 <span class="math inline">\(k\)</span> 个bin。这样做的话我们就不需要遍历所有数据，只需要遍历 <span class="math inline">\(k\)</span> 个bin，就可找到最佳的分裂点。寻找最佳分裂点的时间复杂度大大减小，从 <span class="math inline">\(O(\#data \times \#feature)\)</span> 降到了 <span class="math inline">\(O(\#bin \times\#feature)\)</span>。内存占用也更小了，XGBoost 需要用 32位的浮点数去存储特征值，并用 32 位的整型去存储索引，而 LightGBM 只需要用8 位去存储直方图。<br>　　因为是近似算法，特征离散化后无法找到精确的分割点，可能会对模型的精度产生一定的影响，但这种近似的粗粒度的分割也起到了正则化的效果，在一定程度上降低了模型的方差。<br>　　XGBoost 在进行预排序时只对含有 non-missing 的样本进行分裂，而LightGBM 也采用类似策略：只用 non-missing 的样本构建直方图。<br>　　我们还可以进一步加速直方图的构建，在构建叶节点的直方图时，可以通过父节点的直方图与相邻叶节点的直方图相减的方式构建，从而减少了一半的计算量。因此，在实际操作过程中，可以先计算直方图小的叶子节点，然后利用直方图作差来获得直方图大的叶子节点，大大减少了计算。</p><h2 id="goss">GOSS</h2><p>　　如我们在 XGBoost里提到的，梯度大小可以反应样本的权重，梯度越小说明模型拟合的越好，XGBoost利用了二阶梯度，而 LightGBM 利用了一阶梯度。GOSS利用这一信息对样本进行抽样，保留了梯度大的样本，并对梯度小的样本进行随机抽样，在训练时丢弃了大量梯度小的样本，在接下来的计算中只需关注梯度高的样本，极大的减少了计算量。同时，为了不改变样本的数据分布，在计算增益时为梯度小的样本引入一个常数进行平衡。算法如下：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/7.png"></p><h2 id="efb">EFB</h2><p>　　很高维的特征通常是稀疏的，特征间可能是互斥的（如两个特征不同时取非零值），如果两个特征并不完全互斥（如只有一部分情况下是不同时取非零值），可以用互斥率表示互斥程度。EFB将一些特征进行融合绑定来降低特征数量，这也涉及到了两个问题，哪些特征可以一起绑定？特征绑定后特征值如何确定？<br>　　针对第一个问题 EFB利用特征和特征间的关系构造一个加权无向图，并将其转换为图染色问题，图染色问题是NP-Hard 的，因此只能采用贪心算法得到近似解：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/8.png"></p><p>　　思想也很简单，就是构造一个加权无向图，顶点是特征，边是两个特征间互斥程度，然后根据节点的度进行降序排序，度越大，与其他特征的冲突越大，最后遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小。这个算法使得构建直方图的复杂度从<span class="math inline">\(O(\#data \times \#feature)\)</span> 降到了<span class="math inline">\(O(\#data \times \#bundle)\)</span>。<br>　　当特征很多时，计算图的度数开销会变大，因此算法进一步做出优化，不按度数排序，而是按每个特征里的非零值的个数去做排序，因为非零值越多的越容易造成conflict。<br>　　针对第二个问题，论文给出了特征合并算法，它通过给某些特征添加一个offset，使得原始特征能从合并的特征中分离出来：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/9.png"></p><h1 id="comparison">Comparison</h1><p>　　下图是 XGBoost 和 LightGBM 的一个对比：</p><p><img src="/blog/2020/06/16/XGBoost-%E5%92%8C-Lightgbm-%E7%AC%94%E8%AE%B0/10.png"></p><h2 id="树生长策略">树生长策略</h2><p>　　XGBoost 中，树的增长是Level-wise，它基于层进行生长，直到达到停止条件，这样做方便并行计算每一层的分裂节点，提高了训练速度，但同时也因为节点增益过小增加了很多不必要的分裂，增加了计算量。<br>　　而 LightGBM 里是Leaf-wise，每次分裂增益最大的叶子节点，直到达到停止条件。这样可以减少了计算量，配合最大深度的限制防止过拟合，由于每次都需要计算增益最大的节点，所以无法并行分裂。</p><h2 id="分割点查找算法">分割点查找算法</h2><p>　　XGBoost 使用特征预排序算法，LightGBM使用基于直方图的切分点算法，其优势如下：</p><ul><li>减少内存占用，比如离散为 256 个bin时，只需要用 int8就可以保存一个样本被映射为哪个 bin，对比预排序的 exact greedy算法来说（用 int32 来存储索引 + float32 保存特征值），可以节省 <span class="math inline">\(\frac{7}{8}\)</span> 的空间。空间复杂度从 <span class="math inline">\(O(2 * \#data)\)</span> 降低为 <span class="math inline">\(O(2 * \#bin)\)</span>。</li><li>计算效率提高：预排序的 Exact greedy对每个特征都需要遍历一遍数据，并计算增益，而直方图算法在建立完直方图后，只需要对每个特征遍历直方图即可，LightGBM还可以使用直方图做差加速，一个节点的直方图可以通过父节点的直方图减去兄弟节点的直方图得到，从而加速计算</li></ul><blockquote><p>但实际上 XGBoost 的近似直方图算法也类似于 LightGBM这里的直方图算法，为什么 XGBoost 的近似算法比 LightGBM还是慢很多呢？XGBoost 在每一层都动态构建直方图，因为 XGBoost的直方图算法不是针对某个特定的特征，而是所有特征共享一个直方图(每个样本的权重是二阶导)，所以每一层都要重新构建直方图，而LightGBM 中对每个特征都有一个直方图，所以构建一次直方图就够了。</p></blockquote><h2 id="支持离散变量">支持离散变量</h2><p>　　无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM 可以直接处理类别型变量，主要采用 many-vs-many的切分方式将类别特征分为两个子集，实现类别特征的最优切分。</p><h2 id="cache-命中率">Cache 命中率</h2><p>　　XGBoost 使用 Block结构的一个缺点是取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache 缓存命中率低，从而影响算法效率。而 LightGBM是基于直方图分裂特征的，梯度信息都存储在一个个 bin中，所以访问梯度是连续的，缓存命中率高。</p><h2 id="并行策略">并行策略</h2><ul><li>特征并行：LightGBM 特征并行的前提是每个 worker留有一份完整的数据集，但是每个 worker仅在特征子集上进行最佳切分点的寻找。worker之间需要相互通信，通过比对损失来确定最佳切分点。然后将这个最佳切分点的位置进行全局广播，每个worker 进行切分即可。XGBoost 的特征并行与 LightGBM 的最大不同在于XGBoost 每个 worker 节点中仅有部分的列数据，也就是垂直切分，每个 worker寻找局部最佳切分点，worker 之间相互通信，然后在具有最佳切分点的 worker上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他worker 才能开始分裂。二者的区别就导致了 LightGBM 中 worker间通信成本明显降低，只需通信一个特征分裂点即可，而 XGBoost中要广播样本索引。</li><li>数据并行：当数据量很大，特征相对较少时，可采用数据并行策略。LightGBM中先对数据水平切分，每个 worker上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得worker 间的通信成本降低一倍，因为只用通信以此样本量少的节点。XGBoost中的数据并行也是水平切分，然后单个 worker建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个 worker上的节点分裂时会单独计算子节点的样本索引，因此效率慢，每个 worker间的通信量也就变得很大。</li><li>投票并行（LightGBM）：当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。大致思想是：每个worker首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择 top的特征进行直方图的合并，再寻求全局的最优分割点。</li></ul><h1 id="refer">Refer</h1><ul><li>[1] Chen T, Guestrin C. Xgboost: A scalable tree boostingsystem[C]//Proceedings of the 22nd acm sigkdd international conferenceon knowledge discovery and data mining. 2016: 785-794.</li><li>[2] Ke G, Meng Q, Finley T, et al. Lightgbm: A highly efficientgradient boosting decision tree[C]//Advances in neural informationprocessing systems. 2017: 3146-3154.</li><li>[3] J. Friedman. Stochastic gradient boosting. ComputationalStatistics &amp; Data Analysis, 38(4):367–378, 2002.</li><li><a href="https://zhuanlan.zhihu.com/p/84139957">深入理解提升树（BoostingTree）算法</a></li><li><a href="https://cloud.tencent.com/developer/article/1513194">【ML】XGBoost超详细推导，终于有人讲明白了！</a></li><li><a href="https://mp.weixin.qq.com/s/_QgnYoW827GDgVH9lexkNA">珍藏版 |20道XGBoost面试题，你会几个？(上篇)</a></li><li><a href="https://mp.weixin.qq.com/s/BbelOsYgsiOvwfwYs5QfpQ">珍藏版20道XGBoost面试题，你会几个？(下篇)</a></li><li><a href="https://zhuanlan.zhihu.com/p/87885678">【机器学习】决策树（下）——XGBoost、LightGBM（非常详细）</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，
谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　XGBoost 和 LightGBM 是现在非常流行的两种 Boosting Tree
的实现，本文对它们做一些笔记以及比较。Boosting Tree
的基本原理本文就不赘述了，可以看&lt;a href=&quot;https://zhuanlan.zhihu.com/p/84139957&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="Machine Learning" scheme="http://chengfeng96.com/categories/AI/Machine-Learning/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="GBDT" scheme="http://chengfeng96.com/tags/GBDT/"/>
    
    <category term="Boosting Tree" scheme="http://chengfeng96.com/tags/Boosting-Tree/"/>
    
    <category term="XGBoost" scheme="http://chengfeng96.com/tags/XGBoost/"/>
    
    <category term="LightGBM" scheme="http://chengfeng96.com/tags/LightGBM/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记（八）- 连续空间的确定性策略</title>
    <link href="http://chengfeng96.com/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/"/>
    <id>http://chengfeng96.com/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/</id>
    <published>2020-02-28T03:38:03.000Z</published>
    <updated>2020-06-08T05:53:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　之前我们说的基本都是离散空间内的算法，本文记录连续空间内的算法。在连续空间内，动作的个数往往是无穷的，很难计算出<span class="math inline">\(\max_aq(s,a;\boldsymbol{\theta})\)</span>。面对这个问题，我们可以在策略梯度中引入确定性策略。</p><span id="more"></span><h1 id="同策确定性算法">同策确定性算法</h1><p>　　　　我们先回顾下 DQN 中的更新式：</p><p><span class="math display">\[Q(s, a)=Q(s, a)+\alpha\left(r+\gamma \max _{a} Q\left(s_{t+1},a_{t+1}\right)-Q(s, a)\right)\]</span></p><p>　　在更新的过程中，先完成策略评估的工作，再进行策略改进，即需要先计算出下一时刻状态下所以动作的价值，并从中选出最优的行动价值，如果动作数量是有限的，这是可行的，而如果动作空间连续，无法进行这种计算和选择。所以我们可以采取策略梯度的更新方式：</p><p><span class="math display">\[\theta^{\prime}=\theta+\alpha \nabla_{\theta} J(\theta)\]</span></p><p>　　策略梯度法直接对轨迹的价值期望求导，不需要进行最优行动的选择，因此连续型动作空间的问题可以使用策略梯度算法求解。<br>　　对于没有回合区分的连续性空间问题，我们需要定义一个平均回报率，它实际上是轨迹的价值期望，不需要进行最优行动的选择：</p><p><span class="math display">\[\begin{aligned}J(\boldsymbol{\theta}) \doteq r(\pi) &amp; \doteq \lim _{h \rightarrow\infty} \frac{1}{h} \sum_{t=1}^{h} \mathbb{E}\left[R_{t} | S_{0}, A_{0:t-1} \sim \pi\right] \\&amp;=\lim _{t \rightarrow \infty} \mathbb{E}\left[R_{t} | S_{0}, A_{0:t-1} \sim \pi\right] \\&amp;=\sum_{s} \mu(s) \sum_{a} \pi(a | s) \sum_{s^{\prime}, r}p\left(s^{\prime}, r | s, a\right) r\end{aligned} \\\mu(s) \doteq \lim _{t \rightarrow \infty}\operatorname{Pr}\left\{S_{t}=s | A_{0: t} \sim \pi\right\}\]</span></p><p>　　下图是基于后向视角的连续空间下的 Actor–Critic算法，它当中也用到了资格迹：</p><p><img src="/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/1.png"></p><p>　　在连续性空间下，我们有： <span class="math display">\[v_{\pi}(s) \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right] \\q_{\pi}(s,a,a) \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s,A_{t}=a\right]\]</span></p><p>　　因此便可得到连续性空间下的 return：</p><p><span class="math display">\[G_{t} \doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\cdots\]</span></p><p>　　原先回合任务下的策略梯度定理便可以拓展到连续性空间下了，证明过程如下：</p><p><img src="/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/2.png"></p><p>　　对比随机策略梯度公式，确定性策略梯度公式少了对动作求期望，多了 Q值函数对动作的导数。随机策略梯度里，需要对动作和状态求积分，而在确定性策略梯度里只需要对状态求积分，这就决定了求确定性策略梯度只需要更小的样本，特别是动作空间很大的时候。<br>　　在强化学习算法中，很多时候做策略更新的时候是贪心的，即对 Q值取最大值。但如果是在连续动作的强化学习问题里，对 Q求最优值往往不是很容易，或者计算量很大。这也是 Q-learning 和 DQN不好解决高维连续动作问题的原因。而在确定性策略梯度里，其梯度的更新可以视为策略往Q 值梯度的方向走，这样就一步步往 Q 值增大的方向走。</p><h2 id="策略参数化">策略参数化</h2><p>　　基于策略的方法为较大 action空间的问题提供了实用的处理方法，甚至对于连续型问题，通常有着无穷动作空间的情况也没问题，它并不去计算某个具体动作的概率值，而是直接去学习概率分布。<br>　　举个例子，假设动作集合是一些实数，并且来自一个高斯分布，其概率分布便可写作：</p><p><span class="math display">\[\pi(a | s, \boldsymbol{\theta}) \doteq \frac{1}{\sigma(s,\boldsymbol{\theta}) \sqrt{2 \pi}} \exp \left(-\frac{(a-\mu(s,\boldsymbol{\theta}))^{2}}{2 \sigma(s, \boldsymbol{\theta})^{2}}\right)\]</span> 　　其中 <span class="math inline">\(\mu: \mathcal{S} \times\mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}, \sigma: \mathcal{S}\times \mathbb{R}^{d^{\prime}} \rightarrow \mathbb{R}^{+}\)</span>是参数化的近似函数。策略参数 <span class="math inline">\(\boldsymbol{\theta}\)</span> 由 <span class="math inline">\(\boldsymbol{\theta}=\left[\boldsymbol{\theta}_{\mu},\boldsymbol{\theta}_{\sigma}\right]^{\top}\)</span>两部分组成，第一部分用于均值的近似，第二部分用于标准差的近似：</p><p><span class="math display">\[\mu(s, \boldsymbol{\theta}) \doteq \boldsymbol{\theta}_{\mu}^{\top}\mathbf{x}_{\mu}(s) \quad \\\quad \sigma(s, \boldsymbol{\theta}) \doteq \exp\left(\boldsymbol{\theta}_{\sigma}^{\top} \mathbf{x}_{\sigma}(s)\right)\]</span></p><p>　　<span class="math inline">\(\mathbf{x}_{\mu}(s)\)</span> 和 <span class="math inline">\(\mathbf{x}_{\sigma}(s)\)</span>是状态特征向量，可以通过一些方法构造，Sutton 的书里 9.5节有介绍。</p><h1 id="异策确定性算法">异策确定性算法</h1><p>　　一般而言在一个学习过程中，智能体需要探索不同的路径。在相同的状态下需要采取不同的行为，去探索一些更优策略的可能性。随机策略没有这个问题，因为策略本身就带有随机性和探索性，但确定性策略不行，它在某个特定的状态下采取的行为是一定的，所以策略没有探索性。于是在确定性策略算法里，一般采用异策方法。策略梯度法是一个同策的方法，它非常依赖与环境交互的过程，而 DQN方法直接对值函数进行优化，可以使用异策的方法进行训练。所以，如果想在连续行动空间使用异策算法进行优化，可以考虑结合两种算法的特点。</p><h2 id="dpgdeterministic-policy-gradient">DPG（Deterministic PolicyGradient）</h2><p>　　在与环境交互时，DQN 算法一般使用 <span class="math inline">\(\epsilon-greedy\)</span> 策略，策略梯度则是从一个概率分布中采样得到的， 而 DPG的交互方式结合了前面两种算法。从形式上看， DPG 使用了 <span class="math inline">\(\epsilon-greedy\)</span>策略，以一定的概率使用随机策略，而在剩下的情况下使用最优行动；从策略产生的动作上看， DPG将先得到一个确定的行动，这个动作由确定的策略得到，不需要从概率分布中采样，相当于当前状态下的最优行动。如果决定使用随机策略，那么就在求出的确定行动基础上加上一定的噪声，反之则没有噪声。<br>　　虽然确定策略的思想和 DQN 相近，但实际上， DPG也可以看作是策略梯度法的一种特殊情况。我们知道随机策略梯度的输出是动作分布形式，对于离散行动空间，模型输出的是一个类别的分布，也就是每一个取值的概率。而对于连续行动空间，一般会输出一个高斯分布，其中一部分值表示分布的均值，另一部分值表示分布的方差，然后可以使用这些分布的参数采样出动作值。DPG的输出也可以想像成一个连续的分布，只不过这个分布的方差为 0，这样我们就把 DPG 和策略梯度法统一起来了。<br>　　我们再看一眼随机策略的梯度计算公式：</p><p><span class="math display">\[\begin{aligned}\nabla_{\theta} J\left(\pi_{\theta}\right) &amp;=\int_{\mathcal{S}}\rho^{\pi}(s) \int_{\mathcal{A}} \nabla_{\theta} \pi_{\theta}(a | s)Q^{\pi}(s, a) \mathrm{d} a \mathrm{d} s \\&amp;=\mathbb{E}_{s \sim \rho^{\pi}, a \sim\pi_{\theta}}\left[\nabla_{\theta} \log \pi_{\theta}(a | s) Q^{\pi}(s,a)\right]\end{aligned}\]</span></p><p>　　随机性策略梯度需要在整个动作的空间进行采样，策略梯度是关于状态和动作的期望，在求期望时，需要对状态分布和动作分布求积分，这就要求在状态空间和动作空间采集⼤量的样本，这样求均值才能近似期望。<br>　　由于确定性策略产生的动作是确定的，和随机策略不同，相同的策略（即<span class="math inline">\(\boldsymbol{\theta}\)</span>相同时），在状态为<span class="math inline">\(s\)</span>时，动作是唯⼀确定的，因此策略梯度的求解不需要在动作空间采样积分。相比于随机策略方法，确定性策略需要的样本数据更小，算法效率⾼。<br>　　如果采用确定性策略，当初始状态已知时，⽤确定性策略所产⽣的轨迹是固定的，智能体⽆法探索其他轨迹或访问其他状态，即智能体⽆法学习。事实上，确定性策略使用异策方法进行学习，即动作策略和评估策略不是同⼀个策略，此处具体为动作策略是随机策略，以保证充足的探索，评估策略是确定性策略，即利用函数逼近方法估计值函数。则异策确定性策略梯度为：</p><p><span class="math display">\[\begin{aligned}\nabla_{\theta} J(\mu_{\theta}) &amp;=\int_{\mathcal{S}} \rho^{\mu}(s)\nabla_{\theta} \mu_{\theta}(s) \nabla_{a} Q^{\mu}(s,a)|_{a=\mu_{\theta}(s)} \mathrm{d} s \\&amp;=\mathbb{E}_{s \sim \rho^{\mu}} [\nabla_{\theta} \mu_{\theta}(s)\nabla_{a} Q^{\mu}(s, a)|_{a=\mu_{\theta}(s)}]\end{aligned}\]</span></p><p>　　DPG 则是确定性策略梯度与 Actor-Critic 算法的结合，Actor采用随机策略，Critic 采用确定性策略，更新过程如下：</p><p><span class="math display">\[\begin{array}{c}\delta_{t}=r_{t}+\gamma Q^{w}\left(s_{t+1},\mu_{\theta}\left(s_{t+1}\right)\right)-Q^{w}\left(s_{t}, a_{t}\right)\\w_{t+1}=w_{t}+\alpha_{w} \delta_{t} \nabla_{w} Q^{w}\left(s_{t},a_{t}\right) \\\theta_{t+1}=\theta_{t}+\left.\alpha_{\theta} \nabla_{\theta}\mu_{\theta}\left(s_{t}\right) \nabla_{a} Q^{w}\left(s_{t},a_{t}\right)\right|_{a=\mu_{\theta}(s)}\end{array}\]</span></p><p>　　利用值函数逼近的方法更新值函数参数来进行 Critic的参数更新，即上面更新公式的前两行时，动作为输入，权重连接的是输入状态和动作。在进行Actor 更新的时候，需要更新的参数是 <span class="math inline">\(\theta\)</span>，确定性策略计算中 <span class="math inline">\(\nabla_{a} Q^{w}\left(s_{t},a_{t}\right)|_{a=\mu_{\theta}(s)}\)</span> 与参数 <span class="math inline">\(\theta\)</span> 无关。</p><h2 id="ddpgdeep-deterministic-policy-gradient">DDPG（Deep DeterministicPolicy Gradient）</h2><p>　　DDPG 是深度神经网络和 DPG的结合，所谓深度是指利⽤深度神经⽹络逼近 Q 值函数 <span class="math inline">\(Q^w(s,a)\)</span>和确定性策略。为了打破数据之间的相关性，DDPG 和 DQN一样同样使用了经验回放和独⽴的目标网络，其实现框架和流程如下：</p><p><img src="/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/3.jpg"></p><p>　　我们简单说下各个组件的作用：</p><ul><li>Actor当前网络：负责策略网络参数 <span class="math inline">\(\theta\)</span> 的迭代更新，负责根据当前状态 <span class="math inline">\(s\)</span> 选择当前动作 <span class="math inline">\(a\)</span>，用于和环境交互生成 <span class="math inline">\(s^{\prime}, r\)</span>。</li><li>Actor目标网络：负责根据经验回放池中采样的下一状态 <span class="math inline">\(s^{\prime}\)</span> 选择最优下一动作 <span class="math inline">\(a^{\prime}\)</span>，网络参数 <span class="math inline">\(\theta\)</span> 定期从 <span class="math inline">\(\theta^{\prime}\)</span> 复制。</li><li>Critic当前网络：负责价值网络参数 <span class="math inline">\(w\)</span> 的迭代更新，负责计算负责计算当前 <span class="math inline">\(Q\)</span> 值 <span class="math inline">\(Q(s,a,w)\)</span>。目标 Q 值 <span class="math inline">\(y_{i}=r+\gamma Q^{\prime}\left(s^{\prime},a^{\prime}, w^{\prime}\right)\)</span></li><li>Critic目标网络：负责计算目标Q值中 <span class="math inline">\(Q^{\prime}\left(s^{\prime}, a^{\prime},w^{\prime}\right)\)</span> 的部分，网络参数 <span class="math inline">\(w\)</span> 定期从 <span class="math inline">\(w^{\prime}\)</span> 复制。。</li></ul><p>　　DDPG 从当前网络到目标网络的复制和 DQN 不一样，DQN 是直接把当前 Q网络的参数复制到目标 Q 网络，即 <span class="math inline">\(w =w^{\prime}\)</span>，DDPG没有使用这种硬更新，而是使用了软更新：</p><p><span class="math display">\[\begin{array}{l}w^{\prime} \leftarrow \tau w+(1-\tau) w^{\prime} \\\theta^{\prime} \leftarrow \tau \theta+(1-\tau) \theta^{\prime}\end{array}\]</span></p><p>　　同时，为了学习过程可以增加一些随机性，增加学习的覆盖，DDPG对选择出来的动作会增加一定的噪声，例如 Ornstein-Uhlenbeck 噪声：</p><p><span class="math display">\[a=\pi_{\theta}(s)+\mathcal{N}\]</span></p><p>　　对于 DDPG 的损失函数，Critic 当前网络的损失函数和 DQN是类似的，都是均方误差：</p><p><span class="math display">\[\text { Loss }=\frac{1}{N}\sum_{j=1}^{N}\left(y_{j}-Q\left(\phi\left(s_{j}\right), a_{j},w\right)\right)^{2}\]</span></p><p>　　而对于 Actor当前网络，由于是确定性策略，原论文定义的损失梯度是：</p><p><span class="math display">\[\nabla_{\theta} J(\theta)=\partial g(x) 1 N \sum_{i=1}^{N}[\nabla_{a}Q_{(} s_{i}, a_{i}, w)|_{s=s_{i}, a=\pi_{\theta}(s)} \nabla_{\theta}\pi_{\theta(s)}|_{s=s_{i}}]\]</span></p><p>　　假如对同一个状态，我们输出了两个不同的动作 <span class="math inline">\(a_1\)</span> 和 <span class="math inline">\(a_2\)</span>，从 Critic当前网络得到了两个反馈的Q值 <span class="math inline">\(Q_1\)</span> 和<span class="math inline">\(Q_2\)</span>，假设 <span class="math inline">\(Q_1 \gt Q_2\)</span>，那么策略梯度的思想就是增加<span class="math inline">\(a_1\)</span> 的概率，降低 <span class="math inline">\(a_2\)</span> 的概率，也就是说，Actor想要尽可能的得到更大的 Q 值。所以我们的 Actor的损失可以简单的理解为得到的反馈 Q 值越大损失越小，得到的反馈 Q值越小损失越大，因此只要对状态估计网络返回的 Q 值取个负号即可，即：</p><p><span class="math display">\[\operatorname{Loss}=-\frac{1}{m} \sum_{i=1}^{m} Q( s_{i}, a_{i}, w)\]</span></p><p>　　算法伪代码如下：</p><p><img src="/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/4.png"></p><h2 id="td3">TD3</h2><p>　　Dueling DQN 可以消除 DQN 里 overestimation bias的问题，而 TD3算法可以 消除 Actor-Critic 类算法里 overestimation bias的问题，同时目标网络在 TD update 中还可以消除累积误差。<br>　　伪代码如下：</p><p><img src="/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/5.png"></p><p>　　论文的理解可以看看<a href="https://zhuanlan.zhihu.com/p/47182584%5D">这篇文章</a>。</p><h1 id="总结">总结</h1><p>　　我们可以看出在连续性空间下，有同策和异策的算法，我们一般是使用确定性策略的Actor–Critic 算法。</p><h1 id="refer">Refer</h1><ul><li><a href="https://book.douban.com/subject/2866455/">ReinforcementLearning</a></li><li><a href="https://book.douban.com/subject/34478302/">强化学习：原理与Python实现</a></li><li><a href="https://www.jianshu.com/p/a8608c98adc0">深度强化学习（六）：连续动作空间的问题</a></li><li><a href="https://zhuanlan.zhihu.com/p/41822485">强化学习笔记十三：确定性策略梯度方法</a></li></ul><h1 id="相关内容">相关内容</h1><ul><li><a href="http://chengfeng96.com/blog/2020/02/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89-Markov-%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%A8%A1%E5%9E%8B/">强化学习笔记（一）-Markov 决策过程模型</a></li><li><a href="http://chengfeng96.com/blog/2020/02/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/">强化学习笔记（二）-动态规划方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/">强化学习笔记（三）-蒙特卡罗方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/">强化学习笔记（四）-时序差分学习</a></li><li><a href="https://chengfeng96.com/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/">强化学习笔记（五）-函数近似方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/">强化学习笔记（六）-策略梯度</a></li><li><a href="https://chengfeng96.com/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/">强化学习笔记（七）-资格迹</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　之前我们说的基本都是离散空间内的算法，本文记录连续空间内的算法。在连续空间内，动作的个数往往是无穷的，很难计算出
&lt;span class=&quot;math inline&quot;&gt;\(\max_a
q(s,a;\boldsymbol{\theta})\)&lt;/span&gt;。面对这个问题，我们可以在策略梯度中引入确定性策略。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="Reinforcement Learning" scheme="http://chengfeng96.com/categories/AI/Reinforcement-Learning/"/>
    
    
    <category term="RL" scheme="http://chengfeng96.com/tags/RL/"/>
    
    <category term="Policy Gradient" scheme="http://chengfeng96.com/tags/Policy-Gradient/"/>
    
    <category term="Actor–Critic" scheme="http://chengfeng96.com/tags/Actor%E2%80%93Critic/"/>
    
    <category term="DQN" scheme="http://chengfeng96.com/tags/DQN/"/>
    
    <category term="DPG" scheme="http://chengfeng96.com/tags/DPG/"/>
    
    <category term="DDPG" scheme="http://chengfeng96.com/tags/DDPG/"/>
    
    <category term="experience repaly" scheme="http://chengfeng96.com/tags/experience-repaly/"/>
    
    <category term="target network" scheme="http://chengfeng96.com/tags/target-network/"/>
    
    <category term="TD3" scheme="http://chengfeng96.com/tags/TD3/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记（七）- 资格迹</title>
    <link href="http://chengfeng96.com/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/"/>
    <id>http://chengfeng96.com/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/</id>
    <published>2020-02-24T02:53:52.000Z</published>
    <updated>2020-06-08T05:53:07.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　资格迹（EligibilityTrace）是强化学习中一种非常基础的机制，它可以使时序差分学习更加高效，能在回合更新（MC方法）和单步时序差分更新（TD(0)）间进行折中。</p><span id="more"></span><p>　　我们该如何理解资格迹呢，通常有两种理解的思路，前向视角和后向视角：</p><ul><li>前向视角（forwardview），资格迹是一种理论思想，也叫理论视角，即由当前状态向还未访问的状态观察。这种思想作用于基本TD(0) 算法上，把它改造成一类新的，更强大的 TD 算法。这一类新的TD算法介于MC 和 TD(0) 之间，但是效果比这两者都要好，因此可以把资格迹看成 TD(0) 和MC 的折中。<br></li><li>后向视角（backwardview），资格迹是轨迹中每个状态的附加属性，也叫机理视角，即由当前状态向已经访问过的状态观察。每个状态的资格迹决定了该状态与当前正在访问的状态的价值更新值之间的关联程度，或者说影响程度。</li></ul><p>　　前向视角，告诉我们资格迹在理论层面是如何工作的。后向视角，则告诉我们资格迹在工程层面是如何实现的。</p><h1 id="lambda-return"><span class="math inline">\(\lambda\)</span>-return</h1><p>　　<span class="math inline">\(n\)</span>-steps return：</p><p><span class="math display">\[G_{t: t+n} = R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^{n} \hat{v}\left(S_{t+n}, \mathbf{w}_{t+n-1}\right),\quad 0 \leq t \leq T-n\]</span></p><p>　　对多个 <span class="math inline">\(n\)</span>-steps return作加权平均，只需权重和为 1，例如下面的 backup 图所示的 <span class="math inline">\(\frac{1}{2} G_{t: t+2}+\frac{1}{2} G_{t:t+4}\)</span>，这种更新方式称为 compound update：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/1.png"></p><p>　　我们接下来要说的 TD(<span class="math inline">\(\lambda\)</span>)也属于这种 averaging n-step update，它包括了所有的 <span class="math inline">\(n\)</span>-steps updates，<span class="math inline">\(\lambda\)</span>-return 的定义如下：</p><p><span class="math display">\[G_{t}^{\lambda} =(1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_{t:t+n}\]</span></p><p>　　我们可以看出它的权重系数为 <span class="math inline">\(\lambda^{n-1}\)</span>，它的 backup 图：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/2.png"></p><p>　　如图中所写一样，当 <span class="math inline">\(\lambda=1\)</span>时就是 MC，当 <span class="math inline">\(\lambda=0\)</span> 时就是1-step TD。权重的递减如下图所示：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/3.png"></p><p>　　因为要满足权重和为 1，所以上式又可以写为：</p><p><span class="math display">\[G_{t}^{\lambda}=(1-\lambda) \sum_{n=1}^{T-t-1} \lambda^{n-1} G_{t:t+n}+\lambda^{T-t-1} G_{t}\]</span></p><p>　　将 <span class="math inline">\(G_{t}^{\lambda}\)</span>作为回报更新，这个算法也叫做为 offline <span class="math inline">\(\lambda\)</span>-returnalgorithm，可以采用半梯度更新：</p><p><span class="math display">\[\mathbf{w}_{t+1} =\mathbf{w}_{t}+\alpha\left[G_{t}^{\lambda}-\hat{v}\left(S_{t},\mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t},\mathbf{w}_{t}\right), \quad t=0, \ldots, T-1\]</span></p><p>　　这个算法也是一个前向视角的算法，它在每个状态都需要一些未来状态的信息，示意图如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/4.png"></p><h1 id="tdlambda">TD(<span class="math inline">\(\lambda\)</span>)</h1><p>　　相比于 offline <span class="math inline">\(\lambda\)</span>-return，TD(<span class="math inline">\(\lambda\)</span>) 单步更新权重向量，无需等待episode 结束，并且计算在时间上均匀分布，不仅可用于 episode问题，还适用于连续问题。<br>　　我们记资格迹 <span class="math inline">\(\mathbf{z}_{t} \in\mathbb{R}^{d}\)</span>：</p><p><span class="math display">\[\begin{aligned}&amp;\mathbf{z}_{-1} = \mathbf{0}\\&amp;\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1}+\nabla\hat{v}\left(S_{t}, \mathbf{w}_{t}\right), \quad 0 \leq t \leq T\end{aligned}\]</span></p><p>　　通过它的定义式，我们可以发现其与权重 <span class="math inline">\(\mathbf{w}_{t}\)</span> 维度相同，<span class="math inline">\(\mathbf{w}_{t}\)</span>有长期的记忆性，其存在时间与系统等长，而资格迹则体现了短期记忆，存在于回合内的一个子片段中。<br>　　资格迹试图找到那些对最近状态价值有贡献的权重分量，这个最近体现在系数<span class="math inline">\(\gamma \lambda\)</span> 上。<br>　　one-step TD error：</p><p><span class="math display">\[\delta_{t} = R_{t+1}+\gamma \hat{v}\left(S_{t+1},\mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\]</span></p><p>　　TD(<span class="math inline">\(\lambda\)</span>) 更新式：</p><p><span class="math display">\[\begin{aligned}\mathbf{w}_{t+1} &amp;= \mathbf{w}_{t}+\alpha \delta_{t} \nabla\hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \\&amp;= \mathbf{w}_{t}+\alpha \delta_{t}\left[\nabla \hat{v}\left(S_{t},\mathbf{w}_{t}\right)+\gamma \lambda \mathbf{z}_{t-1}\right](给梯度增加一些信息)\\&amp;= \mathbf{w}_{t}+\alpha \delta_{t} \mathbf{z}_{t}\end{aligned}\]</span></p><p>　　伪代码如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/5.png"></p><p>　　TD(<span class="math inline">\(\lambda\)</span>)是后向视角的算法，每个时刻得到新的 TD error后，又依据它对之前状态的资格迹作一些调整，示意图如下所示：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/6.png"></p><p>　　使用了资格迹后仍能统一地表示各种算法。当 <span class="math inline">\(\lambda=0\)</span>时，恰好就是价值函数的梯度，此时正好对应 one-step TD。当 <span class="math inline">\(\lambda=1\)</span>时，可以看出每一个状态都刚好衰减 <span class="math inline">\(\gamma\)</span> 倍，跑完这个回合 ，恰好就是MC。</p><h1 id="n-step-truncated-lambda-return-methods">n-step Truncated <span class="math inline">\(\lambda\)</span>-return Methods</h1><p>　　offline <span class="math inline">\(\lambda\)</span>-return是很难使用的，因为它需要等到一个回合结束才能计算，对于连续性任务来说更不可用，因为这个<span class="math inline">\(n\)</span>可以无限大，所以我们需要做一些截断，用估计值代替抛弃掉的较远的rewards，定义 Truncated <span class="math inline">\(lambda\)</span>-return：</p><p><span class="math display">\[G_{t: h}^{\lambda} =(1-\lambda) \sum_{n=1}^{h-t-1} \lambda^{n-1} G_{t:t+n}+\lambda^{h-t-1} G_{t: h}, \quad 0 \leq t&lt;h \leq T\]</span></p><p>　　该算法也称为 Truncated TD(<span class="math inline">\(\lambda\)</span>) ，或 TTD(<span class="math inline">\(\lambda\)</span>) ，backup 图如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/7.png"></p><p>　　更新式如下：</p><p><span class="math display">\[\mathbf{w}_{t+n} = \mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}^{\lambda}-\hat{v}\left(S_{t}, \mathbf{w}_{t+n-1}\right)\right]\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t+n-1}\right), \quad 0 \leqt&lt;T \\G_{t: t+k}^{\lambda}=\hat{v}\left(S_{t},\mathbf{w}_{t-1}\right)+\sum_{i=t}^{t+k-1}(\gamma \lambda)^{i-t}\delta_{i} \\\delta_{t}^{\prime} = R_{t+1}+\gamma \hat{v}\left(S_{t+1},\mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t-1}\right)\]</span></p><h1 id="redoing-updates-the-online-lambda--return-algorithm">RedoingUpdates: The Online <span class="math inline">\(\lambda\)</span> -returnAlgorithm</h1><p>　　对 TTD(<span class="math inline">\(\lambda\)</span>) 来说，<span class="math inline">\(n\)</span> 越大越接近 offline <span class="math inline">\(\lambda\)</span>-return，<span class="math inline">\(n\)</span>越小更新速度越快，我们可以达到一个平衡，每次获得更新数据后，重新从头开始执行所有更新：</p><p><span class="math display">\[\begin{aligned}h=1: &amp; \mathbf{w}_{1}^{1} = \mathbf{w}_{0}^{1}+\alpha\left[G_{0:1}^{\lambda}-\hat{v}\left(S_{0}, \mathbf{w}_{0}^{1}\right)\right] \nabla\hat{v}\left(S_{0}, \mathbf{w}_{0}^{1}\right) \\h=2: &amp; \mathbf{w}_{1}^{2} = \mathbf{w}_{0}^{2}+\alpha\left[G_{0:2}^{\lambda}-\hat{v}\left(S_{0}, \mathbf{w}_{0}^{2}\right)\right] \nabla\hat{v}\left(S_{0}, \mathbf{w}_{0}^{2}\right) \\&amp; \mathbf{w}_{2}^{2} = \mathbf{w}_{1}^{2}+\alpha\left[G_{1:2}^{\lambda}-\hat{v}\left(S_{1}, \mathbf{w}_{1}^{2}\right)\right] \nabla\hat{v}\left(S_{1}, \mathbf{w}_{1}^{2}\right) \\h=3: &amp; \mathbf{w}_{1}^{3} = \mathbf{w}_{0}^{3}+\alpha\left[G_{0:3}^{\lambda}-\hat{v}\left(S_{0}, \mathbf{w}_{0}^{3}\right)\right] \nabla\hat{v}\left(S_{0}, \mathbf{w}_{1}^{3}\right) \\&amp;\mathbf{w}_{2}^{3} = \mathbf{w}_{1}^{3}+\alpha\left[G_{1:3}^{\lambda}-\hat{v}\left(S_{1}, \mathbf{w}_{1}^{3}\right)\right] \nabla\hat{v}\left(S_{1}, \mathbf{w}_{1}^{3}\right) \\&amp; \mathbf{w}_{3}^{3} = \mathbf{w}_{2}^{3}+\alpha\left[G_{2:3}^{\lambda}-\hat{v}\left(S_{2}, \mathbf{w}_{2}^{3}\right)\right] \nabla\hat{v}\left(S_{2}, \mathbf{w}_{2}^{3}\right)\end{aligned}\]</span></p><p>　　每个 <span class="math inline">\(\mathbf{w}_{0}^{h}\)</span>都继承自上一个回合，所有的 <span class="math inline">\(\mathbf{w}_{0}^{h}\)</span> 都为同一值，<span class="math inline">\(\mathbf{w}_{t}^{h}\)</span>则为每一步的更新结果。更一般的表示为：</p><p><span class="math display">\[\mathbf{w}_{t+1}^{h} = \mathbf{w}_{t}^{h}+\alpha\left[G_{t:h}^{\lambda}-\hat{v}\left(S_{t}, \mathbf{w}_{t}^{h}\right)\right] \nabla\hat{v}\left(S_{t}, \mathbf{w}_{t}^{h}\right), \quad 0 \leq t&lt;h \leqT^{=}\]</span></p><p>　　这就是 online <span class="math inline">\(\lambda\)</span>-returnalgorithm，是完全在线算法，每个时间点 <span class="math inline">\(t\)</span> 均使用已有数据产生一个新权重向量 <span class="math inline">\(\mathbf{w}_{t}\)</span>。他在每个时间点 <span class="math inline">\(h\)</span> 都能充分利用上一个 <span class="math inline">\(h\)</span>时间之前的所有信息，可以看出现在这种算法对数据的利用率更高，更新效果更好，但是它每次均需从头计算，复杂度高。</p><h1 id="true-online-tdlambda">True Online TD(<span class="math inline">\(\lambda\)</span>)</h1><p>　　为了解决 online <span class="math inline">\(\lambda\)</span>-return计算复杂度太高，我们可以利用资格迹将算法变换为后向视角的算法，这就是TrueOnline TD(<span class="math inline">\(\lambda\)</span>)。online <span class="math inline">\(\lambda\)</span> -return方法产生的权重序列如下：</p><p><span class="math display">\[\begin{aligned}&amp;\mathbf{w}_{0}^{0}\\&amp;\begin{array}{lll}{\mathbf{w}_{0}^{1}} &amp; {\mathbf{w}_{1}^{1}} \\{\mathbf{w}_{0}^{2}} &amp; {\mathbf{w}_{1}^{2}} &amp;{\mathbf{w}_{2}^{2}} \\{\mathbf{w}_{0}^{3}} &amp; {\mathbf{w}_{1}^{3}} &amp;{\mathbf{w}_{2}^{3}} &amp; {\mathbf{w}_{3}^{3}}\end{array}\\&amp;\begin{array}{lllll}{\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\vdots} &amp; {\ddots} \\{\mathbf{w}_{0}^{T}} &amp; {\mathbf{w}_{1}^{T}} &amp;{\mathbf{w}_{2}^{T}} &amp; {\mathbf{w}_{3}^{T}} &amp; {\cdots} &amp;{\mathbf{w}_{T}^{T}}\end{array}\end{aligned}\]</span></p><p>　　其实我们需要的只是每行最后一个权重 <span class="math inline">\(\mathbf{w}_{t}^{t}\)</span> ，得到 <span class="math inline">\(\mathbf{w}_{t}=\mathbf{w}_{t}^{t}\)</span>。对于线性情况，Trueonline TD(<span class="math inline">\(\lambda\)</span>) 定义如下：</p><p><span class="math display">\[\mathbf{w}_{t+1} = \mathbf{w}_{t}+\alpha \delta_{t}\mathbf{z}_{t}+\alpha\left(\mathbf{w}_{t}^{\top}\mathbf{x}_{t}-\mathbf{w}_{t-1}^{\top}\mathbf{x}_{t}\right)\left(\mathbf{z}_{t}-\mathbf{x}_{t}\right) \\\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1}+\left(1-\alpha \gamma\lambda \mathbf{z}_{t-1}^{\top} \mathbf{x}_{t}\right) \mathbf{x}_{t}\]</span></p><p>　　True online TD(<span class="math inline">\(\lambda\)</span>)可以产生和 online <span class="math inline">\(\lambda\)</span>-return算法一样的结果，空间要求和 TD(<span class="math inline">\(\lambda\)</span>)相同，计算量稍微增加了50%，但仍为<span class="math inline">\(O(d)\)</span>复杂度。伪代码如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/8.png"></p><p>　True online TD(<span class="math inline">\(\lambda\)</span>)使用的资格迹称为 Dutch Trace，之前的 TD(<span class="math inline">\(\lambda\)</span>) 中则称为 AccumulatingTrace。还有一种叫 Replacing Trace，定义如下：</p><p><span class="math display">\[z_{i, t} =\left\{\begin{array}{ll}{1} &amp; {\text { if } x_{i, t}=1} \\{\gamma \lambda z_{i, t-1}} &amp; {\text { otherwise }}\end{array}\right.\]</span></p><p>　　它仅针对表格形式或二进制特征向量，例如由图块编码产生的特征向量定义。</p><h1 id="dutch-traces-in-monte-carlo-learning">Dutch Traces in MonteCarlo Learning</h1><p>　　资格迹其实于 TD 在本质上并无联系，资格迹其实是来自MC。线性情况下，梯度 MC 法的更新式如下：</p><p><span class="math display">\[\mathbf{w}_{t+1} = \mathbf{w}_{t}+\alpha\left[G-\mathbf{w}_{t}^{\top}\mathbf{x}_{t}\right] \mathbf{x}_{t}, \quad 0 \leq t&lt;T\]</span></p><p>　　为了简化，这里的 <span class="math inline">\(G\)</span>只是回合结束后得到的单个奖励，所以没有下标，并且没有做 <span class="math inline">\(\gamma\)</span>折扣。我们考虑这样一个优化，在回合里的每一步都进行一些计算，但仍只在回合结束后才进行更新，所以复杂度仍为<span class="math inline">\(O(d)\)</span> 。算法如下，其中引入 <span class="math inline">\(\mathbf{F}_{t} = \mathbf{I}-\alpha \mathbf{x}_{t}\mathbf{x}_{t}^{\top}\)</span> 为遗忘矩阵（衰退矩阵）：</p><p><span class="math display">\[\begin{aligned}\mathbf{w}_{T}&amp;=\mathbf{w}_{T-1}+\alpha\left(G-\mathbf{w}_{T-1}^{\top}\mathbf{x}_{T-1}\right) \mathbf{x}_{T-1} \\&amp;=\mathbf{w}_{T-1}+\alpha\mathbf{x}_{T-1}\left(-\mathbf{x}_{T-1}^{\top}\mathbf{w}_{T-1}\right)+\alpha G \mathbf{x}_{T-1} \\&amp;=\left(\mathbf{I}-\alpha \mathbf{x}_{T-1}\mathbf{x}_{T-1}^{\top}\right) \mathbf{w}_{T-1}+\alpha G\mathbf{x}_{T-1} \\&amp;=\mathbf{F}_{T-1} \mathbf{w}_{T-1}+\alpha G \mathbf{x}_{T-1} \\&amp;=\mathbf{F}_{T-1}\left(\mathbf{F}_{T-2} \mathbf{w}_{T-2}+\alpha G\mathbf{x}_{T-2}\right)+\alpha G \mathbf{x}_{T-1}\\&amp;=\mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{w}_{T-2}+\alphaG\left(\mathbf{F}_{T-1} \mathbf{x}_{T-2}+\mathbf{x}_{T-1}\right)\\&amp;=\mathbf{F}_{T-1} \mathbf{F}_{T-2}\left(\mathbf{F}_{T-3}\mathbf{w}_{T-3}+\alpha G \mathbf{x}_{T-3}\right)+\alphaG\left(\mathbf{F}_{T-1} \mathbf{x}_{T-2}+\mathbf{x}_{T-1}\right)\\&amp;=\mathbf{F}_{T-1} \mathbf{F}_{T-2} \mathbf{F}_{T-3}\mathbf{w}_{T-3}+\alpha G\left(\mathbf{F}_{T-1} \mathbf{F}_{T-2}\mathbf{x}_{T-3}+\mathbf{F}_{T-1}\mathbf{x}_{T-2}+\mathbf{x}_{T-1}\right)\\&amp;=\underbrace{\mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots\mathbf{F}_{0} \mathbf{w}_{0}}_{\mathbf{a}_{T-1}}+\alpha G\sum_{k=0}^{T-1} \mathbf{F}_{T-1} \mathbf{F}_{T-2} \cdots\mathbf{F}_{k+1} \mathbf{x}_{k}\\&amp;=\mathbf{a}_{T-1}+\alpha G \mathbf{z}_{T-1}\end{aligned}\]</span></p><p>　　其中 <span class="math inline">\(\mathbf{a}_{T-1}\)</span> 和<span class="math inline">\(\mathbf{z}_{T-1}\)</span> 是 T-1时刻的两个辅助记忆向量，即使不知道 <span class="math inline">\(G\)</span>，也能在每步先做一些这样的计算，用以存储一些信息。其实 <span class="math inline">\(\mathbf{z}_{t}\)</span> 是一个 dutch-styleeligibility trace，初始值为 <span class="math inline">\(\mathbf{z}_{0}=\mathbf{x}_{0}\)</span>，更新式为：</p><p><span class="math display">\[\begin{aligned}\mathbf{z}_{t} &amp; = \sum_{k=0}^{t} \mathbf{F}_{t} \mathbf{F}_{t-1}\cdots \mathbf{F}_{k+1} \mathbf{x}_{k}, \quad 1 \leq t&lt;T \\&amp;=\sum_{k=0}^{t-1} \mathbf{F}_{t} \mathbf{F}_{t-1} \cdots\mathbf{F}_{k+1} \mathbf{x}_{k}+\mathbf{x}_{t} \\&amp;=\mathbf{F}_{t} \mathbf{F}_{t-1} \cdots \mathbf{F}_{k+1}\mathbf{x}_{k}+\mathbf{x}_{t} \\&amp;=\mathbf{F}_{t} \sum_{k=0}^{t-1} \mathbf{F}_{t-1} \mathbf{F}_{t-2}\cdots \mathbf{F}_{k+1} \mathbf{x}_{k}+\mathbf{x}_{t} \\&amp;=\left(\mathbf{I}-\alpha \mathbf{x}_{t-1}\mathbf{F}_{t-1}+\mathbf{x}_{t}\right.\\&amp;=\mathbf{z}_{t-1}-\alpha\left(\mathbf{z}_{t-1}^{\top}\mathbf{x}_{t}\right) \mathbf{x}_{t}+\mathbf{x}_{t} \\&amp;=\mathbf{z}_{t-1}+\left(1-\alpha \mathbf{z}_{t-1}^{\top}\mathbf{x}_{t}\right) \mathbf{x}_{t} \\&amp;=\mathbf{z}_{t-1}+\left(1-\alpha \mathbf{z}_{t-1}^{\top}\mathbf{x}_{t}\right) \mathbf{x}_{t}\end{aligned}\]</span></p><p>　　辅助向量 <span class="math inline">\(\mathbf{a}_{t}\)</span>初始值为 <span class="math inline">\(\mathbf{a}_{0}=\mathbf{w}_{0}\)</span>，更新式为：</p><p><span class="math display">\[\mathbf{a}_{t} = \mathbf{F}_{t} \mathbf{F}_{t-1} \cdots \mathbf{F}_{0}\mathbf{w}_{0}=\mathbf{F}_{t} \mathbf{a}_{t-1}=\mathbf{a}_{t-1}-\alpha\mathbf{x}_{t} \mathbf{x}_{t}^{\top} \mathbf{a}_{t-1}, \quad 1 \leqt&lt;T_{-}\]</span></p><p>　　在 t &lt; T 时，每步更新辅助向量 <span class="math inline">\(\mathbf{a}_{t}\)</span> 和 <span class="math inline">\(\mathbf{z}_{t}\)</span> ，当 T 时刻获得 <span class="math inline">\(G\)</span> 后再最终计算出 <span class="math inline">\(\mathbf{w}_{t}\)</span> 。得到的结果和 MC相同，但计算更简单。因此资格迹并不局限于 TD方法，只要想做长期预测，都可以采用资格迹。</p><h1 id="sarsalambda">SARSA(<span class="math inline">\(\lambda\)</span>)</h1><p>　　将资格迹从状态价值拓展到动作价值，只需把状态价值函数的估计替换成动作价值函数的估计，n-stepreturn：</p><p><span class="math display">\[G_{t: t+n} = R_{t+1}+\cdots+\gamma^{n-1} R_{t+n}+\gamma^{n}\hat{q}\left(S_{t+n}, A_{t+n}, \mathbf{w}_{t+n-1}\right), \quad t+n&lt;T\]</span></p><p>　　<span class="math inline">\(t + n \gt T\)</span> 时 <span class="math inline">\(G_{t: t+n} = G_t\)</span>，结合 n-stepreturn，构造动作价值形式的 Truncated <span class="math inline">\(\lambda\)</span>-return <span class="math inline">\(G_{\lambda}^t\)</span>，进而得到动作价值形式的offline <span class="math inline">\(\lambda\)</span>-return 算法：</p><p><span class="math display">\[\mathbf{w}_{t+1} =\mathbf{w}_{t}+\alpha\left[G_{t}^{\lambda}-\hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right)\right] \nabla \hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right), \quad t=0, \ldots, T-1\]</span></p><p>　　backup 图如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/9.png"></p><p>　　SARSA(<span class="math inline">\(\lambda\)</span>) 的更新式也与TD(<span class="math inline">\(\lambda\)</span>) 的类似：</p><p><span class="math display">\[\begin{aligned}&amp;\mathbf{w}_{t+1} = \mathbf{w}_{t}+\alpha \delta_{t}\mathbf{z}_{t}\\&amp;\begin{array}{l}{\delta_{t} = R_{t+1}+\gamma \hat{q}\left(S_{t+1}, A_{t+1},\mathbf{w}_{t}\right)-\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)}\\{\mathbf{z}_{-1} = \mathbf{0}}\end{array}\\&amp;\mathbf{z}_{t} = \gamma \lambda \mathbf{z}_{t-1}+\nabla\hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)\end{aligned}\]</span></p><p>　　伪代码如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/10.png"></p><p>　　上面是 offline 版本的，我们再看看 True Online SARSA(<span class="math inline">\(\lambda\)</span>)：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/11.png"></p><h1 id="variable-lambda-and-gamma">Variable <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span></h1><p>　　为了更泛化地表示每一步中自益（bootstrapping）和折扣（discounting）的程度，需要更通用的<span class="math inline">\(\lambda\)</span> 和 <span class="math inline">\(\gamma\)</span>。定义函数 <span class="math inline">\(\lambda: \mathcal{S} \times \mathcal{A}\rightarrow[0,1]\)</span>，有 <span class="math inline">\(\lambda_t=\lambda(S_t,A_t)\)</span>。定义函数<span class="math inline">\(\gamma: \mathcal{S}\rightarrow[0,1]\)</span>，有 <span class="math inline">\(\gamma_t =\gamma(S_t)\)</span>。<br>　　我们先谈谈对折扣的泛化，函数 <span class="math inline">\(\gamma\)</span> 称为 terminationfunction，重新定义 return ：</p><p><span class="math display">\[\begin{aligned}G_{t} &amp; = R_{t+1}+\gamma_{t+1} G_{t+1} \\&amp;=R_{t+1}+\gamma_{t+1} R_{t+2}+\gamma_{t+1} \gamma_{t+2}R_{t+3}+\gamma_{t+1} \gamma_{t+2} \gamma_{t+3} R_{t+4}+\cdots \\&amp;=\sum_{k=t}^{\infty}\left(\prod_{i=t+1}^{k} \gamma_{i}\right)R_{k+1}\end{aligned}\]</span></p><p>　　为保证结果有限，需要 <span class="math inline">\(\Pi_{k=t}^{\infty} \gamma_t =0\)</span>。这种形式的好处是，基于回合任务无需再指定开始状态和结束状态，只需使<span class="math inline">\(\gamma(s)=0\)</span>即可，因此便能统一回合和折扣。<br>　　下面介绍对自益的泛化。若对状态价值做自益，则泛化参数记为 <span class="math inline">\(\lambda_s\)</span> ，同理 <span class="math inline">\(\lambda_a\)</span> 为对动作价值做自益，<span class="math inline">\(\lambda\)</span> 控制了自益的程度，当为 1时完全不做自益，当为 0时则完全是在做自益，于是可以得到新的递归形式的基于状态价值的 <span class="math inline">\(\lambda\)</span>-return ：</p><p><span class="math display">\[G_{t}^{\lambda s} =R_{t+1}+\gamma_{t+1}\left(\left(1-\lambda_{t+1}\right)\hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)+\lambda_{t+1}G_{t+1}^{\lambda s}\right)\]</span></p><p>　　基于动作价值的 <span class="math inline">\(\lambda\)</span>-return 也类似： <span class="math display">\[G_{t}^{\lambda a} =R_{t+1}+\gamma_{t+1}\left(\left(1-\lambda_{t+1}\right)\hat{q}\left(S_{t+1}, A_{t+1}, \mathbf{w}_{t}\right)+\lambda_{t+1}G_{t+1}^{\lambda a}\right)\]</span></p><p>　　还有 Expected SARSA 形式：</p><p><span class="math display">\[G_{t}^{\lambda a} =R_{t+1}+\gamma_{t+1}\left(\left(1-\lambda_{t+1}\right)\bar{V}_{t}\left(S_{t+1}\right)+\lambda_{t+1} G_{t+1}^{\lambda a}\right)\\\bar{V}_{t}(s) = \sum_{a} \pi(a | s) \hat{q}\left(s, a,\mathbf{w}_{t}\right)\]</span></p><h1 id="off-policy-traces-with-control-variates">Off-policy Traces withControl Variates</h1><p>　　将重要性采样整合进算法得到异策算法。采用 per-decision方法，基于状态的 <span class="math inline">\(\lambda\)</span>-return：</p><p><span class="math display">\[G_{t}^{\lambda s} =\rho_{t}\left(R_{t+1}+\gamma_{t+1}\left(\left(1-\lambda_{t+1}\right)\hat{v}\left(S_{t+1}, \mathbf{w}_{t}\right)+\lambda_{t+1}G_{t+1}^{\lambda s}\right)\right)+\left(1-\rho_{t}\right)\hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \\\rho_{t}=\frac{\pi\left(A_{t} | S_{t}\right)}{b\left(A_{t} |S_{t}\right)}\]</span></p><p>　　利用 TD error 逼近得到 Truncated <span class="math inline">\(G_{t}^{\lambda s}\)</span>：</p><p><span class="math display">\[\begin{aligned}&amp;\delta_{t}^{s} = R_{t+1}+\gamma_{t+1} \hat{v}\left(S_{t+1},\mathbf{w}_{t}\right)-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\\&amp;G_{t}^{\lambda s} \approx \hat{v}\left(S_{t},\mathbf{w}_{t}\right)+\rho_{t} \sum_{k=t}^{\infty} \delta_{k}^{s}\prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \rho_{i}\end{aligned}\]</span></p><p>　　这样就很容易进行基于前向视角的更新：</p><p><span class="math display">\[\begin{aligned}\mathbf{w}_{t+1} &amp;=\mathbf{w}_{t}+\alpha\left(G_{t}^{\lambdas}-\hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right) \nabla\hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \\&amp; \approx \mathbf{w}_{t}+\alpha \rho_{t}\left(\sum_{k=t}^{\infty}\delta_{k}^{s} \prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \rho_{i}\right)\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\end{aligned}\]</span></p><p>　　我们来探究一下前向视角和后向视角的近似关系。对整个前向视角的过程进行求和，得到：</p><p><span class="math display">\[\begin{aligned}&amp;\begin{aligned}\sum_{t=1}^{\infty}\left(\mathbf{w}_{t+1}-\mathbf{w}_{t}\right) &amp;\approx \sum_{t=1}^{\infty} \sum_{k=t}^{\infty} \alpha \rho_{t}\delta_{k}^{s} \nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \rho_{i} \\&amp;=\sum_{k=1}^{\infty} \sum_{t=1}^{k} \alpha \rho_{t} \nabla\hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \delta_{k}^{s}\prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \rho_{i}\end{aligned}\\&amp;\text { (using the summation rule: }\left.\sum_{t=x}^{y}\sum_{k=t}^{y}=\sum_{k=x}^{y} \sum_{t=x}^{k}\right)\\&amp;=\sum_{k=1}^{\infty} \alpha \delta_{k}^{s} \sum_{t=1}^{k} \rho_{t}\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k}\gamma_{i} \lambda_{i} \rho_{i}\end{aligned}\]</span></p><p>　　若上式的第二个求和项可写作资格迹并用于更新，则更新式变为基于后向视角的TD update，也就是若表达式为 k 时刻的 trace，那他可由 k-1时刻的值更新而得：</p><p><span class="math display">\[\begin{aligned}\mathbf{z}_{k} &amp;=\sum_{t=1}^{k} \rho_{t} \nabla \hat{v}\left(S_{t},\mathbf{w}_{t}\right) \prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \rho_{i}\\&amp;=\sum_{t=1}^{k-1} \rho_{t} \nabla \hat{v}\left(S_{t},\mathbf{w}_{t}\right) \prod_{i=t+1}^{k} \gamma_{i} \lambda_{i}\rho_{i}+\rho_{k} \nabla \hat{v}\left(S_{k}, \mathbf{w}_{k}\right) \\&amp;=\gamma_{k} \lambda_{k} \rho_{k} \sum_{t=1}^{k-1} \rho_{t} \nabla\hat{v}\left(S_{t}, \mathbf{w}_{t}\right) \prod_{i=t+1}^{k-1} \gamma_{i}\lambda_{i} \rho_{i} \\&amp;=\rho_{k}\left(\gamma_{k} \lambda_{k} \mathbf{z}_{k-1}+\nabla\hat{v}\left(S_{k}, \mathbf{w}_{k}\right)\right)\end{aligned}\]</span></p><p>　　更加通用的形式：</p><p><span class="math display">\[\mathbf{z}_{t} = \rho_{t}\left(\gamma_{t} \lambda_{t}\mathbf{z}_{t-1}+\nabla \hat{v}\left(S_{t}, \mathbf{w}_{t}\right)\right)\]</span></p><p>　　这个资格迹结合半梯度 TD(<span class="math inline">\(\lambda\)</span>) 更新即为一般的 TD(<span class="math inline">\(\lambda\)</span>) 算法，<span class="math inline">\(\rho_t=1\)</span>时就变成了同策算法。在异策情况下，算法性能还不错，但是作为一个半梯度算法，稳定性欠佳。<br>　　基于状态价值的情况类似，基于动作价值的 <span class="math inline">\(\lambda\)</span>-return ：</p><p><span class="math display">\[\begin{aligned}G_{t}^{\lambda a} &amp; =R_{t+1}+\gamma_{t+1}\left(\left(1-\lambda_{t+1}\right)\bar{V}_{t}\left(S_{t+1}\right)+\lambda_{t+1}\left[\rho_{t+1}G_{t+1}^{\lambda a}+\bar{V}_{t}\left(S_{t+1}\right)-\rho_{t+1}\hat{q}\left(S_{t+1}, A_{t+1}, \mathbf{w}_{t}\right)\right]\right) \\&amp;=R_{t+1}+\gamma_{t+1}\left(\bar{V}_{t}\left(S_{t+1}\right)+\lambda_{t+1}\rho_{t+1}\left[G_{t+1}^{\lambda a}-\hat{q}\left(S_{t+1}, A_{t+1},\mathbf{w}_{t}\right)\right]\right)\end{aligned}\\\bar{V}_{t}(s) = \sum_{a} \pi(a | s) \hat{q}\left(s, a,\mathbf{w}_{t}\right)\]</span></p><p>　　用基于动作价值的 TD error 来逼近 <span class="math inline">\(\lambda\)</span>-return ：</p><p><span class="math display">\[G_{t}^{\lambda a} \approx \hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right)+\sum_{k=t}^{\infty} \delta_{k}^{a}\prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \rho_{i} \\\delta_{t}^{a}=R_{t+1}+\gamma_{t+1}\bar{V}_{t}\left(S_{t+1}\right)-\hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right)\]</span></p><p>　　做类似变换，得到动作价值下的资格迹：</p><p><span class="math display">\[\mathbf{z}_{t} \doteq \gamma_{t} \lambda_{t} \rho_{t}\mathbf{z}_{t-1}+\nabla \hat{q}\left(S_{t}, A_{t}, \mathbf{w}_{t}\right)\]</span></p><p>　　将其用于半梯度更新可得到更一般的 SARSA(<span class="math inline">\(\lambda\)</span>) ，同样可通用于同策和异策。<br>　　当 <span class="math inline">\(\lambda=1\)</span> 这些算法和 MC联系密切，而 <span class="math inline">\(\lambda \lt 1\)</span>时，上面所有的 off-policy 算法都将面临致命三因素（the deadlytriad），也就是 approximation、bootstrapping、off-policy这三个问题。</p><h1 id="watkinss-qlambda-to-tree-backuplambda">Watkins's Q(<span class="math inline">\(\lambda\)</span>) to Tree-Backup(<span class="math inline">\(\lambda\)</span>)</h1><p>　　Watkins's Q(<span class="math inline">\(\lambda\)</span>) 是在Q-learning 上使用资格迹的算法，若采取 greedyaction，则对资格迹进行衰退，否则，就将首个 non-greedy action之后的迹重置为 0。backup 图如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/12.png"></p><p>　　将资格迹结合进无需 importance sampling 的 n-step tree backup算法就得到了 <span class="math inline">\(TB(\)</span>$)。backup图如下：</p><p><img src="/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/13.png"></p><p>　　它使用的 <span class="math inline">\(\lambda\)</span>-return是基于动作价值的递归式：</p><p><span class="math display">\[\begin{aligned}G_{t}^{\lambda a} &amp; \doteqR_{t+1}+\gamma_{t+1}\left(\left(1-\lambda_{t+1}\right)\bar{V}_{t}\left(S_{t+1}\right)+\lambda_{t+1}\left[\sum_{a \neq A_{t+1}}\pi\left(a | S_{t+1}\right) \hat{q}\left(S_{t+1}, a,\mathbf{w}_{t}\right)+\pi\left(A_{t+1} | S_{t+1}\right) G_{t+1}^{\lambdaa}\right]\right) \\&amp;=R_{t+1}+\gamma_{t+1}\left(\bar{V}_{t}\left(S_{t+1}\right)+\lambda_{t+1}\pi\left(A_{t+1} | S_{t+1}\right)\left(G_{t+1}^{\lambdaa}-\hat{q}\left(S_{t+1}, A_{t+1}, \mathbf{w}_{t}\right)\right)\right)\end{aligned}\]</span></p><p>　　对 <span class="math inline">\(\lambda\)</span>-return做逼近可得：</p><p><span class="math display">\[\begin{aligned}&amp;\delta_{t}^{a}=R_{t+1}+\gamma_{t+1}\bar{V}_{t}\left(S_{t+1}\right)-\hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right)\\&amp;G_{t}^{\lambda a} \approx \hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right)+\sum_{k=t}^{\infty} \delta_{k}^{a}\prod_{i=t+1}^{k} \gamma_{i} \lambda_{i} \pi\left(A_{i} | S_{i}\right)\end{aligned}\]</span></p><p>　　即可得到资格迹的更新式：</p><p><span class="math display">\[\mathbf{z}_{t} \doteq \gamma_{t} \lambda_{t} \pi\left(A_{t} |S_{t}\right) \mathbf{z}_{t-1}+\nabla \hat{q}\left(S_{t}, A_{t},\mathbf{w}_{t}\right)\]</span></p><p>　　按照之前的更新规则可得：</p><p><span class="math display">\[\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t} \mathbf{z}_{t}\]</span></p><p>　　这便是 TB(<span class="math inline">\(\lambda\)</span>)算法，不是非常稳定，还需要结合其他 方法。</p><h1 id="stable-off-policy-methods-with-traces">Stable Off-policy Methodswith Traces</h1><p>　　前面介绍的一些资格迹方法是可以在异策中取得稳定解的，下面四种最为重要的使用了自益和折扣的函数，他们的思想都是基于Gradient-TD 和Emphatic-TD。下面的算法都假定了线性函数逼近这一前提，至于非线性，理论上也能做相似的处理。</p><ul><li>GTD(<span class="math inline">\(\lambda\)</span>) ：TDC算法的资格迹形式，目标参数：<span class="math inline">\(\hat{v}(s,\mathbf{w}) \doteq \mathbf{w}_{t}^{\top} \mathbf{x}(s) \approxv_{\pi}(s)\)</span>。更新式： <span class="math display">\[\begin{aligned}\mathbf{w}_{t+1} &amp; \doteq \mathbf{w}_{t}+\alpha \delta_{t}^{s}\mathbf{z}_{t}-\alpha\gamma_{t+1}\left(1-\lambda_{t+1}\right)\left(\mathbf{z}_{t}^{\top}\mathbf{v}_{t}\right) \mathbf{x}_{t+1} \\\mathbf{v}_{t+1} &amp; \doteq \mathbf{v}_{t}+\beta \delta_{t}^{s}\mathbf{z}_{t}-\beta\left(\mathbf{v}_{t}^{\top} \mathbf{x}_{t}\right)\mathbf{x}_{t}\end{aligned}\]</span></li><li>GQ(<span class="math inline">\(\lambda\)</span>) ：Gradient-TD算法（基于动作价值）的资格迹形式，目标参数：<span class="math inline">\(\hat{q}\left(s, a, \mathbf{w}_{t}\right) \doteq\mathbf{w}_{t}^{\top} \mathbf{x}(s, a) \approx q_{\pi}(s,a)\)</span>。更新式： <span class="math display">\[\begin{array}{l}{\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t}^{a}\mathbf{z}_{t}-\alpha\gamma_{t+1}\left(1-\lambda_{t+1}\right)\left(\mathbf{z}_{t}^{\top}\mathbf{v}_{t}\right) \overline{\mathbf{x}}_{t+1}} \\{\overline{\mathbf{x}}_{t} \doteq \sum_{a} \pi\left(a | S_{t}\right)\mathbf{x}\left(S_{t}, a\right)} \\{\delta_{t}^{a} \doteq R_{t+1}+\gamma_{t+1} \mathbf{w}_{t}^{\top}\overline{\mathbf{x}}_{t+1}-\mathbf{w}_{t}^{\top} \mathbf{x}_{t}}\end{array}\]</span></li><li>HTD(<span class="math inline">\(\lambda\)</span>) ：由 GTD(<span class="math inline">\(\lambda\)</span>) 和 TD(<span class="math inline">\(\lambda\)</span>) 的结合算法，目标参数：<span class="math inline">\(\hat{v}(s, \mathbf{w}) \doteq\mathbf{w}_{t}^{\top} \mathbf{x}(s) \approx v_{\pi}(s)\)</span>。更新式： <span class="math display">\[\begin{aligned}\mathbf{w}_{t+1} &amp; \doteq \mathbf{w}_{t}+\alpha \delta_{t}^{s}\mathbf{z}_{t}+\alpha\left(\left(\mathbf{z}_{t}-\mathbf{z}_{t}^{b}\right)^{\top}\mathbf{v}_{t}\right)\left(\mathbf{x}_{t}-\gamma_{t+1}\mathbf{x}_{t+1}\right) \\\mathbf{v}_{t+1} &amp; \doteq \mathbf{v}_{t}+\beta \delta_{t}^{s}\mathbf{z}_{t}-\beta\left(\mathbf{z}_{t}^{b}\mathbf{v}_{t}\right)\left(\mathbf{x}_{t}-\gamma_{t+1}\mathbf{x}_{t+1}\right), \quad \text { with } \mathbf{v}_{0} \doteq\mathbf{0} \\\mathbf{z}_{t} &amp; \doteq \rho_{t}\left(\gamma_{t} \lambda_{t}\mathbf{z}_{t-1}+\mathbf{x}_{t}\right), \quad \text { with }\mathbf{z}_{-1} \doteq \mathbf{0} \\\mathbf{z}_{t}^{b} &amp; \doteq \gamma_{t} \lambda_{t}\mathbf{z}_{t-1}^{b}+\mathbf{x}_{t}, \quad \text { with }\mathbf{z}_{-1}^{b} \doteq \mathbf{0}\end{aligned}\]</span></li><li>Emphatic TD(<span class="math inline">\(\lambda\)</span>) ：EmphaticTD 的资格迹形式，目标参数：<span class="math inline">\(\hat{v}(s,\mathbf{w}) \doteq \mathbf{w}_{t}^{\top} \mathbf{x}(s) \approxv_{\pi}(s)\)</span>，此算法在异策情况下收敛性很强，代价是高方差、慢速，允许任何程度的自益。更新式：<span class="math display">\[\begin{aligned}&amp;\mathbf{w}_{t+1} \doteq \mathbf{w}_{t}+\alpha \delta_{t}\mathbf{z}_{t}\\&amp;\delta_{t} \doteq R_{t+1}+\gamma_{t+1} \mathbf{w}_{t}^{\top}\mathbf{x}_{t+1}-\mathbf{w}_{t}^{\top} \mathbf{x}_{t}\\&amp;\mathbf{z}_{t} \doteq \rho_{t}\left(\gamma_{t} \lambda_{t}\mathbf{z}_{t-1}+M_{t} \mathbf{x}_{t}\right), \text { with }\mathbf{z}_{-1} \doteq \mathbf{0}\\&amp;M_{t} \doteq \lambda_{t} I_{t}+\left(1-\lambda_{t}\right) F_{t}\\&amp;F_{t} \doteq \rho_{t-1} \gamma_{t} F_{t-1}+I_{t}, \quad \text {with } F_{0} \doteq i\left(S_{0}\right)\end{aligned}\]</span> 　　其中，<span class="math inline">\(M_t \ge 0\)</span> 代表emphasis，<span class="math inline">\(F_t \ge 0\)</span> 代表 followontrace，<span class="math inline">\(I_t \ge 0\)</span> 代表interest。</li></ul><h1 id="总结">总结</h1><p>　　本文根据强化学习圣经记录了许多资格迹的算法，几乎强化学习中的所有算法都能结合资格迹，需要好好的理解。</p><h1 id="refer">Refer</h1><ul><li><a href="https://book.douban.com/subject/2866455/">ReinforcementLearning</a></li><li><a href="https://book.douban.com/subject/34478302/">强化学习：原理与Python实现</a></li></ul><h1 id="相关内容">相关内容</h1><ul><li><a href="http://chengfeng96.com/blog/2020/02/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89-Markov-%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%A8%A1%E5%9E%8B/">强化学习笔记（一）-Markov 决策过程模型</a></li><li><a href="http://chengfeng96.com/blog/2020/02/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/">强化学习笔记（二）-动态规划方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/">强化学习笔记（三）-蒙特卡罗方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/">强化学习笔记（四）-时序差分学习</a></li><li><a href="https://chengfeng96.com/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/">强化学习笔记（五）-函数近似方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/">强化学习笔记（六）-策略梯度</a></li><li><a href="https://chengfeng96.com/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/">强化学习笔记（八）-连续空间的确定性策略</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　资格迹（Eligibility
Trace）是强化学习中一种非常基础的机制，它可以使时序差分学习更加高效，能在回合更新（MC
方法）和单步时序差分更新（TD(0)）间进行折中。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="Reinforcement Learning" scheme="http://chengfeng96.com/categories/AI/Reinforcement-Learning/"/>
    
    
    <category term="RL" scheme="http://chengfeng96.com/tags/RL/"/>
    
    <category term="Temporal-Difference Learning" scheme="http://chengfeng96.com/tags/Temporal-Difference-Learning/"/>
    
    <category term="Eligibility Trace" scheme="http://chengfeng96.com/tags/Eligibility-Trace/"/>
    
    <category term="Dutch Trace" scheme="http://chengfeng96.com/tags/Dutch-Trace/"/>
    
    <category term="Accumulating Trace" scheme="http://chengfeng96.com/tags/Accumulating-Trace/"/>
    
    <category term="Replacing Trace" scheme="http://chengfeng96.com/tags/Replacing-Trace/"/>
    
    <category term="on-policy" scheme="http://chengfeng96.com/tags/on-policy/"/>
    
    <category term="off-policy" scheme="http://chengfeng96.com/tags/off-policy/"/>
    
    <category term="SARSA" scheme="http://chengfeng96.com/tags/SARSA/"/>
    
    <category term="Q-learning" scheme="http://chengfeng96.com/tags/Q-learning/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记（六）- 策略梯度</title>
    <link href="http://chengfeng96.com/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/"/>
    <id>http://chengfeng96.com/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/</id>
    <published>2020-02-21T08:11:59.000Z</published>
    <updated>2020-06-18T03:58:40.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　之前我们几篇文章提到的算法都是最优价值算法（optimal valuealgorithm），因为它们在求解最优策略的过程中试图估计最优策略。本文提到的策略梯度（PolicyGradient）算法，它求解最优策略不一定要估计最优价值函数，而试图用含参函数近似最优策略，并通过迭代更新参数值。</p><span id="more"></span><h1 id="策略近似">策略近似</h1><p>　　用函数近似方法估计最优策略 <span class="math inline">\(\pi(a|s)\)</span> 的基本思想使用含参函数 <span class="math inline">\(\pi(a|s;\boldsymbol{\theta})\)</span>来近似最优策略。为了满足 <span class="math inline">\(\sum_a \pi(a|s) =1\)</span>，我们引入动作偏好函数（Action Preference Function），它的softmax 值为 <span class="math inline">\(\pi(a|s;\boldsymbol{\theta})\)</span>：</p><p><span class="math display">\[\pi(a | s, \boldsymbol{\theta}) \doteq \frac{e^{h(s, a,\boldsymbol{\theta})}}{\sum_{b} e^{h(s, b, \boldsymbol{\theta})}}\]</span></p><p>　　之前我们从动作价值函数导出的最优策略估计往往有特定的形式，例如<span class="math inline">\(\epsilon\)</span>贪心策略，然而从动作偏好导出的最优策略的估计则不拘泥于特定的形式，<span class="math inline">\(\boldsymbol{\theta}\)</span>随着迭代可以逼近最优策略。</p><h1 id="策略梯度定理">策略梯度定理</h1><p>　　策略梯度定理给出了期望回报和策略梯度之间的关系，我们给出回合制任务下的目标函数，它是Start Value 形式的：</p><p><span class="math display">\[J(\boldsymbol{\theta}) \doteq v_{\pi_{\theta}}\left(s_{0}\right)\]</span></p><p>　　除了 Start Value 形式的目标函数，还有 Average Value 形式的 <span class="math inline">\(J(\boldsymbol{\theta}) = \sum_s\mu(s)v_{\pi_{\theta}}(s)\)</span> 和 Average reward per time-step形式的 <span class="math inline">\(J(\boldsymbol{\theta}) = \sum_s\mu(s) \sum_a \pi\left(a | s, \boldsymbol{\theta}\right)q(s,a)\)</span>。其中 <span class="math inline">\(\mu(s)\)</span> 是状态<span class="math inline">\(s\)</span> 的分布函数，其满足 <span class="math inline">\(\mu(s) \ge 0, \sum_s \mu(s) = 1\)</span>。</p><p>　　我们先给出策略梯度定理给出的结论，目标函数无论是何种形式，对任意的MDP 过程，目标函数对策略参数的梯度均为如下形式：</p><p><span class="math display">\[\begin{aligned}\nabla J(\boldsymbol{\theta}) &amp; \propto \sum_{s} \mu(s) \sum_{a}q_{\pi}(s, a) \nabla \pi(a | s, \boldsymbol{\theta}) \\&amp;=\mathbb{E}_{\pi}\left[\sum_{a} q_{\pi}\left(S_{t}, a\right) \nabla\pi(a | S_{t}, \boldsymbol{\theta})\right]\end{aligned}\]</span></p><p>　　下面是 Start Value 形式的目标函数的证明过程：</p><p><img src="/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/1.png"></p><p>　　根据此定理，目标函数的梯度等于策略函数梯度与 Q值两部分乘积的期望，这两部分都是较为容易确定的，因此参数的更新就变得容易了。</p><h1 id="同策回合更新策略梯度算法">同策回合更新策略梯度算法</h1><h2 id="reinforcemente-carlo-policy-gradient">REINFORCE：Mente CarloPolicy Gradient</h2><p>　　这里所说的同策回合更新策略梯度算法主要就是蒙特卡洛策略梯度算法，在每一个回合结束后，我们可以做以下更新：</p><p><span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha \sum_a\hat{q}\left(S_{t}, a, \mathbf{w}\right) \nabla \pi\left(a | S_{t},\boldsymbol{\theta}\right)\]</span></p><p>　　上式列出的方法也叫做 all-actions方法，因为它每次更新都需要用到所有动作，下面我们只考虑在 <span class="math inline">\(t\)</span> 时刻采取 <span class="math inline">\(A_t\)</span> 的情况：</p><p><span class="math display">\[\begin{aligned}\nabla J(\boldsymbol{\theta})&amp;=\mathbb{E}_{\pi}\left[\sum_{a}\pi\left(a | S_{t}, \boldsymbol{\theta}\right) q_{\pi}\left(S_{t},a\right) \frac{\nabla \pi\left(a | S_{t},\boldsymbol{\theta}\right)}{\pi\left(a | S_{t},\boldsymbol{\theta}\right)}\right]\\&amp;\begin{array}{ll}{=\mathbb{E}_{\pi}\left[q_{\pi}\left(S_{t}, A_{t}\right) \frac{\nabla\pi\left(A_{t} | S_{t}, \boldsymbol{\theta}\right)}{\pi\left(A_{t} |S_{t}, \boldsymbol{\theta}\right)}\right]} &amp; { \text { (replacing}\left.a \text { by the sample } A_{t} \sim \pi\right)} \\{=\mathbb{E}_{\pi}\left[G_{t} \frac{\nabla \pi\left(A_{t} | S_{t},\boldsymbol{\theta}\right)}{\pi\left(A_{t} | S_{t},\boldsymbol{\theta}\right)}\right],} &amp; { \text { (because}\left.\mathbb{E}_{\pi}\left[G_{t} | S_{t},A_{t}\right]=q_{\pi}\left(S_{t}, A_{t}\right)\right)}\end{array}\end{aligned}\]</span></p><p>　　于是更新式可以变为：</p><p><span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t}\frac{\nabla \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)}\]</span></p><p>　　因为 <span class="math inline">\(\nabla \ln x=\frac{\nablax}{x}\)</span>，所以迭代式又可以变为：</p><p><span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha G_{t}\nabla \ln \pi\left(A_{t} | S_{t}, \boldsymbol{\theta}_{t}\right)\]</span></p><p>　　<span class="math inline">\(\nabla \ln \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)\)</span> 又被称为 eligibilityvector，这样的算法也称为 REINFORCE（REward Increment = NonnegativeFactor ✖ Offset Reinforcement ✖ CharasteristicEligibility），表示了增量是由三个部分组成的。<br>　　算法的伪代码如下，其中加入了折扣因子，之前我们一直忽略折扣因子，是因为默认<span class="math inline">\(\gamma=1\)</span>：</p><p><img src="/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/2.png"></p><h2 id="reinforce-with-baseline">REINFORCE with Baseline</h2><p>　　我们对策略梯度算法做一些改进，给它加入基线，引入基线函数 <span class="math inline">\(B(s)\)</span>：</p><p><span class="math display">\[\nabla J(\boldsymbol{\theta}) \propto \sum_{s} \mu(s)\sum_{a}\left(q_{\pi}(s, a)-b(s)\right) \nabla \pi(a | s,\boldsymbol{\theta})\]</span></p><p>　　它可以降低学习过程中的方差，基线函数可以是任意随机函数或确定函数，它可以与状态<span class="math inline">\(s\)</span> 有关，但是不能和动作 <span class="math inline">\(a\)</span> 有关。添加基线有效，是因为下式为0：</p><p><span class="math display">\[\sum b(s) \nabla \pi(a | s, \boldsymbol{\theta})=b(s) \nabla \sum \pi(a| s, \boldsymbol{\theta})=b(s) \nabla 1=0\]</span></p><p>　　更新式也就变为了：</p><p><span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq\boldsymbol{\theta}_{t}+\alpha\left(G_{t}-b\left(S_{t}\right)\right)\frac{\nabla \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)}{\pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)}\]</span></p><p>　　一个能有效降低方差的基线是状态价值函数的估计，下面的伪代码给出了用状态价值函数的估计作为基线的算法，算法里的两套参数<span class="math inline">\(\boldsymbol{\theta}\)</span> 和 <span class="math inline">\(\boldsymbol{w}\)</span>分别是最优策略估计和最优状态价值函数估计的参数，每次迭代时，它们都可以以各自的学习算法进行学习：</p><p><img src="/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/3.png"></p><h1 id="异策回合更新策略梯度算法">异策回合更新策略梯度算法</h1><p>　　在简单的策略梯度算法上引入重要性采样，就可以得到对应的异策算法，记行为策略<span class="math inline">\(b(a|S_t)\)</span>：</p><p><span class="math display">\[\begin{aligned}&amp; \sum_{a} \pi\left(a | S_{t}, \boldsymbol{\theta}\right)q_{\pi}\left(S_{t}, a\right) \ln \nabla \pi\left(a | S_{t},\boldsymbol{\theta}\right) \\&amp;= \sum_{a} b(a|S_t) \frac{\pi\left(a | S_{t},\boldsymbol{\theta}\right)}{b(a|S_t)} q_{\pi}\left(S_{t}, a\right) \ln\nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right) \\&amp;= \sum_{a} b(a|S_t) \frac{1}{b(a|S_t)} q_{\pi}\left(S_{t}, a\right)\ln \nabla \pi\left(a | S_{t}, \boldsymbol{\theta}\right)\end{aligned}\]</span></p><p>　　即：</p><p><span class="math display">\[\mathbb{E}_{\pi}\left[\gamma^t G_t \ln \nabla \pi(A_t | S_{t},\boldsymbol{\theta}) \right] = \mathbb{E}_{b}\left[\frac{1}{b(A_t|S_t)}\gamma^t G_t \ln \nabla \pi(A_t | S_{t}, \boldsymbol{\theta}) \right]\]</span></p><h1 id="actorcritic-算法">Actor–Critic 算法</h1><p>　　我们之前说的算法基本只能用于回合制任务，下面说的 Actor-Critic算法不仅可以用于回合制任务，还可以用于连续性任务，用于时序差分，可以使用自益。<br>　　Actor-Critic 算法是策略（Policy Based）和价值（ValueBased）相结合的方法，这种算法就是通过引入一种评价机制来解决高方差的问题。具体来说，Critic就类似于策略评估，去估计动作值函数，而Actor就是我们之前说到的策略函数，负责生成动作并和环境交互。所以Actor-Critic算法中就有两组参数，也就是两个估计：</p><ul><li>Critic：更新价值函数的估计，例如 <span class="math inline">\(\hat{v}(s, w) \approx v_{\pi}(s)\)</span>，<span class="math inline">\(\hat{q}(s, a, w) \approx q_{\pi}(s,a)\)</span></li><li>Actor：策略函数的估计，<span class="math inline">\(\pi_{\boldsymbol{\theta}}(s, a)=P(a | s,\boldsymbol{\theta}) \approx \pi(a | s)\)</span></li></ul><p>　　再利用 MC 方法 REINFORCE 中，梯度更新部分中，eligibility vector是不用动的，要变成 Actor 的话改动的是 <span class="math inline">\(G_t\)</span>，这块不能再使用 MC 来得到，而应该从Critic 得到。而对于 Critic 来说，可以参考之前DQN的做法，用一个 Q网络来做为 Critic, 这个 Q网络的输入可以是状态，而输出是每个动作的价值或者最优动作的价值。<br>　　总的来说，拿评估点 <span class="math inline">\(v_t\)</span>举例来说，就是 Critic 通过 Q 网络计算状态的最优价值 <span class="math inline">\(v_t\)</span>，而 Actor 利用 <span class="math inline">\(v_t\)</span> 这个最优价值迭代更新策略函数的参数<span class="math inline">\(\boldsymbol{\theta}\)</span>，进而选择动作，并得到反馈和新的状态，Critic使用反馈和新的状态更新 Q 网络参数 <span class="math inline">\(\boldsymbol{w}\)</span>，在后面 Critic会使用新的网络参数 <span class="math inline">\(\boldsymbol{w}\)</span>来帮 Actor 计算状态的最优价值 <span class="math inline">\(v_t\)</span>。<br>　　除了评估 <span class="math inline">\(v_t\)</span>，还有很多其他的指标来做为 Critic的评估点：</p><ul><li>基于状态价值：<span class="math inline">\(v(s,\boldsymbol{w})\)</span>，更新公式为： <span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha v(s,\boldsymbol{w}) \nabla \ln \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)\]</span></li><li>基于动作价值：<span class="math inline">\(q(s,a,\boldsymbol{w})\)</span>，更新公式为： <span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alphaq(s,a,\boldsymbol{w}) \nabla \ln \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)\]</span></li><li>基于 TD 误差：<span class="math inline">\(\delta(t) = r_{t+1} +\gamma v(S_{t+1}) - v(S_t)\)</span> 或 <span class="math inline">\(\delta(t) = r_{t+1} + \gamma q(S_{t+1}, A_{t+1}) -q(S_{t}, A_{t})\)</span>，更新公式为： <span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha\delta(t) \nabla \ln \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)\]</span></li><li>基于优势函数：<span class="math inline">\(a(s,a;\mathbf{w}) =q(s,a;\mathbf{w}) - v(s;\mathbf{w})\)</span>，更新式为： <span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alphaa(s,a;\mathbf{w}) \nabla \ln \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)\]</span></li><li>基于 TD(<span class="math inline">\(\lambda\)</span>) 误差：是 TD误差和效用迹 E的乘积 <span class="math display">\[\boldsymbol{\theta}_{t+1} \doteq \boldsymbol{\theta}_{t}+\alpha\delta(t) E(t) \nabla \ln \pi\left(A_{t} | S_{t},\boldsymbol{\theta}_{t}\right)\]</span></li></ul><p>　　对于 Critic 本身的模型参数 <span class="math inline">\(\boldsymbol{w}\)</span>，一般都是使用均方误差损失函数来做做迭代更新。<br>　　下面是一个基于单步 TD 估计的伪代码：</p><p><img src="/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/4.png"></p><p>　　下面是带资格迹的：</p><p><img src="/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/5.png"></p><h1 id="总结">总结</h1><p>　　我们可以看出 策略梯度有以下优点：</p><ul><li>基于策略的学习可能会具有更好的收敛性，这是因为基于策略的学习虽然每次只改善一点点，但总是朝着好的方向在改善，但是有些价值函数在后期会一直围绕最优价值函数持续小的震荡而不收敛。</li><li>在对于那些拥有高维度或连续状态空间来说，使用基于价值函数的学习在得到价值函数后，制定策略时，需要比较各种行为对应的价值大小，这样如果行为空间维度较高或者是连续的，则从中比较得出一个有最大价值函数的行为这个过程就比较难了，这时候使用基于策略的学习就高效的多。</li><li>能够学到一些随机策略</li><li>有时候计算价值函数非常复杂。比如当小球从从空中某个位置落下你需要左右移动接住时，计算小球在某一个位置时采取什么行为的价值是很难得，但是基于策略就简单许多，你只需要朝着小球落地的方向移动修改策略就行。</li></ul><p>　　但是策略梯度也有些缺点，原始的、未经改善（Naive）的基于策略的学习有时候效率不够高，有时候还有较高的变异性（方差，Variance）。因为基于价值函数的策略决定每次都是推促个体去选择一个最大价值的行为；但是基于策略的，更多的时候策略的选择时仅会在策略某一参数梯度上移动一点点，使得整个的学习比较平滑，因此不够高效。有时候计算朝着梯度方向改变的增量也会有较高的变异性（方差），以至于拖累了整个算法速度，但是通过一些修饰，可以改进。</p><h1 id="refer">Refer</h1><ul><li><a href="https://book.douban.com/subject/2866455/">ReinforcementLearning</a></li><li><a href="https://book.douban.com/subject/34478302/">强化学习：原理与Python实现</a></li><li><a href="https://zhuanlan.zhihu.com/p/93629846">理解策略梯度算法</a></li><li><a href="https://zhuanlan.zhihu.com/p/28348110">《强化学习》第七讲策略梯度</a></li><li><a href="https://zhuanlan.zhihu.com/p/36494307">强化学习——策略梯度与Actor-Critic算法</a></li><li><a href="https://www.cnblogs.com/pinard/p/10272023.html">强化学习(十四)Actor-Critic</a></li></ul><h1 id="相关内容">相关内容</h1><ul><li><a href="http://chengfeng96.com/blog/2020/02/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89-Markov-%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%A8%A1%E5%9E%8B/">强化学习笔记（一）-Markov 决策过程模型</a></li><li><a href="http://chengfeng96.com/blog/2020/02/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/">强化学习笔记（二）-动态规划方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/">强化学习笔记（三）-蒙特卡罗方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/">强化学习笔记（四）-时序差分学习</a></li><li><a href="https://chengfeng96.com/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/">强化学习笔记（五）-函数近似方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/">强化学习笔记（七）-资格迹</a></li><li><a href="https://chengfeng96.com/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/">强化学习笔记（八）-连续空间的确定性策略</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　之前我们几篇文章提到的算法都是最优价值算法（optimal value
algorithm），因为它们在求解最优策略的过程中试图估计最优策略。本文提到的策略梯度（Policy
Gradient）算法，它求解最优策略不一定要估计最优价值函数，而试图用含参函数近似最优策略，并通过迭代更新参数值。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="Reinforcement Learning" scheme="http://chengfeng96.com/categories/AI/Reinforcement-Learning/"/>
    
    
    <category term="RL" scheme="http://chengfeng96.com/tags/RL/"/>
    
    <category term="Monte Carlo Methods" scheme="http://chengfeng96.com/tags/Monte-Carlo-Methods/"/>
    
    <category term="Importance Sampling" scheme="http://chengfeng96.com/tags/Importance-Sampling/"/>
    
    <category term="Policy Gradient" scheme="http://chengfeng96.com/tags/Policy-Gradient/"/>
    
    <category term="Actor–Critic" scheme="http://chengfeng96.com/tags/Actor%E2%80%93Critic/"/>
    
    <category term="function approximation" scheme="http://chengfeng96.com/tags/function-approximation/"/>
    
    <category term="Action Preference Function" scheme="http://chengfeng96.com/tags/Action-Preference-Function/"/>
    
    <category term="REINFORCE" scheme="http://chengfeng96.com/tags/REINFORCE/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记（五）- 函数近似方法</title>
    <link href="http://chengfeng96.com/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/"/>
    <id>http://chengfeng96.com/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/</id>
    <published>2020-02-19T03:47:52.000Z</published>
    <updated>2020-07-09T02:49:09.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　对于有模型的数值迭代算法，无模型的回合更新算法和时序差分更新算法，在每次更新价值函数时都只更新某个状态或状态动作对下的价值估计，这就带来一个问题，如果状态数和动作数巨大的话，甚至无穷大，是不可能做到对这些状态和动作逐一更新的。于是函数近似算法用参数化的模型来近似整个状态价值函数或者动作价值函数，在每次学习时更新函数。</p><span id="more"></span><h1 id="随机梯度下降stochastic-gradient-descent-sgd">随机梯度下降（Stochastic-GradientDescent, SGD ）</h1><p>　　我们列出梯度下降的公式：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/0.png"></p><p>　　这里随机梯度下降中“随机”的含义是我们的更新总是基于单个的样本，或者批样本，而这些样本都是随机选择的。<br>　　上式其实有一个问题，那就是很多时候更新的目标 <span class="math inline">\(v_{\pi}(S_{t})\)</span>是未知的，更一般的情况是我们用一个近似的值 <span class="math inline">\(U_t\)</span>来表示更新目标，它可能是含有噪音的，或者是通过自举得到的 <span class="math inline">\(v_{\pi}(S_{t})\)</span>的一个近似值，在这种情况下，表达式就变成了：</p><p><span class="math display">\[\mathbf{w}_{t+1} \doteq\mathbf{w}_{t}+\alpha\left[U_{t}-\hat{v}\left(S_{t},\mathbf{w}_{t}\right)\right] \nabla \hat{v}\left(S_{t},\mathbf{w}_{t}\right)\tag{2}\]</span></p><p>　　如果 <span class="math inline">\(U_t\)</span> 是无偏的，即 <span class="math inline">\(\mathbb{E}\left[U_{t} |S_{t}=s\right]=v_{\pi}\left(S_{t}\right)\)</span>，那么 <span class="math inline">\(\mathbf{w}_{t}\)</span>可以保证收敛到局部最优。<br>　　用MC估计的值函数 <span class="math inline">\(G_t\)</span>是无偏的，因此我们就可以得到一个基于 MC 的值预测方法，叫做梯度 MC算法，SGD 方法还是非常简单容易理解的，它的伪代码如下：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/1.png"></p><p>　　算法过程很简单，就是根据 MC 方法得到的期望回报 <span class="math inline">\(G_t\)</span>，然后来拟合一个函数 <span class="math inline">\(\hat{v}\)</span>，使得他在每个状态的值都接近 <span class="math inline">\(G_t\)</span>，不断的仿真轨迹，计算 <span class="math inline">\(G_t\)</span>，然后执行随机梯度下降。</p><h1 id="半梯度下降semi-gradient-descent">半梯度下降（Semi-GradientDescent）</h1><p>　　但通常来说都是有偏的估计，也就是说如果 <span class="math inline">\(U_t\)</span> 是通过自举得到的估计，比如 <span class="math inline">\(n\)</span> 步时序差分的回报 <span class="math inline">\(G_{t:t+n}\)</span> ，或者是DP目标 <span class="math inline">\(\sum_{a, s^{\prime}, r} \pi\left(a | S_{t}\right)p\left(s^{\prime}, r | S_{t}, a\right)\left[r+\gamma\hat{v}\left(s^{\prime}, \mathbf{w}_{t}\right)\right]\)</span>。这些目标是依赖于当前的权重值，既然当前权重值不是最优的，那必然得到的目标也是有偏的。<br>　　除此之外，另外一种看待这个问题的思路是，从表达式 <span class="math inline">\((1)\)</span>的求导过程中，如果想要结论成立，那么目标 <span class="math inline">\(U_t\)</span> 必须是独立于 <span class="math inline">\(\mathbf{w}_{t}\)</span>的。但是对于自益的目标，目标也是 <span class="math inline">\(\mathbf{w}_{t}\)</span> 的函数。这种情况下 <span class="math inline">\((2)\)</span>就不再是一个真正的梯度下降。因为它考虑了 <span class="math inline">\(\mathbf{w}_{t}\)</span> 对于估计 <span class="math inline">\(\hat{v}\)</span> 的影响，却没有考虑它对于 <span class="math inline">\(U_{t}\)</span>的影响。结果就是只考虑了部分的梯度，因此我们称这样的方法为半梯度方法（semi-gradientmethods）。它的伪代码如下：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/2.png"></p><p>　　尽管半梯度方法在收敛特性上没有梯度方法鲁棒，但是对于近似器为线性的情况，这种收敛是有保证的。另外的话，由于采用了自益，就使得半梯度方法能够在线、更快的学习，这带来了很多计算优势。<br>　　半梯度下降的 SARSA 算法：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/3.png"></p><p>　　半梯度下降的 <span class="math inline">\(n\)</span> 阶 SARSA算法：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/4.png"></p><h1 id="线性方法">线性方法</h1><p>　　线性近似还是很容易理解的，以状态价值近似为例，我们可以为每个状态定义多个不同的特征，进而定义近似函数为这些特征的线性组合，即：</p><p><span class="math display">\[\hat{v}(s, \mathbf{w}) \doteq \mathbf{w}^{\top} \mathbf{x}(s) \doteq\sum_{i=1}^{d} w_{i} x_{i}(s)\]</span></p><p>　　我们拿时序差分过程来说，阐述一下它的求导过程，对于状态价值函数求导：</p><p><span class="math display">\[\nabla \hat{v}(s, \mathbf{w})=\mathbf{x}(s)\]</span></p><p>　　利用 SGD 可得：</p><p><span class="math display">\[\mathbf{w}_{t+1} \doteq\mathbf{w}_{t}+\alpha\left[U_{t}-\hat{v}\left(S_{t},\mathbf{w}_{t}\right)\right] \mathbf{x}\left(S_{t}\right)\]</span></p><p>　　简记 <span class="math inline">\(\mathbf{x}_t =\mathbf{x}(S_t)\)</span>，把 <span class="math inline">\(U_t\)</span>通过单步时序差分展开可得：</p><p><span class="math display">\[\begin{aligned}\mathbf{w}_{t+1} &amp; \doteq \mathbf{w}_{t}+\alpha\left(R_{t+1}+\gamma\mathbf{w}_{t}^{\top} \mathbf{x}_{t+1}-\mathbf{w}_{t}^{\top}\mathbf{x}_{t}\right) \mathbf{x}_{t} \\&amp;=\mathbf{w}_{t}+\alpha\left(R_{t+1}\mathbf{x}_{t}-\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma\mathbf{x}_{t+1}\right)^{\top} \mathbf{w}_{t}\right)\end{aligned}\]</span></p><p>　　当系统达到稳态时，对于每个 <span class="math inline">\(\mathbf{w}_{t}\)</span>，对于下一个 <span class="math inline">\(\mathbf{w}_{t+1}\)</span> 的期望可以写成：</p><p><span class="math display">\[\mathbb{E}\left[\mathbf{w}_{t+1} |\mathbf{w}_{t}\right]=\mathbf{w}_{t}+\alpha\left(\mathbf{b}-\mathbf{A}\mathbf{w}_{t}\right) \\\mathbf{b} \doteq \mathbb{E}\left[R_{t+1} \mathbf{x}_{t}\right] \in\mathbb{R}^{d} \quad \\\quad \mathbf{A} \doteq\mathbb{E}\left[\mathbf{x}_{t}\left(\mathbf{x}_{t}-\gamma\mathbf{x}_{t+1}\right)^{\top}\right] \in \mathbb{R}^{d} \times\mathbb{R}^{d}\]</span></p><p>　　所以要想达到收敛的话，<span class="math inline">\(\mathbf{w}_{t}\)</span> 和 <span class="math inline">\(\mathbf{w}_{t+1}\)</span> 要相等，即：</p><p><span class="math display">\[\begin{aligned}\mathbf{b}-\mathbf{A} \mathbf{w}_{\mathrm{TD}} &amp;=\mathbf{0} \\\mathbf{b} &amp;=\mathbf{A} \mathbf{w}_{\mathrm{TD}} \\\mathbf{w}_{\mathrm{TD}} &amp; \doteq \mathbf{A}^{-1} \mathbf{b}\end{aligned}\]</span></p><h2 id="线性最小二乘时序差分更新least-squares-td-lstd">线性最小二乘时序差分更新（Least-SquaresTD, LSTD）</h2><p>　　如果我们不想通过迭代求解，也可以直接求解，就是通过最小二乘法，我们还是拿时序差分举例子：</p><p><span class="math display">\[\mathbf{w}_t = \widehat{\mathbf{A}}_{t}^{-1} \widehat{\mathbf{b}}_{t}\\\widehat{\mathbf{A}}_{t} \doteq \sum_{k=0}^{t-1}\mathbf{x}_{k}\left(\mathbf{x}_{k}-\gamma\mathbf{x}_{k+1}\right)^{\top}+\varepsilon \mathbf{I} \quad \\\quad \widehat{\mathbf{b}}_{t} \doteq \sum_{k=0}^{t-1} R_{k+1}\mathbf{x}_{k}\]</span></p><p>　　其中 <span class="math inline">\(\varepsilon \mathbf{I}\)</span>保证了 <span class="math inline">\(\widehat{\mathbf{A}}_{t}\)</span>的可逆性，进一步得到 <span class="math inline">\(\widehat{\mathbf{A}}_{t}^{-1}\)</span> 和 <span class="math inline">\(\widehat{\mathbf{A}}_{t-1}^{-1}\)</span>的关系：</p><p><span class="math display">\[\begin{aligned}\widehat{\mathbf{A}}_{t}^{-1}&amp;=\left(\widehat{\mathbf{A}}_{t-1}+\mathbf{x}_{t-1}\left(\mathbf{x}_{t-1}-\gamma\mathbf{x}_{t}\right)^{\top}\right)^{-1} \\&amp;=\widehat{\mathbf{A}}_{t-1}^{-1}-\frac{\widehat{\mathbf{A}}_{t-1}^{-1}\mathbf{x}_{t-1}\left(\mathbf{x}_{t-1}-\gamma\mathbf{x}_{t}\right)^{\top}\widehat{\mathbf{A}}_{t-1}^{-1}}{1+\left(\mathbf{x}_{t-1}-\gamma\mathbf{x}_{t}\right)^{\top} \widehat{\mathbf{A}}_{t-1}^{-1}\mathbf{x}_{t-1}}\end{aligned}\]</span></p><p>　　LSTD 的伪代码为：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/5.png"></p><h1 id="dqn">DQN</h1><p>　　深度 Q 学习将深度学习与强化学习相结合，核心就是用一个人工神经网络<span class="math inline">\(q(s,a,w)\)</span> 来代替动作价值函数。<br>　　当同时出现异策，自益和函数近似时，无法保证收敛性，会出现训练不稳定或训练困难的问题，针对这些问题，研究者提出了经验回放和目标网络这两种改进。</p><h2 id="经验回放experience-repaly">经验回放（experience repaly）</h2><p>　　经验回放指的就是将经验（历史状态，动作，奖励等）存储起来，再在存储的经验中按一定的规则采样，它可以让经验分布变得更加稳定，提高训练的稳定性。它主要有两个好处：</p><ul><li>在训练 Q网络时，可以消除数据的关联，使得数据更像是独立同分布的，这样可以减少参数更新的方差，更快的收敛。</li><li>能够重复使用经验</li></ul><p>　　它主要包括两个步骤，存储和采样回放，前者是存储轨迹 <span class="math inline">\((S_t,A_t,S_{t+1},A_{t+1})\)</span>，后者是使用某种规则从存储的<span class="math inline">\((S_t,A_t,S_{t+1},A_{t+1})\)</span>中随机取出一条或多条经验。<br>　　从存储的角度可以分为：</p><ul><li>集中式回放：agent 在一个环境中运行，把经验统一存储在经验池中</li><li>分布式回放：agent的多份拷贝同时在多个环境中运行，并将经验统一存储在经验池中</li></ul><p>　　从采样的角度，又可以分为：</p><ul><li>均匀回放：等概率从经验集中取经验，并且用取得的经验来更新最优价值函数</li><li>优先回放（Prioritized Experience Replay,PER）：为经验池里的每个经验指定一个优先级，采样时倾向于选择优先级高的经验，优先级可以通过成比例优先（proportionalpriority）<span class="math inline">\(p_i = (\delta_i +\epsilon)^\alpha\)</span> 和基于排序优先（rank-based priority） <span class="math inline">\(p_i = (\frac{1}{rank_i})^\alpha\)</span>等方法计算。</li></ul><p>　　伪代码如下：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/6.png"></p><h2 id="目标网络target-network">目标网络（target network）</h2><p>　　对于自益的 Q学习，其回报的估计和动作价值都和权重有关，当权重变化时，回报的估计和动作价值都会变化。在学习的过程中，动作价值试图追逐一个变化的回报，也容易出现不稳定。因为DQN 把预测 Q值当做回归问题。回归问题需要监督信号。如果用同一个网络既做预测，又做监督，会造成moving target，在回归问题中，你的目标一直在变，会很难收敛。<br>　　目标网络可以解决这个问题，减少预测的方差，目标网络是在原有的神经网络之外再搭建一份结构完全相同的网络，原有的网络称为评估网络（evaluationnetwork），在学习的过程中，使用目标网络来进行自益得到回报的评估值，作为学习的目标。在权重更新的过程，只更新评估网络的权重，而不是更新目标网络的权重。这样更新权重时针对的目标不会在每次迭代都变化，是一个固定的目标。再完成一定次数的更新后，再将评估网络的权重值付给目标网络，进而进行下一批更新，这样目标网络也能得到更新，更新目标网络的权重时，我们也可以引入学习率采取一个加权平均的做法<span class="math inline">\(\mathbf{w}_{target} \leftarrow(1-\alpha_{target})\mathbf{w}_{target} +\alpha_{target}\mathbf{w}\)</span>。</p><h2 id="double-dqn">Double DQN</h2><p>　　之前说过 Q-learning 会带来最大化偏差，DQN同样也有这个问题，Double DQN可以缓解这个问题，每次更新动作价值时用其中一个网络确定动作，用确定的动作和另一个网络来估计回报。<br>　　伪代码如下：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/7.png"></p><p>　　乍一看，Double DQN 和 Natural DQN 有点像，但它们最本质的区别是Double DQN 不再是直接在目标Q网络里面找各个动作中最大 Q值，而是先在当前Q网络中先找出最大Q值对应的动作，然后利用这个选择出来的动作在目标网络里面去计算目标Q 值。</p><h2 id="dueling-dqn">Dueling DQN</h2><p>　　对偶网络定义了一个新的函数，优势函数（AdvantageFunction），它是动作价值函数和状态价值函数之差：</p><p><span class="math display">\[a(s,a) = q(s,a) -v(s)\]</span></p><p>　　它仍然使用 <span class="math inline">\(q(\mathbf{w})\)</span>值来估计动作价值，只不过此时 <span class="math inline">\(q(\mathbf{w})\)</span>值是状态价值估计和优势函数估计的和：</p><p><span class="math display">\[q(s,a;\mathbf{w}) = v(s;\mathbf{w}) + a(s,a;\mathbf{w})\]</span></p><p>　　<span class="math inline">\(v(s;\mathbf{w})\)</span> 和 <span class="math inline">\(a(s,a;\mathbf{w})\)</span>分别通过不同的网络估计出来，在论文中，这两个网络有一部分是共用的，网络图如下：</p><p><img src="/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/8.png"></p><p>　　同一个 <span class="math inline">\(q(\mathbf{w})\)</span>值可能存在无穷多种分解为 <span class="math inline">\(v(\mathbf{w})\)</span> 和 <span class="math inline">\(a(\mathbf{w})\)</span>，例如一个 <span class="math inline">\(q(s,a;\mathbf{w})\)</span> 可以分解成某个 <span class="math inline">\(v(s;\mathbf{w})\)</span> 和 <span class="math inline">\(a(s,a;\mathbf{w})\)</span>，也可以分解成 <span class="math inline">\(v(s;\mathbf{w}) + a(s,a;\mathbf{w}) -c(s)\)</span>，<span class="math inline">\(c(s)\)</span>是任意一个只和状态 <span class="math inline">\(s\)</span>有关的函数。我们可以通过增加一个优势函数导出的量，使得等效的优势函数满足固定的特征，使得分解唯一，常用两种方法：</p><ul><li>优势函数的最大值，令 <span class="math inline">\(q(s,a;\mathbf{w}) =v(s;\mathbf{w}) + a(s,a;\mathbf{w}) - \max_{a \in A}a(s,a;\mathbf{w})\)</span>，使得等效优势函数 <span class="math inline">\(a^{\prime}(s,a;\mathbf{w}) =a(s,a;\mathbf{w})-\max_{a \in A} a(s,a;\mathbf{w})\)</span> 满足 <span class="math inline">\(\max_{a \in A} a^{\prime}(s,a;\mathbf{w}) =0\)</span></li><li>优势函数的平均值，令 <span class="math inline">\(q(s,a;\mathbf{w}) =v(s;\mathbf{w}) + a(s,a;\mathbf{w}) - \frac{1}{\mathbf{|A|}}\sum_{a \inA} a(s,a;\mathbf{w})\)</span> 使得等效优势函数 <span class="math inline">\(a^{\prime}(s,a;\mathbf{w}) = a(s,a;\mathbf{w}) -\frac{1}{\mathbf{|A|}}\sum_{a \in A} a(s,a;\mathbf{w})\)</span> 满足<span class="math inline">\(\sum_{a \in A} a^{\prime}(s,a;\mathbf{w}) =0\)</span></li></ul><p>　　关于这两个做法的理解，也可以看看这篇<a href="https://ai.stackexchange.com/questions/8128/difficulty-in-understanding-identifiability-in-the-dueling-network-architecture">解答</a>。<br>　　那么为什么要提出优势函数呢，论文给出的解释是在游戏中，存在很多状态，不管你采用什么样的动作，对下一步的状态转变是没什么影响的。这些情况下计算动作的价值函数的意义没有状态函数的价值意义大。例如状态很好，状态价值很大，那么不管动作好坏，对于Q值的影响不是特别大。所以这个时候，为了制定策略，最好是把动作的作用给提取出来，去除状态对于决策的影响。这个也是可以看做是减去了baseline，减少了训练模型的方差，使模型的泛化性能更好，更加稳定。提取出了优势函数之后，对于动作之间的即使细微的差别也更容易发现，并且通过对偶这种方法，从实验中可以发现，它可以使得网络更快收敛。</p><h1 id="总结">总结</h1><p>　　本文主要记录了函数近似的一些方法，基于回报的随机梯度下降，基于自益目标的半梯度下降，还有函数近似的两种主要方法，线性近似和人工神经网络，前者主要利用了最小二乘法，后者主要是DQN，包括 nature DQN，Double DQN 和 DuelingDQN，还有采样回放和目标网络一些概念。</p><h1 id="refer">Refer</h1><ul><li><a href="https://book.douban.com/subject/2866455/">ReinforcementLearning</a></li><li><a href="https://book.douban.com/subject/34478302/">强化学习：原理与Python实现</a></li></ul><h1 id="相关内容">相关内容</h1><ul><li><a href="http://chengfeng96.com/blog/2020/02/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89-Markov-%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%A8%A1%E5%9E%8B/">强化学习笔记（一）-Markov 决策过程模型</a></li><li><a href="http://chengfeng96.com/blog/2020/02/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/">强化学习笔记（二）-动态规划方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/">强化学习笔记（三）-蒙特卡罗方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/">强化学习笔记（四）-时序差分学习</a></li><li><a href="https://chengfeng96.com/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/">强化学习笔记（六）-策略梯度</a></li><li><a href="https://chengfeng96.com/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/">强化学习笔记（七）-资格迹</a></li><li><a href="https://chengfeng96.com/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/">强化学习笔记（八）-连续空间的确定性策略</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　对于有模型的数值迭代算法，无模型的回合更新算法和时序差分更新算法，在每次更新价值函数时都只更新某个状态或状态动作对下的价值估计，这就带来一个问题，如果状态数和动作数巨大的话，甚至无穷大，是不可能做到对这些状态和动作逐一更新的。于是函数近似算法用参数化的模型来近似整个状态价值函数或者动作价值函数，在每次学习时更新函数。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="Reinforcement Learning" scheme="http://chengfeng96.com/categories/AI/Reinforcement-Learning/"/>
    
    
    <category term="RL" scheme="http://chengfeng96.com/tags/RL/"/>
    
    <category term="SGD" scheme="http://chengfeng96.com/tags/SGD/"/>
    
    <category term="DQN" scheme="http://chengfeng96.com/tags/DQN/"/>
    
    <category term="experience repaly" scheme="http://chengfeng96.com/tags/experience-repaly/"/>
    
    <category term="target network" scheme="http://chengfeng96.com/tags/target-network/"/>
    
    <category term="function approximation" scheme="http://chengfeng96.com/tags/function-approximation/"/>
    
    <category term="Semi-Gradient Descent" scheme="http://chengfeng96.com/tags/Semi-Gradient-Descent/"/>
    
    <category term="LSTD" scheme="http://chengfeng96.com/tags/LSTD/"/>
    
    <category term="Double DQN" scheme="http://chengfeng96.com/tags/Double-DQN/"/>
    
    <category term="Dueling DQN" scheme="http://chengfeng96.com/tags/Dueling-DQN/"/>
    
    <category term="Advantage Function" scheme="http://chengfeng96.com/tags/Advantage-Function/"/>
    
  </entry>
  
  <entry>
    <title>强化学习笔记（四）- 时序差分学习</title>
    <link href="http://chengfeng96.com/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/"/>
    <id>http://chengfeng96.com/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-02-16T03:40:20.000Z</published>
    <updated>2020-06-08T05:53:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><em>版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢</em></strong></p><h1 id="引言">引言</h1><p>　　上文我们提到的 MC方法，用于回合制任务中，并且必须等到回合结束之后才可以更新价值估计。而时序差分学习（Temporal-DifferenceLearning）不需要等到回合结束也可以更新价值估计，并且不仅可以用于回合制任务，还可以用于连续性任务。</p><span id="more"></span><h1 id="同策时序差分更新">同策时序差分更新</h1><p>　　我们先来解释下何谓时序差分，在给定策略 <span class="math inline">\(\pi\)</span> 下，状态价值为：</p><p><span class="math display">\[\begin{aligned}v_{\pi}(s) &amp; \doteq \mathbb{E}_{\pi}\left[G_{t} | S_{t}=s\right] \\&amp;=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma G_{t+1} | S_{t}=s\right] \\&amp;=\mathbb{E}_{\pi}\left[R_{t+1}+\gamma v_{\pi}\left(S_{t+1}\right) |S_{t}=s\right]\end{aligned}\]</span></p><p>　　如果用 MC方法来估计价值函数的话，为了得到回报样本，我们就必须从状态对 <span class="math inline">\(s\)</span>出发，一直采样到回合结束。但是对于单步时序差分来说，更新只需要依据 <span class="math inline">\(v_{\pi}(s) = \mathbb{E}_{\pi}\left[R_{t+1}+\gammav_{\pi}\left(S_{t+1}\right) |S_{t}=s\right]\)</span>，只需要采样一步，进而用 <span class="math inline">\(U_t = R_{t+1}+\gamma v_{\pi}(S_{t+1})\)</span>来估计回报样本的值，与由奖励直接计算得到的无偏回报样本 <span class="math inline">\(G_t\)</span> 作区分，<span class="math inline">\(U_t\)</span>表示使用自益得到的有偏回报样本。<br>　　所以，单步时序差分目标可以定义为：</p><p><span class="math display">\[U_{t: t+1} \doteq R_{t+1}+\gamma V_{t}\left(S_{t+1}\right)\]</span></p><p>　　下标 <span class="math inline">\(t: t+1\)</span> 表示用 <span class="math inline">\(S_{t+1}\)</span> 的估计值来估计 <span class="math inline">\(S_t\)</span>。相应的 <span class="math inline">\(n\)</span> 步时序差分目标可以定义为：</p><p><span class="math display">\[U_{t: t+n} \doteq R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^{n} V_{t+n-1}\left(S_{t+n}\right)\]</span></p><p>　　 TD 误差也可以定义为：</p><p><span class="math display">\[\delta_{t} \doteq R_{t+1}+\gammaV\left(S_{t+1}\right)-V\left(S_{t}\right)\]</span></p><p>　　下图是从单步 TD 到多步 TD，再到没有自益，也就是 MC估计的一个示意图：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/1.png"></p><h2 id="时序差分更新策略评估">时序差分更新策略评估</h2><p>　　我们用下面形式的增量更新来学习状态价值函数：</p><p><span class="math display">\[V\left(S_{t}\right) \leftarrowV\left(S_{t}\right)+\alpha\left[G_{t}-V\left(S_{t}\right)\right]\]</span></p><p>　　上面的 <span class="math inline">\(G_t\)</span> 就是我们之前说的<span class="math inline">\(U_t\)</span>，单步时序差分更新评估策略的状态价值的算法的伪代码如下：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/2.png"></p><p>　　时序差分更新不仅可以用于回合制任务，也可用于非回合制任务，对于非回合制任务，我们可以自行将某些时段抽出来当作多个回合，也可以不划分回合当作只有一个回合进行更新。<br>　　<span class="math inline">\(n\)</span>步时序差分更新评估策略的状态价值的算法如下：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/3.png"></p><h2 id="sarsastate-action-reward-state-action">SARSA（State-Action-Reward-State-Action）</h2><p>　　SARSA 算法就比较容易理解了，就如它的名字一样，它涉及 <span class="math inline">\((S_t, A_t, R_{t+1}, S_{t+1},A_{t+1})\)</span>，更新式：</p><p><span class="math display">\[Q\left(S_{t}, A_{t}\right) \leftarrow Q\left(S_{t},A_{t}\right)+\alpha\left[R_{t+1}+\gamma Q\left(S_{t+1},A_{t+1}\right)-Q\left(S_{t}, A_{t}\right)\right]\]</span></p><p>　　伪代码如下：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/4.png"></p><p>　　<span class="math inline">\(n\)</span> 步的 SARSA也很容易理解：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/5.png"></p><h2 id="期望-sarsa-算法expected-sarsa">期望 SARSA 算法（ExpectedSARSA）</h2><p>　　期望 SARSA 算法与普通的 SARSA算法的区别就是，它不使用基于动作价值的时序差分目标，而是使用基于状态价值的时序差分目标，利用Bellman 方程，这样的目标又可以写为：</p><p><span class="math display">\[\begin{aligned}Q\left(S_{t}, A_{t}\right) &amp; \leftarrow Q\left(S_{t},A_{t}\right)+\alpha\left[R_{t+1}+\gamma\mathbb{E}_{\pi}\left[Q\left(S_{t+1}, A_{t+1}\right) |S_{t+1}\right]-Q\left(S_{t}, A_{t}\right)\right] \\&amp; \leftarrow Q\left(S_{t}, A_{t}\right)+\alpha\left[R_{t+1}+\gamma\sum_{a} \pi\left(a | S_{t+1}\right) Q\left(S_{t+1},a\right)-Q\left(S_{t}, A_{t}\right)\right]\end{aligned}\]</span></p><p>　　因为需要计算动作价值的求和，所以它有着更大的计算量，但是这样的期望运算减小了SARSA算法中出现的个别不恰当决策，这样可以避免在更新后期极个别不当决策对最终效果带来不好的影响，因此它通常需要更大的学习率</p><h1 id="异策时序差分更新">异策时序差分更新</h1><h2 id="基于重要性采样的异策算法">基于重要性采样的异策算法</h2><p>　　回顾一下重要性采样比率：</p><p><span class="math display">\[\rho_{t: h} \doteq \prod_{k=t}^{\min (h, T-1)} \frac{\pi\left(A_{k} |S_{k}\right)}{b\left(A_{k} | S_{k}\right)}\]</span></p><p>　　也就是说，通过行为策略 <span class="math inline">\(b\)</span>拿到的估计，在原策略 <span class="math inline">\(\pi\)</span>出现的概率是在策略 <span class="math inline">\(b\)</span> 中出现概率的<span class="math inline">\(\rho_{t: h}\)</span> 倍。<br>　　伪代码如下：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/6.png"></p><h2 id="q-learning">Q-learning</h2><p>　　Q-learning 是从改进后策略的行为出发，将时序差分目标改为 <span class="math inline">\(U_t = R_{t+1}+\gamma \max _{a} Q\left(S_{t+1},a\right)\)</span>，它这么做的原因是因为在根据 <span class="math inline">\(S_{t+1}\)</span> 估计 <span class="math inline">\(U_t\)</span> 时，与其使用 <span class="math inline">\(Q(S_{t+1}, A_{t+1})\)</span>（SARSA 算法）或 <span class="math inline">\(v(S_{t+1})\)</span>（期望 SARSA算法），不如使用根据动作价值改进后的策略来更新，这样可以更接近最优价值。因此，Q-learning的更新式不是基于当前的策略，而是基于另外一个并不一定要使用的确定性策略来更新动作价值，从这个角度来看Q-learning 也是一个异策算法。伪代码如下：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/7.png"></p><h2 id="double-q-learning">Double Q-learning</h2><p>　　Q-learning 使用 <span class="math inline">\(\max _{a}Q\left(S_{t+1}, a\right)\)</span>来更新动作价值，会导致最大化偏差（maximizationbias），主要会在一些中间状态出问题，需要大量的数据才能纠正。<br>　　出现这个问题的一个主要原因是使用了相同的样本来确定最大化动作并估计其值，用相同的值来选择和评价一个动作，这使得其更偏向于选择过分评估值（overestimated values），导致次优的估计值。Double Q-learning可以解决这一问题，它使用两个独立的动作价值函数，更新式如下：</p><p><span class="math display">\[Q_{1}\left(S_{t}, A_{t}\right) \leftarrow Q_{1}\left(S_{t},A_{t}\right)+\alpha\left[R_{t+1}+\gamma Q_{2}\left(S_{t+1},\underset{a}{\arg \max } Q_{1}\left(S_{t+1},a\right)\right)-Q_{1}\left(S_{t}, A_{t}\right)\right]\]</span></p><p>　　每步学习可以等概率的选择其中的一个动作价值函数：</p><p><img src="/blog/2020/02/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%9B%9B%EF%BC%89-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0/8.png"></p><p>　　Double Q-learning加倍了内存开销，但是却没有增加额外的计算开销。</p><h1 id="总结">总结</h1><p>　　时序差分更新与回合更新的区别就在于，时序差分更新汲取了动态规划方法中“自益”的思想。本文也介绍了几种经典的无模型时序差分更新方法，包括同策算法SARSA 和 异策算法 Q-learning，以及它们的一些衍生品。</p><h1 id="refer">Refer</h1><ul><li><a href="https://book.douban.com/subject/2866455/">ReinforcementLearning</a></li><li><a href="https://book.douban.com/subject/34478302/">强化学习：原理与Python实现</a></li></ul><h1 id="相关内容">相关内容</h1><ul><li><a href="http://chengfeng96.com/blog/2020/02/11/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89-Markov-%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%E6%A8%A1%E5%9E%8B/">强化学习笔记（一）-Markov 决策过程模型</a></li><li><a href="http://chengfeng96.com/blog/2020/02/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95/">强化学习笔记（二）-动态规划方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E8%92%99%E7%89%B9%E5%8D%A1%E7%BD%97%E6%96%B9%E6%B3%95/">强化学习笔记（三）-蒙特卡罗方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/19/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89-%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC%E6%96%B9%E6%B3%95/">强化学习笔记（五）-函数近似方法</a></li><li><a href="https://chengfeng96.com/blog/2020/02/21/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AD%EF%BC%89-%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6/">强化学习笔记（六）-策略梯度</a></li><li><a href="https://chengfeng96.com/blog/2020/02/24/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%83%EF%BC%89-%E8%B5%84%E6%A0%BC%E8%BF%B9/">强化学习笔记（七）-资格迹</a></li><li><a href="https://chengfeng96.com/blog/2020/02/28/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E5%85%AB%EF%BC%89-%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E7%9A%84%E7%A1%AE%E5%AE%9A%E6%80%A7%E7%AD%96%E7%95%A5/">强化学习笔记（八）-连续空间的确定性策略</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;strong&gt;&lt;em&gt;版权声明：本文原创，转载请留意文尾，如有侵权请留言，谢谢&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;h1 id=&quot;引言&quot;&gt;引言&lt;/h1&gt;
&lt;p&gt;　　上文我们提到的 MC
方法，用于回合制任务中，并且必须等到回合结束之后才可以更新价值估计。而时序差分学习（
Temporal-Difference
Learning）不需要等到回合结束也可以更新价值估计，并且不仅可以用于回合制任务，还可以用于连续性任务。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="http://chengfeng96.com/categories/AI/"/>
    
    <category term="Reinforcement Learning" scheme="http://chengfeng96.com/categories/AI/Reinforcement-Learning/"/>
    
    
    <category term="ML" scheme="http://chengfeng96.com/tags/ML/"/>
    
    <category term="RL" scheme="http://chengfeng96.com/tags/RL/"/>
    
    <category term="Temporal-Difference Learning" scheme="http://chengfeng96.com/tags/Temporal-Difference-Learning/"/>
    
    <category term="on-policy" scheme="http://chengfeng96.com/tags/on-policy/"/>
    
    <category term="off-policy" scheme="http://chengfeng96.com/tags/off-policy/"/>
    
    <category term="SARSA" scheme="http://chengfeng96.com/tags/SARSA/"/>
    
    <category term="Q-learning" scheme="http://chengfeng96.com/tags/Q-learning/"/>
    
    <category term="Importance Sampling" scheme="http://chengfeng96.com/tags/Importance-Sampling/"/>
    
    <category term="Double Q-learning" scheme="http://chengfeng96.com/tags/Double-Q-learning/"/>
    
  </entry>
  
</feed>
